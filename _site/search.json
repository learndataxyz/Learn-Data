[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn\n\n\n\npython\n\n\nseaborn\n\n\nhistogram\n\n\ndensity plot\n\n\nkde\n\n\ndistribution\n\n\nstatistics\n\n\ndata visualization\n\n\ndata science\n\n\nmachine learning\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCI/CD with Python Shiny and GitHub Actions\n\n\n\ngithub actions\n\n\npytest\n\n\nunit testing\n\n\nautomation\n\n\nCI/CD\n\n\npython\n\n\npython shiny\n\n\npython testing frameworks\n\n\nprogramming\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnd-to-end Testing with Playwright and Python Shiny\n\n\n\nE2E\n\n\nend-to-end testing\n\n\npython\n\n\npython shiny\n\n\npython testing frameworks\n\n\nprogramming\n\n\nplaywright\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nForecasting Time Series Data With Facebook Prophet in R\n\n\n\nr\n\n\nstatistics\n\n\nmachine learning\n\n\ndata science\n\n\ndata visualization\n\n\ntime series\n\n\nforecasting\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Create and Read a Forest Plot in R\n\n\n\ndata visualization\n\n\nR\n\n\ndata analysis\n\n\ndata storytelling\n\n\nplot\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Conduct Unit Tests in Python Shiny with Pytest\n\n\n\npython unit testing\n\n\npytest\n\n\npython shiny\n\n\npython testing frameworks\n\n\nprogramming\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Kaplan-Meier Survival Analysis Estimation with Python\n\n\n\npython\n\n\nstatistics\n\n\ndata visualization\n\n\ndata science\n\n\nmachine learning\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Build a Language Translator Application with Strapi, Streamlit, and Hugging Face Models\n\n\n\napi\n\n\ndata science\n\n\nmachine learning\n\n\nprogramming\n\n\npython\n\n\nstreamlit\n\n\nstrapi\n\n\nheadless cms\n\n\nhugging face\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression with Python Statsmodels: Assumptions and Interpretation\n\n\n\npython\n\n\nstatsmodels\n\n\nstatistics\n\n\nmachine learning\n\n\ndata science\n\n\ndata visualization\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Deployment in R with Plumber\n\n\n\nmachine learning\n\n\nR\n\n\nAPI\n\n\nmodel deployment\n\n\ndata science\n\n\nmlops\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonitoring Model Performance and Data Drift for Diabetes Classification\n\n\n\nmlops\n\n\ndata drift\n\n\nclassification\n\n\ntutorial\n\n\npython\n\n\ndata science\n\n\nmachine learning\n\n\nhealthcare\n\n\ndeployment\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecuring ML APIs with FastAPI\n\n\n\napi\n\n\ndata science\n\n\nmachine learning\n\n\nprogramming\n\n\nsecurity\n\n\nfastapi\n\n\npython\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Engineer’s Guide to Low Code/No Code ELT Tools\n\n\n\ndata engineering\n\n\nlow code\n\n\nguide\n\n\nno code\n\n\nELT\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "navs/about.html",
    "href": "navs/about.html",
    "title": "About",
    "section": "",
    "text": "learndata.xyz is a cutting-edge data science consulting firm specializing in R and Python programming, statistical analysis, and Shiny web application development. Founded by a team of passionate statisticians and data scientists, we bridge the gap between complex data and actionable insights."
  },
  {
    "objectID": "navs/courses.html",
    "href": "navs/courses.html",
    "title": "Courses",
    "section": "",
    "text": "@learndata.xyz"
  },
  {
    "objectID": "posts/language_translator/index.html",
    "href": "posts/language_translator/index.html",
    "title": "How to Build a Language Translator Application with Strapi, Streamlit, and Hugging Face Models",
    "section": "",
    "text": "If you’re building a cool side project or an MVP, you must store user and application content. This article will teach you about Strapi, a headless CMS you can use as your application backend. You will build a language translator application using Streamlit and a language translation model from Hugging Face that allows users to translate text written in any language to English, using Strapi as a back-end to store user inputs and outputs."
  },
  {
    "objectID": "posts/language_translator/index.html#prerequisites",
    "href": "posts/language_translator/index.html#prerequisites",
    "title": "How to Build a Language Translator Application with Strapi, Streamlit, and Hugging Face Models",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore starting, ensure you have;\n\nInstalled Python 3.9+ or above.\nInstalled Node.js\nInstalled npm\nSet up a Hugging Face account.\nSet up a Strapi account.\nSet up a Streamlit Cloud account.\nSet up a Github Account."
  },
  {
    "objectID": "posts/language_translator/index.html#set-up-the-project-directory",
    "href": "posts/language_translator/index.html#set-up-the-project-directory",
    "title": "How to Build a Language Translator Application with Strapi, Streamlit, and Hugging Face Models",
    "section": "Set up the Project Directory",
    "text": "Set up the Project Directory\n\nOn your terminal, create a new project language_translator.\n$ mkdir language_translator\nInside the project directory, create a Python virtual environment. This environment maintains the library versions used in your code, ensuring your code is reproducible every time it runs.\n$ python -m venv venv\nThe first venv command creates a virtual environment, while the second venv specifies the name of your virtual environment, which you can give any name you like. After running the above command, a venv folder should be created in your project directory.\n\nRun the following code to activate the virtual environment and start working on it.\n$ ./venv/Scripts/Activate\nUpon activation, you should see the name of your virtual environment in green, signifying that you are already in a virtual environment.\n\nInside the project directory, create a requirements.txt file\nnano requirements.txt\nCopy and paste the following Python Libraries into the requirements.txt file.\nstreamlit\npython-dotenv\nrequests\n\nstreamlit is a Python UI library for building interactive web applications.\npython-dotenv is a Python library for working with environment variables.\nrequests is an HTTP client library that you can use to make requests from an API.\n\nInstall the above Python libraries using the following command\n$ pip install -r requirements.txt\nCreate a .env file to store environment variables such as API keys and secrets.\n$ touch .env\nInside your language_translator directory, create a folder frontend for the application frontend.\n$ mkdir frontend\nIn the folder frontend, create the files main.py and utilis.py\ntouch main.py utilis.py\nIn the project folder, create a .gitignore file, to ignore the venv and .env path when pushing to git.\n/venv\n.env"
  },
  {
    "objectID": "posts/language_translator/index.html#build-the-front-end",
    "href": "posts/language_translator/index.html#build-the-front-end",
    "title": "How to Build a Language Translator Application with Strapi, Streamlit, and Hugging Face Models",
    "section": "Build the Front-end",
    "text": "Build the Front-end\nThe front end is where users will interact with the application. You will build it using Streamlit and translate user inputs using a language translator model from Hugging Face.\n\nSelect a Model from Hugging Face\n\nLog in to Hugging Face, go to the search bar, and search for facebook/mbart-large-50-many-to-one-mmt. This is the model you will use to allow users to give input in any language and translate it into English.\nClick on Deploy, then Inference API.\n\nCopy the code displayed to you and paste it into utilis.py.\n\nimport requests\n\nAPI_URL = \"<https://api-inference.huggingface.co/models/facebook/mbart-large-50-many-to-one-mmt>\"\nheaders = {\"Authorization\": \"Bearer <your-api-token>\"}\n\ndef query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.json()\n\noutput = query({\n    \"inputs\": \"The answer to the universe is\",\n})\n\nAPI_URL is the variable storing the link to the model API.\nheaders is the variable storing your Hugging Face authorization token to use the API.\nThe query function makes a POST request to the model API to translate a given input text and return the JSON output.\n\nCopy your API-TOKEN, go to the .env file, and create the variable. HUGGING_FACE_TOKEN.\nHUGGING_FACE_TOKEN=\"<your-api-token>\"\nGo back to the utilis.py file, copy and paste the following code to create a function of Step 3 that accepts a text in any language and translates it to English.\nimport requests\nimport os\nimport json\nfrom datetime import datetime\nfrom dotenv import load_dotenv\n\nload_dotenv(\".env\")\n\nHUGGING_FACE_TOKEN = os.getenv(\"HUGGING_FACE_TOKEN\")\n\nAPI_URL = \"<https://api-inference.huggingface.co/models/facebook/mbart-large-50-many-to-one-mmt>\"\nheaders = {\"Authorization\": f\"Bearer {HUGGING_FACE_TOKEN}\"}\n\ndef translate(inputs):\n    def query(payload):\n        response = requests.post(API_URL, headers=headers, json=payload)\n        return response.json()\n\n    output = query(\n        {\n            \"inputs\": inputs,\n        }\n    )\n    return output[0][\"generated_text\"]\n\nload_dotenv(\".env\") loads your .env file, making it visible for os.getenv() to get the environment variable specified.\nThe translate() function takes input text, while the query function processes it and returns it in JSON. Finally, the translate() function extracts the translated text.\nHere is an example of how the translate function works. Copy and run the text below.\n\ntext = \"संयुक्त राष्ट्र के प्रमुख का कहना है कि सीरिया में कोई सैन्य समाधान नहीं है\"\ntranslate(text)\n\n>>Output: 'The head of the UN says there is no military solution in Syria'\n\n\n\nBuild the User Interface with Streamlit\n\nInside your main.py file, import the translate function and streamlit.\nfrom utilis import translate\nimport streamlit as st\nCreate a title and input area.\nst.title(\"Language Translator to English\")\n\ninput_text = st.text_area(\"Enter the text you want to translate:\", height=150)\n\nst.title sets a text in bold and large size.\nst.text_area is a multi-line text input widget that allows users to give input.\n\nRun the code below on your terminal to launch the Streamlit application.\nstreamlit run main.py\n\nCreate a button to process any input given in the text area.\nif st.button(\"Translate\"):\n    if input_text:\n        translated_text = translate(input_text)\n        st.write(\"## Translated Text\")\n        st.write(translated_text)\n    else:\n        st.warning(\"Please enter some text to translate.\")\n\nst.button creates a button to translate a text.\nIf there is an input text, the translation function is called on the input_text and saved as translated_text; otherwise, a warning is given.\nSave your file and go to the Streamlit application to see the changes.\n\n\nPaste the Hindi text given earlier into the text area and click on translate.\n\nWhat is left is to set up a back-end on Strapi to save any translated text."
  },
  {
    "objectID": "posts/language_translator/index.html#build-the-back-end",
    "href": "posts/language_translator/index.html#build-the-back-end",
    "title": "How to Build a Language Translator Application with Strapi, Streamlit, and Hugging Face Models",
    "section": "Build the Back-end",
    "text": "Build the Back-end\n\nSet up Strapi\n\nIn your main directory language_translator, create a new Strapi project called backend.\nnpx create-strapi-app@latest backend --quickstart\nThis will install Strapi and an SQLite database in your project directory.\nRun the following code in your terminal to open the admin panel at http://localhost:1337/admin. Fill in your credentials, Sign up, and Log in.\nnpm run develop\n\nGo to Content Type Builder, click Create a new collection, create a new collection Translation, then Continue.\n\nCreate input_text and translated_text as Text fields while translation_date is a Date field and Save.\n\nGo to Settings on your admin menu, under User & Permissions Plugin, click on Roles, and edit the Public role.\n\nIn the Public role, set the Translation permission to create and find, then Save.\n\n\n\n\nConnect Strapi with Streamlit\n\nCopy and paste the following code to create a function save_translation that saves both the input_text and output_text to Strapi.\nSTRAPI_URL = \"<http://localhost:1337/api>\"\n\ndef save_translation(input_text, translated_text):\n    data = {\n        \"data\": {\n            \"input_text\": input_text,\n            \"translated_text\": translated_text,\n            \"translation_date\": datetime.now().isoformat(),\n        }\n    }\n\n    response = requests.post(\n        f\"{STRAPI_URL}/translations\",\n        headers={\"Content-Type\": \"application/json\"},\n        data=json.dumps(data),\n    )\n    return response.json()\n\nSTRAPI_URL is the link to the Strapi backend API. You will use this to send and receive data from Strapi.\nThe save_translation() function receives the user input_text, translated_text, and translation_date as JSON in the variable data and sends it for storage in the Strapi back-end.\ndatetime.now().isoformat() saves the current date and time an input_text was translated as \"translation_date\" into data.\nThe response variable makes a POST request to save the values into the Translation collection in Strapi.\n\nGo back to main.py and update the code just before the end of the inner if condition. This will ensure that anytime a user clicks the Translate button, the text displays, and the input and output text are saved to Strapi through the save_translation() function.\n\nif st.button(\"Translate\"):\n    if input_text:\n        #code to display text after input is given\n        #....\n        #code to update\n        #--------------\n        save_translation(input_text, translated_text)\n    else:\n        st.warning(\"Please enter some text to translate.\")\n\nTry to translate a text, and go back to your dashboard under the Content Manager menu to see the saved text.\n\nTo output the history from latest to oldest, add the get_history() function to utilis.py.\ndef get_history():\n    response = requests.get(f\"{STRAPI_URL}/translations\")\n    if response.status_code == 200:\n        return response.json()\n    return []\n\nThis function gets all the items stored in the Translation collection using a GET request and saves it in the variable response as JSON.\n\nUpdate main.py with the following code to create a History button where users can view previous translations.\nif st.button(\"History\"):\n    history = get_history()\n    if history:\n        for item in reversed(history[\"data\"]):\n            st.text(\n                f\"Date: {item['attributes']['translation_date']}\\\\nInput: {item['attributes']['input_text']}\\\\nTranslated: {item['attributes']['translated_text']}\"\n            )\n    else:\n        st.write(\"No history found.\")\n\nIf the History button is clicked, the get_history() is called, fetching and displaying all items in the Translation collection.\nThe reversed() function loops the list history[data] in a reverse order to display recently added text first.\nIf no history is found, the application returns a warning.\nIf you click the history button, you should have something like this."
  },
  {
    "objectID": "posts/language_translator/index.html#deployment",
    "href": "posts/language_translator/index.html#deployment",
    "title": "How to Build a Language Translator Application with Strapi, Streamlit, and Hugging Face Models",
    "section": "Deployment",
    "text": "Deployment\nThe application consists of two components: the back end and the front end. There are various ways to deploy these components, but for this tutorial, you will use Strapi Cloud to deploy the back end and Streamlit Cloud to deploy the front end.\nStrapi Cloud deployment is free for 14 days, but the Streamlit Cloud is completely free. Before proceeding, ensure you have already versioned your project on GitHub.\n\nDeploy the Backend on Strapi Cloud\n\nGo to Deploy on your admin dashboard menu, and click Deploy to Strapi Cloud.\n\nSign in to Strapi Cloud.\n\nClick on Create Project to create a new project.\n\nSelect the Free trial and click on GitHub to permit Strapi to authorize your GitHub account.\n\nAfter completing the authorization steps, you should have something like this. Select the account you want Strapi to access and the project repository.\n\nGive the application a display name, and leave other options as the default.\n\nUnder Show Advanced Settings, type the backend directory, which is /backend, leave other options as default, and click on Create Project.\n\nYou can see the application build and deploy logs displayed while the application is being built.\n\nClick on the Visit app to open up the live admin panel.\n\nLike before, when you open the admin panel locally, fill in your credentials and log in.\n\nSince the backend is deployed, change the STRAPI_URL variable in utilis.py to the live URL. Copy the URL of the live dashboard, excluding the part /admin, and include /API at the end.\nSTRAPI_URL=\"<your-strapi-api-url>/api\"\nEnsure you cross-check your settings to see if all the roles are set. If not, you can set them back using Step 6 in the Set up Strapi section.\n\nThat’s it, you now have your back-end live, let’s deploy the frontend and the back-end.\n\n\nDeploy the Frontend on Streamlit Cloud\n\nLog in to Streamlit Cloud and authorize Streamlit to access your GitHub account.\nClick on Create app.\n\nPlease complete the form by providing the project repository and the path of the streamlet application, which is /frontend/main.py. If you have authorized Streamlit to access your GitHub repositories, it will list all the repositories in your GitHub account.\n\nClick on Advanced Settings, type in your HUGGING_FACE_TOKEN as it is in your .env file, and Save.\n\nClick on Deploy! to start building the application. This is going to take a while.\nNow, you have successfully deployed the Streamlit application."
  },
  {
    "objectID": "posts/language_translator/index.html#conclusion",
    "href": "posts/language_translator/index.html#conclusion",
    "title": "How to Build a Language Translator Application with Strapi, Streamlit, and Hugging Face Models",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, you have learned how to use a model from Hugging Face to build a Streamlit application and store your user inputs and outputs on a Strapi backend. You also learned how to deploy the back-end and front-end components on Strapi and Streamlit Cloud. Strapi has a fun and active Discord community to help answer your questions whenever you feel stuck.\nHere is the GitHub repository for the language translator application. You can further extend the application by:\n\nAdd authorization to your Streamlit front end and set various user roles and permissions for your Strapi backend.\nExploring various language models on Hugging Face, such as English-to-many language translators or audio translators."
  },
  {
    "objectID": "posts/language_translator/index.html#recommended-reads",
    "href": "posts/language_translator/index.html#recommended-reads",
    "title": "How to Build a Language Translator Application with Strapi, Streamlit, and Hugging Face Models",
    "section": "Recommended Reads",
    "text": "Recommended Reads\n\nML Model Deployment with FastAPI and Streamlit\nUsing Python with Strapi\nDeploying to Strapi Cloud\nAdmin Panel Customization\nAnalyzing data from a Strapi API with Python\nDeploying to Streamlit Cloud\n\n\nNeed Help with Data? Let’s Make It Simple.\nAt LearnData.xyz, we’re here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we’ve got your back.\n📧 Shoot us an email at admin@learndata.xyz—let’s chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#introduction",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#introduction",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Introduction",
    "text": "Introduction\nAccording to WHO, the number of people with diabetes rose from 108 million in 1980 to 422 million in 2014. Diabetes is a serious disease that leads to blindness, kidney failure, heart attacks, strokes, and lower limb amputations. It is mostly prevalent in low- and middle-income countries.\nBuilding a healthcare system that uses machine learning to predict patients with diabetes will help in early detection making it easy for healthcare providers to screen patients with diabetes at an early stage, before diagnosis.\n\nMachine learning models tend to degrade with time, highlighting the need for effective and constant monitoring of the model to know when its performance is declining. Often, this arises as a result of the change in the distribution of the data compared to the data the model was trained upon, this phenomenon is known as Data Drift.\nIn this article, you will learn how to monitor a diabetes classifier and detect data drifts in the data received from patients in a health information management system or mobile application using NannyML.\nNannyML is an open-source library for monitoring machine learning model performance in production, even without the predicted values being ready. It allows you to track your machine-learning model over time and see checkpoints where the model degrades."
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#estimating-model-performance-with-nannyml",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#estimating-model-performance-with-nannyml",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Estimating Model Performance with NannyML",
    "text": "Estimating Model Performance with NannyML\nNannyMl offers binary class classification that one could use to estimate the model’s performance, even without targets. Model estimation performance with NannyML involves:\n\nGetting the reference and analysis sets ready: The reference set is the data where the model behaves as expected, usually the test data. The analysis set is the latest production data, either with target features or not.\nTraining a performance estimator on the reference set: NannyML uses the reference set to train a performance estimator, it’s advisable to use the test data as reference data to prevent overfitting.\nUsing the estimator to predict performance on the analysis set (simulating real-world data): NannyML estimates the model performance on the analysis data using the trained performance estimator. One can use various classification metrics, such as accuracy or F1-score. Since misclassifying a patient (false negative) is more severe than misclassifying a healthy patient as diabetic (false positive), the AUC-ROC is the most appropriate metric to use in this case."
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#detecting-data-drift-with-nannyml",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#detecting-data-drift-with-nannyml",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Detecting Data Drift with NannyML",
    "text": "Detecting Data Drift with NannyML\nLet’s say you deployed a machine-learning model. As time goes on, the model tends to degrade, This is due to the nature of the data changing. If you have an application that you initially designed for kids and you train most of your machine learning models using your current user’s data, then all of a sudden middle-aged people and the elderly start using your application, and they become more of your users than the kids you designed it for. This will change the age distribution of your data, If age is one important feature in your machine learning model, your model will get worse with time. This is where you need to monitor when such changes happen in your data so that you can update the ML model.\n\nNannyML uses various algorithms to detect data drift, either using Univariate drift detection or Multivariate drift detection methods.\n\nUnivariate drift detection: In this approach, NannyML looks at each feature used in classifying if a patient is diabetic, and compares the chunks with those created from the analysis period. The result of the comparison is called a drift metric, and it is the amount of drift between the reference and analysis chunks, which is calculated for each chunk.\nMultivariate Drift Detection Instead of taking every feature one by one, NannyML provides a single summary metric explaining the drift between the reference and the analysis sets. Although this approach can detect slight changes in the data, it is difficult to explain compared to univariate drift.\n\nIn the case of classifying diabetic patients, undetected drift is dangerous and can lead to wrong model classifications. This is worse if the number of false negatives is high, the classifier might not detect some patients with diabetes, this can lead to late diagnosis."
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#estimating-model-performance-in-the-diabetes-classifier",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#estimating-model-performance-in-the-diabetes-classifier",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Estimating Model Performance in the Diabetes Classifier",
    "text": "Estimating Model Performance in the Diabetes Classifier\nNannyML uses two main approaches to estimate model performance, Confidence-based Performance estimation (CBPE) and Direct Loss estimation (DLE). In this case, we are interested in using the CBPE, since we are dealing with a classification task.\nThe CBPE uses the confidence score of the predictions to estimate the model performance, the confidence score is a value that the diabetes classifier gives for each predicted observation, expressing its confidence in predicting if a patient is diabetic., with values ranging from 0 to 1 and the closer it is to 1, the more confident the classifier is with it’s prediction.\nThe diabetes data contains 253,680 responses and 21 features. In this section, you will learn how to use this data to build an ML model, estimate your model’s performance, and detect data drift on updated data.\n\nProject Requirements\nTo get started, ensure you have installed NannyML on your JupyterNotebook. Download the analysis and diabetes data. The diabetes data is the data you will train the machine learning model on, and the analysis data is what you would take as the production data from the patients, which you will use to estimate model performance and detect data drift later on.\n\n\nBuilding the ML Model\nLet’s build a simple random forest classifier to classify respondents as diabetic.\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load your data\ndiabetes = pd.read_csv(\"binary_diabetes.csv\")\n\n# Split the data into features (X) and target (y)\nX = diabetes.drop('Diabetes_binary', axis=1)\ny = diabetes['Diabetes_binary']\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Create and train the model\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Get the predicted probabilities\ny_pred_proba = model.predict_proba(X_test)[:, 1]  \n\n# Print the classification report\nprint(classification_report(y_test, y_pred))\n\n# Create a dataframe with the test data, predicted classes, and predicted probabilities\ntest_results = X_test.copy()\ntest_results['y_true'] = y_test\ntest_results['y_pred'] = y_pred\ntest_results['y_pred_prob'] = y_pred_proba\nHere is the output of the classification model.\n                                precision    recall  f1-score   support\n\n         0.0       0.88      0.97      0.92     65605\n         1.0       0.48      0.18      0.26     10499\n\n    accuracy                           0.86     76104\n   macro avg       0.68      0.57      0.59     76104\nweighted avg       0.83      0.86      0.83     76104\nFrom the report above, the classifier can classify those with diabetes with a precision of 0.88 accurately. This means the chances of the classifier missing a diabetic patient are low. An overall accuracy of 0.86 indicates the model is performing well.\n\n\nEstimating Model Performance\nUsing CBPE, you can estimate the model performance when in production, and the analysis data does not necessarily have to contain the target feature.\nimport nannyml as nml\n\nreference_df = test_results\nanalysis_df = pd.read_csv(\"/content/analysis_df.csv\")\n\nestimator = nml.CBPE(\n    y_pred_proba =\"y_pred_prob\",\n    y_pred = \"y_pred\",\n    y_true = \"y_true\",\n    problem_type = \"classification_binary\",\n    metrics = [\"roc_auc\"],\n    chunk_size = 3000\n)\n\nestimator.fit(reference_df)\nresults = estimator.estimate(analysis_df)\n\nmetric_fig = results.plot()\nmetric_fig.show()\n\nnml.CBPE calculates the CBPE and takes the argument, y_pred_proba the predicted probability, y_pred the predicted classification, y_true the actual classification.\nThe problem_type argument takes the type of classification one is interested in, binary classification in this case.\nResults in NannyML are presented per chunk, by aggregating the data to a single data point on the monitoring results; here I used a chunk of 3000.\n\nYou can use any other metric, but the roc-auc metric is preferred in this situation because this is a health problem and it tells us more about the ability of the classifier to detect true positives, to minimize the chance of missing any diabetic patient.\n\nThe plot above shows that the model ROC-AUC metric started to fail at some point, which led to inaccurate predictions. The blue marker indicates the model metric, if this line exceeds the upper or lower threshold in red, it raises an alert signifying a drift.\nThe next step is to see what causes the poor model performance in these chunks by applying various drift detection methods.\n\n\nDetecting Data Drift\nLet’s use a multivariate drift detection method to get a summary number that detects any changes in our data using the domain classifier approach. This provides a measure of discriminating the reference data from the examined chunk data. If there is no data drift, the datasets are not different giving a value of 0.5. The higher the drift, the higher the returned measure, with a value as high as 1. This method provides a general overview of data drift in the analysis data.\nnon_feature_columns = ['y_pred_proba', 'y_pred', 'y_true']\nfeature_column_names = [\n    col for col in reference_df.columns\n    if col not in non_feature_columns\n]\ncat_features = [feature for feature in feature_column_names if feature not in ['BMI', 'MentHlth', 'PhysHlth', 'Age']]\n\ncalc = nml.DomainClassifierCalculator(\n    feature_column_names=feature_column_names,\n    chunk_size=3000\n)\n\ncalc.fit(reference_df)\nresults = calc.calculate(analysis_df)\n\nfigure = results.plot()\nfigure.show()\n\nThe plot above shows the domain classifier values in the reference data are either a little below or above 0.5, but that of the analysis data reaches a value of 1, indicating the presence of data drift in the analysis data. Now that we know we have data drift in the analysis data, let’s see which features are causing this drift.\n\n\nRanking\nWe need to know the features contributing to drift in the analysis data. The ranking method uses the results of univariate drift detection to rank features based on alert counts or correlation ranking.\nLet’s use the alert count’s approach to know which features are causing the drift in data.\nuniv_calc = nml.UnivariateDriftCalculator(\n    column_names=feature_column_names,\n    treat_as_categorical=['y_pred', *cat_features],\n    continuous_methods=['kolmogorov_smirnov', 'jensen_shannon'],\n    categorical_methods=['chi2', 'jensen_shannon'],\n    chunk_size=3000\n)\n\nuniv_calc.fit(reference_df)\nunivariate_results = univ_calc.calculate(analysis_df)\n\nalert_count_ranker = nml.AertCountRanker()\nalert_count_ranked_features = alert_count_ranker.rank(\n    univariate_results.filter(methods=['jensen_shannon']),\n    only_drifting = False)\ndisplay(alert_count_ranked_features)\nkolmogorov_smirnov, jensen_shannonand chi2 are various univariate drift detection methods you can always choose from.\n\nThe table above shows the top 10 features likely to cause drifts based on the alert counts. Next, let’s investigate further the contribution to data drift for each variable using the univariate drift detection method.\n\n\nUnivariate Drift Detection Method\nUnivariate drift detection allows you to see the amount of drift in the suspected features, which was used earlier to rank the features.\nuniv_calc = nml.UnivariateDriftCalculator(\n    column_names=feature_column_names,\n    treat_as_categorical=['y_pred', *cat_features],\n    continuous_methods=['jensen_shannon'],\n    categorical_methods=['jensen_shannon'],\n    chunk_size=3000\n)\n\nuniv_calc.fit(reference_df)\nunivariate_results = univ_calc.calculate(analysis_df)\n\nfigure = univariate_results.filter(column_names=univariate_results.continuous_column_names, methods=['jensen_shannon']).plot(kind='drift')\nfigure.show()\n\nThe plots above show the amount of drift in some of the features using Jensen-Shannon distance, which you can apply to both continuous and categorical features. You can also see that the red dotted points exceed the dotted line in each plot, this signifies drift in the variable.\nYou can also go further into each feature to see the distribution, this lets you know how large this drift is Let’s take a critical look at PhyHlth, this is the first feature on the ranked list.\nfigure = univariate_results.filter(column_names=[\"PhysHlth\"], methods=['jensen_shannon']).plot(kind='distribution')\nfigure.show() \n\nFrom the plot, you can see in highlight chunks with data drift, the plot is wider and bigger compared to that of the reference data, the plot also tells the presence of negative values in the analysis data. You can follow this procedure for each feature in the model indicating data drift using the univariate drift detection method. From the above results, you can understand the change in model prediction and the cause of those changes."
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#conclusion",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#conclusion",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, you learned about NannyML an open-source tool for monitoring model performance and detecting data drifts. You also learned how to use NannyML on a diabetes classifier and how to apply both univariate and multivariate drift detection methods in detecting data drift using NannyML.\nWhat’s next after detecting data drift? Check out this article, to know what to do when you detect drifts in your data."
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#recommended-reads",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#recommended-reads",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Recommended Reads",
    "text": "Recommended Reads\n\nMonitoring a Hotel Booking Cancellation Model Part 1: Creating Reference and Analysis Set\nTutorial: Monitoring an ML Model with NannyML and Google Colab\nHow to Estimate Performance and Detect Drifting Images for a Computer Vision Model?\n\n\nNeed Help with Data? Let’s Make It Simple.\nAt LearnData.xyz, we’re here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we’ve got your back.\n📧 Shoot us an email at admin@learndata.xyz—let’s chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/Securing ML APIs with FastAPI/index.html",
    "href": "posts/Securing ML APIs with FastAPI/index.html",
    "title": "Securing ML APIs with FastAPI",
    "section": "",
    "text": "The end goal of a machine learning model is to serve end users. Still, due to machine learning models requiring regular updates to improve model accuracy and use in other applications, they are exposed as an API. An ML API is an application that serves as a gateway between your client requests or needs and your machine learning model.\nLet’s say you have a recommender model on an e-library platform, that recommends books for users based on user preferences. This recommender model works as an API by getting user preferences and recommending books to the user. The API also makes it easy for you to use the recommender model on another platform.\nDue to the sensitivity of training data in machine learning models, API security is important to avoid data breaches and prevent malicious clients from accessing the model. In this article, I will show you how to secure your machine-learning APIs using FastAPI - an open-source Python framework that allows you to build secured and scalable APIs. As a Python library, the learning curve is low for data scientists and machine learning engineers with Python backgrounds. If you are new to FastAPI check out this course on ML deployment with FastAPI."
  },
  {
    "objectID": "posts/Securing ML APIs with FastAPI/index.html#fundamentals-of-api-security",
    "href": "posts/Securing ML APIs with FastAPI/index.html#fundamentals-of-api-security",
    "title": "Securing ML APIs with FastAPI",
    "section": "Fundamentals of API Security",
    "text": "Fundamentals of API Security\nAPI is usually a target for data breaches and unauthorized access due to the information it contains, making it prone to security attacks, this is why API security is important. API security is a practice set to protect an API from unauthorized access. Here are some of the most common API security threats:\n\nInjection attacks (SQL, command): In this type of attack, someone injects malicious code into the API, using SQL or terminal commands to read or modify the database. These kinds of attacks are usually targeted at the application’s database.\nCross-site scripting (XSS): This is another type of attack where a hacker manipulates a vulnerable site by sending malicious JavaScript to users, which upon execution by a user, the attacker can masquerade as the user and manipulate the user’s data.\nCross-site request forgery (CSRF): In this attack, attackers make users perform actions they don’t intend to do.\nMan-in-the-middle (MITM) attacks: In this attack, hackers eavesdrop between the interaction of clients and the API, to steal relevant credentials such as login details and credit card information.\n\nIn this article, you will learn how to solve these issues and make your machine-learning API secure."
  },
  {
    "objectID": "posts/Securing ML APIs with FastAPI/index.html#prerequisites",
    "href": "posts/Securing ML APIs with FastAPI/index.html#prerequisites",
    "title": "Securing ML APIs with FastAPI",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nHave basic knowledge of Python and FastAPI framework\nEnsure you have installed scikit-learn , fastapi , pydantic, uvicorn , numpy,and joblib libraries."
  },
  {
    "objectID": "posts/Securing ML APIs with FastAPI/index.html#setting-up-fastapi-for-ml-apis",
    "href": "posts/Securing ML APIs with FastAPI/index.html#setting-up-fastapi-for-ml-apis",
    "title": "Securing ML APIs with FastAPI",
    "section": "Setting Up FastAPI for ML APIs",
    "text": "Setting Up FastAPI for ML APIs\n\nCreate a project folder and a virtual environment.\nCopy and paste the following code into a new file called utilis.py in your project directory. This will create a classification model and a model.pkl file based on the iris dataset.\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nimport joblib\n\n# Load the iris dataset\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Train a random forest classifier\nmodel = RandomForestClassifier()\nmodel.fit(X, y)\n\n# Save the trained model\njoblib.dump(model, 'model.pkl')\nCreate an API endpoint for the machine-learning model in a file main.py .\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nimport joblib\n\n# Load the trained model\nmodel = joblib.load(\"model.pkl\")\n\n# Define the request body using Pydantic\nclass PredictionRequest(BaseModel):\n    sepal_length: float\n    sepal_width: float\n    petal_length: float\n    petal_width: float\n\napp = FastAPI()\n\n@app.post(\"/predict\")\ndef predict(request: PredictionRequest):\n    # Convert request data to a format suitable for the model\n    data = [\n        [\n            request.sepal_length,\n            request.sepal_width,\n            request.petal_length,\n            request.petal_width,\n        ]\n    ]\n    # Make a prediction\n    prediction = model.predict(data)\n    # Return the prediction as a response\n    return {\"prediction\": int(prediction[0])}\n\n# To run the app, use the command: uvicorn script_name:app --reload\n# where `script_name` is the name of your Python file (without the .py extension)\n\nWe now have our ML model API, let’s see how we can implement security best practices using this API."
  },
  {
    "objectID": "posts/Securing ML APIs with FastAPI/index.html#implementing-authentication-and-authorization",
    "href": "posts/Securing ML APIs with FastAPI/index.html#implementing-authentication-and-authorization",
    "title": "Securing ML APIs with FastAPI",
    "section": "Implementing Authentication and Authorization",
    "text": "Implementing Authentication and Authorization\nTake API authentication like a passkey that allows a client to access your API, allowing only authorized users to use the API. There are various ways of implementing API authentication in FastAPI, which you will learn subsequently.\n\n\n\nAuthentication vs Authorization. Source: Medium\n\n\nAPI authentication is insufficient to protect your API, you also need to implement API authorization. API authentication is like giving someone a key to your house, while API authorization is like giving them access to specific rooms in the house.\n\nAPI Key-Based Authentication\nThis is the most basic and popular form of implementing API security.\n\nTo implement key-based authentication in FastAPI, add the following code before the @app.post(\"/predict\") endpoint in main.py file.\n# Define the API key\nAPI_KEY = \"your_api_key_here\"\n\n# Dependency to verify the API key\ndef get_api_key(api_key: str = Header(...)):\n    if api_key != API_KEY:\n        raise HTTPException(status_code=403, detail=\"Could not validate credentials\")\n\nAPI_KEY is the variable that contains your environment key, which is supposed to be stored as an environment variable in a .env file.\nget_api_key() function gets the API_KEY and verifies if the provided API key matches what’s on the database. If successful, the user is granted access to the API, else an HTTP error 403 is raised, telling the user that the provided credential is invalid.\n\nNext, go to the predict() function and add api_key as an argument to get the api_key from users.\n@app.post(\"/predict\")\ndef predict(request: PredictionRequest, \n                        #added argument to get API key from user\n            api_key: str = Depends(get_api_key)):\n\nDepends function prevents access to the /predict endpoint without the API key.\n\n\n\n\nHow authentication works in FastAPI\n\n\n\n\n\nOAuth2 with JWT Tokens\nUnlike API keys, OAuth2 is an authorization protocol, granting clients access to resources hosted by other web applications on behalf of the user. With OAuth2, users do not need to give out their password to access a resource.\nA practical example is a client accessing your machine learning API using their Google ID without giving away their details, and your API in turn sends a token back to the client to serve as a temporary password for the client to access the API. It’s very secure compared to the API key. Unlike the API key which grants a user access to all resources in an API, OAuth2 only grants the client access to specified resources.\nWhen a user wants to access a machine learning API through a client application, the process typically uses OAuth2 for secure authentication. The client application starts by redirecting the user to an authentication server, where the user grants permission for the application to access their resources. The authentication server then issues an access token, often, in the form of a JWT (JSON Web Token) to the client application. The application uses this token to make requests to the machine learning API. The API verifies the token to ensure the client is authorized to access the requested resources, thus providing secure and controlled access while protecting user data and privacy.\n\n\n\nOAuth2 Workflow: Source: GeeksforGeeks\n\n\nLet’s implement a simple OAuth2 with JWT on our machine learning API, by updating the main.py file as follows.\n\nEnsure you install pyjwt and import the following Python libraries.\nfrom fastapi import FastAPI, Depends, HTTPException\nfrom fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm\nfrom pydantic import BaseModel\nimport joblib\nfrom typing import Optional\nimport jwt\n\nOAuth2PasswordBearer and OAuth2PasswordRequestForm are used to implement OAuth2 in FastAPI.\njwt is used to create a JSON Web Token.\n\nDefine the user model, to allow the user to provide a username and password using the BaseModel class.\n# Define a user model\nclass User(BaseModel):\n    username: str\n    password: str\nCreate a function to authenticate users.\ndef authenticate_user(username: str, password: str) -> Optional[User]:\n    if username == \"admin\" and password == \"password\":\n        return User(username=username, password=password)\n    return None\n\nThe authenticate_user() function takes in a client username and password to see if it matches what’s in the database and returns a User model.\n\nCreate a SECRET_KEY variable to encode the JWT and create an oauth2_scheme\nSECRET_KEY = \"your-secret-key\"\n\n# OAuth2 scheme using password flow\noauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"token\")\nCreate a function to access the token using JWT.\ndef create_access_token(data: dict):\n    return jwt.encode(data, SECRET_KEY)\n\nThe create_access_token() function takes in the user details and encodes it with the SECRET_KEY\n\nCreate an authentication route to generate the access token.\n@app.post(\"/token\")\nasync def login_for_access_token(form_data: OAuth2PasswordRequestForm = Depends()):\n    user = authenticate_user(form_data.username, form_data.password)\n    if not user:\n        raise HTTPException(\n            status_code=401,\n            detail=\"Incorrect username or password\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n    access_token = create_access_token({\"sub\": user.username})\n    return {\"access_token\": access_token, \"token_type\": \"bearer\"}\n\nThe login_for_access_token()function takes the user inputs; username and password with the OAuth2 flow as an argument to return an access token to give the client application.\nIf user details are right, an access token is created and returned, else a 401 warning is returned\n\nProtect the API route that requires JWT authentication\n@app.post(\"/predict\")\nasync def predict(request: PredictionRequest, token: str = Depends(oauth2_scheme)):\n    try:\n        # Decode JWT token\n        payload = jwt.decode(token, SECRET_KEY, algorithms=[\"HS256\"])\n        # Convert request data to a format suitable for the model\n        data = [\n            [\n                request.sepal_length,\n                request.sepal_width,\n                request.petal_length,\n                request.petal_width,\n            ]\n        ]\n        # Make a prediction\n        prediction = model.predict(data)\n        # Return the prediction as a response\n        return {\"prediction\": int(prediction[0])}\n    except jwt.exceptions.DecodeError:\n        raise HTTPException(\n            status_code=401,\n            detail=\"Could not validate credentials\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n\nThe argument token: str = Depends(oauth2_scheme) means the API endpoint is protected using OAuth2, and receives the access token from the client application.\nThe token is decoded to see if it contains the SECRET_KEY, if it does, access is given to the model prediction, else a warning is given stating that the provided credentials are invalid.\n\n\n\n\nKey takeaways\n\nUser logs in and their data is encoded with a secret key to create an access token\nThe secured API endpoint decodes this access token to see if it contains the secret key before providing access to the resource.\n\n\n\nHow OAuth2 works in FastAPI.\n\n\nHow OAuth2 works in FastAPI.\n\n\n\nRole-Based Access Control (RBAC)\nRBAC is an approach where users are given various roles that provide access to specific API resources. It is an efficient way of ensuring API security, instead of granting all users privileges, users are granted privileges based on their needs in an API.\n\nLet’s implement an RBAC into the OAuth we created recently, by creating a dummy user data inside main.py.\n# Dummy user data\nusers_db = {\n    \"admin\": {\"username\": \"admin\", \"password\": \"password\", \"role\": \"admin\"},\n    \"user\": {\"username\": \"user\", \"password\": \"password\", \"role\": \"user\"}\n}\nTo demonstrate RBAC, admin will have access to our model prediction API endpoint while user will not have access to it. Update the User model to have a role field.\n# Define a user model with role\nclass User(BaseModel):\n    username: str\n    password: str\n    role: str\nJust before the API endpoint, add the following function\n# Role-based access control dependency\ndef role_checker(required_role: str):\n    def role_dependency(current_user: User = Depends(get_current_user)):\n        if current_user.role != required_role:\n            raise HTTPException(\n                status_code=403,\n                detail=\"Operation not permitted\",\n            )\n        return current_user\n    return role_dependency\n\nThe function role_checker() checks for the required role, by taking the required role admin as an argument.\nThe role_dependency() function checks if a user meets a required role, by taking the User as an argument.\nIf the user meets the required role, then the user is granted access, else a 403 status code is returned with a warning \"Operation not permitted\"\n\nUpdate the API endpoint by adding a user argument.\n@app.post(\"/predict\")\nasync def predict(request: PredictionRequest, \n                  token: str = Depends(oauth2_scheme), \n                  current_user: User = Depends(role_checker(\"admin\"))):\n\nThe current_user argument ensures that no User can access an API endpoint unless given permission.\n\n\n\nHow RBAC works in FastAPI."
  },
  {
    "objectID": "posts/Securing ML APIs with FastAPI/index.html#input-validation-and-sanitization",
    "href": "posts/Securing ML APIs with FastAPI/index.html#input-validation-and-sanitization",
    "title": "Securing ML APIs with FastAPI",
    "section": "Input Validation and Sanitization",
    "text": "Input Validation and Sanitization\nInput validation involves checking all inputs in an API to ensure that they meet certain requirements, while sanitization is input modification to ensure validity. Validation checks involve checking for allowed characters, length, format, and range, at the same time, sanitization is the changing of the input to ensure it is valid, such as shortening an input, or the removal of HTML tags in an input.\nInput validation and sanitization help to prevent common attacks like SQL injection and Cross-site scripting, most times you use input validation when your user is to give a particular input type, for example, a mobile number which is all digits. Sanitization is used when the user is expected to provide varying input types such as a user’s profile.\n\nUsing Pydantic for Input Validation\npydantic is a Python library that allows you to define and validate user inputs. It makes it easy to perform schema validation and serialization using type annotations. Earlier on, we used Pyndantic to validate our User and PredictionRequest.\nclass PredictionRequest(BaseModel):\n    sepal_length: float\n    sepal_width: float\n    petal_length: float\n    petal_width: float\n\nclass User(BaseModel):\n    username: str\n    password: str\n    role: str"
  },
  {
    "objectID": "posts/Securing ML APIs with FastAPI/index.html#securing-data-transmission",
    "href": "posts/Securing ML APIs with FastAPI/index.html#securing-data-transmission",
    "title": "Securing ML APIs with FastAPI",
    "section": "Securing Data Transmission",
    "text": "Securing Data Transmission\nWhen exchanging data between systems, it’s important to use data transmission protocols to secure and protect the data from unauthorized access. Data transmission security ensures that only authorized users can transmit data, and protect the system from vulnerabilities. There are various protocols one can force to keep data transmission secured such as HTTPS(Hypertext Transfer Protocol Secure), TLS(Transport Layer Security), SSH(Secure Shell), and FTPS(File Transfer Protocol Secure), we will only talk about HTTPS.\n\nEnforcing HTTPS\nHTTPS is a secured version of HTTP, where the data is encrypted when data is exchanged between a client and an API. Especially, when confidential details are shared such as user login credentials or account details. Unlike HTTP which has no security layer and makes data vulnerable, HTTPS adds an SSL/TLS layer to ensure that data is encrypted and secured.\n\n\n\nHTTPS workflow. Source: GeeksForGeeks\n\n\n\nTo secure data in the API endpoint we created earlier, let’s generate a self-signed certificate for testing. Copy and paste the following code into your terminal.\nopenssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -days 365 -nodes\nThis will generate a self-signed SSL/TLS certificate with a private key using OpenSSL.\n\nopenssl: This is the command-line tool for using the various cryptography functions of OpenSSL’s library.\nreq: This sub-command is used to create and process certificate requests (CSRs) and, in this case, to create a self-signed certificate.\nx509: This option is used to generate a self-signed certificate instead of a certificate request.\nnewkey rsa:4096: This option does two things:\n\nnewkey: It generates a new private key along with the certificate.\nrsa:4096: This specifies the type of key to create, in this case, an RSA key with a size of 4096 bits.\n\nkeyout key.pem: This specifies the file where the newly generated private key will be saved (key.pem).\nout cert.pem: This specifies the file where the self-signed certificate will be saved (cert.pem).\ndays 365: This sets the certificate to be valid for 365 days (1 year).\nnodes: This option ensures that the private key will not be encrypted with a passphrase. Without this option, OpenSSL would prompt for a passphrase to encrypt the private key.\n\nProvide the necessary information to create the key.pem (private key) and cert.pem (certificate).\n\n\n\nGenerating a self-signed certificate using OpenSSL.\n\n\nGenerating a self-signed certificate using OpenSSL.\nAt the end of the main.py file, add the following code.\nimport uvicorn \n\nif __name__ == \"__main__\":\n    uvicorn.run(\n        app, host=\"127.0.0.1\", port=8000, ssl_keyfile=\"key.pem\", ssl_certfile=\"cert.pem\"\n    )\nuvicorn.run ensures your application runs on HTTPS using the generated key.pem and cert.pem.\nYou can now run the API using the following code on your terminal\npython main.py\n\n\n\nURL to the Model API displayed on terminal\n\n\nIn a production environment, it is recommended to use a reverse proxy server like Nginx to handle SSL termination and forwarded requests to the FastAPI application, to ensure better performance and security.\n\n\n\nEncrypting Sensitive Data\nEncryption is simply the encoding of sensitive information, such that even if the information were to leak, the content is secured and remains unknown, upon reaching its target destination the data is decoded. This is very useful in protecting sensitive data such as passwords, and only authorized users can decrypt the information using a decryption key. Here is a simple example of how encryption works.\n\nImport all necessary libraries and create an instance of the FastAPI class.\nfrom fastapi import FastAPI, HTTPException, Depends\nfrom pydantic import BaseModel\nfrom cryptography.fernet import Fernet\n\napp = FastAPI()\nNext is to generate key for encryption and decryption using the Fernet class.\n# Generate a key for encryption and decryption\nkey = Fernet.generate_key()\ncipher_suite = Fernet(key)\nCreate an Item model for receiving a text, and the EncryptedItem model for receiving the encrypted text.\n# Models\nclass Item(BaseModel):\n    plaintext: str\n\nclass EncryptedItem(BaseModel):\n    ciphertext: str\nCreate the encryption endpoint.\n@app.post(\"/encrypt/\", response_model=EncryptedItem)\nasync def encrypt_item(item: Item):\n    plaintext = item.plaintext.encode(\"utf-8\")\n    ciphertext = cipher_suite.encrypt(plaintext)\n    return {\"ciphertext\": ciphertext.decode(\"utf-8\")}\nThis takes the given item and encodes it to utf-8 , the cipher_suite key encrypts the plaintext to ciphertext which is a string of gibberish characters.\nCreate the decryption endpoint that decrypts the gibberish characters to the plaintext.\n# Decryption endpoint\n@app.post(\"/decrypt/\", response_model=Item)\nasync def decrypt_item(encrypted_item: EncryptedItem):\n    ciphertext = encrypted_item.ciphertext.encode(\"utf-8\")\n    try:\n        plaintext = cipher_suite.decrypt(ciphertext)\n        return {\"plaintext\": plaintext.decode(\"utf-8\")}\n    except Exception as e:\n        raise HTTPException(status_code=400, detail=\"Decryption failed\")\nThis endpoint takes the encrypted_item and encodes it to utf-8 before decrypting it to plaintext using the cipher_suite function. If the wrong ciphertext is provided, a 400 status code is returned with the detail \"Decryption failed\".\n\n\n\nEncrypting sensitive data in FastAPI\n\n\n\n\n\nRate Limiting and Throttling\nAnother way of securing APIs is by limiting the number of API calls made to the server. This is where rate limiting and throttling comes into play. Rate limiting is a technique of controlling the amount of incoming and outgoing traffic to or from a network, to prevent abuse and overloading of the server. While throttling on the other hand is temporarily slowing down the rate at which the API processes requests. To apply rate limiting and throttling to our previous example.\n\nEnsure you have installed the slowapi library, a library for implementing rate-limiting and throttling to APIs, and add the following new imports.\nfrom slowapi import Limiter, _rate_limit_exceeded_handler\nfrom slowapi.util import get_remote_address\nfrom slowapi.errors import RateLimitExceeded\nNext is to initialize the rate limiter.\nlimiter = Limiter(key_func=get_remote_address)\napp = FastAPI()\napp.state.limiter = limiter\napp.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\nApply the rate limiter to the /token/ endpoint using @limiter.limit(\"5/minute\") decorator, and the request: Request parameter in the login_for_access_token function.\n@app.post(\"/token\")\n@limiter.limit(\"5/minute\")\nasync def login_for_access_token(request: Request, form_data: OAuth2PasswordRequestForm = Depends()):\nAlso, apply a 10-minute rate limiting to the /predict endpoint. Change the parameter name in the predict function from request to prediction_request to avoid confusion with the new request: Request parameter.\n@app.post(\"/predict\")\n@limiter.limit(\"10/minute\")\nasync def predict(\n    request: Request,\n    prediction_request: PredictionRequest,\n    token: str = Depends(oauth2_scheme),\n    current_user: User = Depends(role_checker(\"admin\"))"
  },
  {
    "objectID": "posts/Securing ML APIs with FastAPI/index.html#conclusion",
    "href": "posts/Securing ML APIs with FastAPI/index.html#conclusion",
    "title": "Securing ML APIs with FastAPI",
    "section": "Conclusion",
    "text": "Conclusion\nYou can combine all these methods in your ML Model API to ensure maximum security as much as possible. In this article, you have learned how to implement various API security techniques in your FastAPI model such as authentication, authorization, input validation, sanitization, encryption, rate limiting, and throttling. If you want to dive deep into model deployment with FastAPI, here are some extra resources to keep you busy.\n\nML Model Deployment with FastAPI and Streamlit\nHow to Build an Image Classifier Application on Vultr Using FastAPI and HuggingFace\nHow to Build a WhatsApp Image Generator Chatbot with DALL-E, Vonage and FastAPI\nBuild an SMS Spam Classifier Serverless Database with FaunaDB and FastAPI\nImplementing Rate Limits in FastAPI: A Step-by-Step Guide\nImplementing Logging in FastAPI Applications\nML - Deploy Machine Learning Models Using FastAPI\nDeploying and Hosting a Machine Learning Model with FastAPI and Heroku\n\n\nNeed Help with Data? Let’s Make It Simple.\nAt LearnData.xyz, we’re here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we’ve got your back.\n📧 Shoot us an email at admin@learndata.xyz—let’s chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#introduction-to-low-codeno-code-elt-tools",
    "href": "posts/The Engineer's Guide to Low Code/index.html#introduction-to-low-codeno-code-elt-tools",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "Introduction to Low-code/No-code ELT Tools",
    "text": "Introduction to Low-code/No-code ELT Tools\nLow-code/No-code tools are tools used in building applications using drag-and-drop components, reducing or eliminating the amount of code used in development. It provides an interactive graphical user interface, making it easy for non-technical users to start developing.\nLow-code tools help developers quickly get started, writing little or no code. It helps professional developers quickly deliver applications, letting them focus on the business side of the applications. Some features of low-code tools are:\n\nIt offers an interactive user interface with high-level functions, eliminating the need to write complex code.\nIt is easy to modify or adapt.\nMostly developed for a specific use case or audience\nIt is easy to scale.\n\nNo-code, on the other hand, is a method of developing applications that allows non-technical business users; business analysts, admin officers, small business owners, and others, to build applications without the need to write a single line of code. Some features of no-code tools are:\n\nPurely visual development, that is, users develop using drag-and-drop interfaces.\nIt offers limited customization, users use what is provided in the tool and can’t extend its capabilities.\nSuited for non-technical individuals\nMostly suited for simple use cases\n\nLow-code tools require some basic coding skills, unlike no-code which does not require any programming knowledge. Low-code/no-code is based on visual programming and automatic code generation. The emergence of low-code/no-code was the fact that domain experts know how difficult it is to impart to the IT team, with the help of low-code/no-code, they can take part in the development process, coupled with the fact that the shortage of skilled developers and loads of workload on the IT professionals is another reason for the emergence of low-code/no-code."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#understanding-the-elt-process",
    "href": "posts/The Engineer's Guide to Low Code/index.html#understanding-the-elt-process",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "Understanding the ELT Process",
    "text": "Understanding the ELT Process\nELT stands for Extract, Load and Transform, which are the three stages involved in the process.\nIt’s a data preprocessing technique that involves moving raw data from a source to a data storage area, either a data lake or a data warehouse. Sources are either social media platforms, streaming platforms or any other place data is stored. During extraction, data is copied in raw form from the source location to a staging area, this is either in a structured or unstructured format from sources such as:\n\nSQL or NoSQL servers\nText and document files\nEmail\nWeb pages\n\nExtraction is either full which involves pulling all rows and columns from a particular data source, using an automated partial extraction with update notifications when data is added to the system, or incremental where update records are extracted as data is added into the data source.\nLoading involves moving the data from the staging area to the data storage area, either a data lake or a data warehouse. This process is automated, continuous and done in batch. One can load all the available data in the data source to the data storage area, load modified data from the source between certain intervals or load data into the storage area in real time.\nIn the transform stage, a pre-written schema is run on the data using SQL for analysis. This stage involves filtering, removing duplications, currency conversions, removal of encryptions, joining data into tables or performing calculations.\nUnlike in ETL where raw data is transformed before being loaded into a destination source, in ELT the data is loaded into a destination source before it’s transformed for analysis as needed.\n\nBy allowing transformation to occur after loading, data is moved quickly to the destination for availability. Because data is transformed after arrival at the destination, ELT allows the data recipient to control data manipulation. This ensures that coding errors when transforming do not affect another stage. ELT uses the powerful big data warehouse and lakes allowing transformation and scalable computation.\nData warehouses use MPP architecture (Massively Parallel Processing), and data lake processes also support the application of schema or transformation models as soon as the data is received, this makes the process flexible, especially for large amounts of data. ELT is suited for data that are in cloud environments, this provides a seamless integration since ELT is cloud-native and allows for the continuous flow of data from sources to storage destinations, hence making them on demand.\nELT is used for instant access to huge volumes of data, for example in IOT, it loads data from IOT devices making it readily available for data scientists or analysts to access raw data and work collaboratively.\nDespite its advantages, ELT has some of its limitations:\n\nData privacy is a challenge in ELT, this is because when transferring data, a breach can occur from the source to the destination storage which poses security and privacy risks.\nWhile in transit, if care is not taken sensitive information is exposed, and extra security measures have to be taken to ensure the confidentiality of the data.\nELT handle large volumes of data, making it computationally intensive, leading to delays in gaining insights.\nBecause the data is loaded into the data storage area without transformation, it makes it challenging to transform the data when compared with ETL. This requires strong technical and domain knowledge of the data scientist and analyst who will write the queries transforming the data."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#low-codeno-code-elt-tools",
    "href": "posts/The Engineer's Guide to Low Code/index.html#low-codeno-code-elt-tools",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "Low-Code/No-Code ELT Tools",
    "text": "Low-Code/No-Code ELT Tools\nLow-code /No-code ELT are used to extract, load and transform data. Here are some of the benefits of using low-code/no-code ELT tools:\n\nIt is easier compared to writing scripts to automate the ELT process.\nIt makes development faster, developers can spend more time solving business problems instead of fixing bugs that result from writing lines of code.\nIt increases automation, a lot of processes that would have to be set up manually are handled automatically, such as monitoring, logging, setting notifications when there is a problem with the pipeline and so on.\nMost ELT tools support a lot of data connectors, making it easy for an organization to connect to any data source with provisions to create custom connectors.\nELT tools lower the cost of building an ELT pipeline by ensuring the whole process is conducted with a single tool from start to finish.\nThey provide better customer experience, by ensuring that even business folks are involved in building the ELT pipeline.\n\nThere are various low-code/no-code  ELT tools out there, each with its strengths and limitations, here are some you can consider for building an ELT pipeline:\n\nAirbyte\nAirbyte is an open-source data movement platform with over 300+ open-source structured and unstructured data sources. Airbyte is made up of two components; platform and connectors. The platform provides all the services required to configure and run data movement operations while the connectors are the modules that pull data from sources or push data to destinations. Airbyte also has a connector builder, a drag-and-drop interface and a low-code YAML format for building data source connectors.\nAirbyte has two plans, Cloud and Enterprise, but it is free to use if you can self-host the open-source version. Airbyte offers real-time and batch data synchronization, tracks the status of data sync jobs, monitors the data pipeline and views logs with a notifications system in case things go wrong. Airbyte also allows you to add custom transformations using dbt.\n\n\nFivetran\nFivetran is an automated data movement platform used for ELT. It offers automated data movement, transformations, security and governance with over 400+ no-code source connectors. You can use the function connector in Fivetran to write the cloud function to extract the data from your source, while it takes care of loading and processing the data into your destination. Fivetran gives you options to manage Fivetran connectors, to have more control over your data integration process runs. It offers a “pay-as-you-use” model, with five Free pricing plans, Starter, Standard, Enterprise and Business Critical.\n\n\nIntegrate.io\nFormerly known as Xplenty, this is another low-code platform for data movement, unlike the others it doesn’t have many connections, it offers both low-code ETL, Reverse ETL and an ELT platform. Pricing on Integrate.io is based on data volume and increases as the number of rows in your data increases. It offers both Starter, Professional and Enterprise plans, with an extra charge for additional connectors.\n\n\nStitch\nStitch is also another data movement platform owned by Qlik. It replicates historical data from your database for free and allows you to add multiple user accounts across your organization to manage and authenticate data sources. It is extensible and has several hundreds of connectors. It offers various pricing models such as standard, advanced and premium which are all charged based on data volume.\n\n\nMatillion\nMatillion is another ELT platform, that uses LLM components to unlock unstructured data and offers custom connectors to build your connectors. It has a complete pushdown architecture supporting SQL, Python, LLMs, Snowflake, Databricks, AWS, Azure and Google. It supports both low-code and no-code for both programmers and business users. You can create an ELT process using either SQL, Python or dbt. It also gives you Auto-Documentation to generate pipeline documentation automatically. It offers three pricing models Basic, Advanced and Enterprise which you can pay only for pipelines run and not, those on development or sampling."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#key-features-of-low-codeno-code-elt-tools",
    "href": "posts/The Engineer's Guide to Low Code/index.html#key-features-of-low-codeno-code-elt-tools",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "Key Features of low-code/no-code ELT Tools",
    "text": "Key Features of low-code/no-code ELT Tools\n\nAvailability of Connectors\nELT connectors are components of an ELT tool that allow the tool to connect to a data source and make it possible for extraction and loading. When trying to go for ELT tools it is important to go for the ELT tool with the highest number of connectors, that is why an organization needs to have a list of all the data sources it uses, this will let the organization know the ELT tool to choose based on organizational data sources, most importantly connectors for revenue-generating applications. Let’s say your organization uses Zoho as a CRM. It’s important to compare various ELT tools with a connector for Zoho and see which offers the best service at the most affordable price.\n\n\nDrag-and-Drop Interfaces\nLow-code/no-code ELT tools offer an intuitive user interface with a drag-and-drop functionality, allowing non-technical users to perform ELT by dragging and dropping components without encountering any challenges. This makes the user experience seamless and users can focus on the application’s business logic. This reduces the workload on the IT team, allowing the organization’s domain experts to partake in the development process.\n\n\nAutomated Scheduling\nDue to their ability to schedule extracting and loading, automating the ELT process is very simple. This can involve creating tasks that can be run using a specified SQL schema at specific intervals. One can easily automate documentation, document process automation, and show the manipulations occurring to data from source to data storage, enabling organizations to save time and costs.\n\n\nData Transformation Capabilities\nLow-code/no-code ELT tools manage dependencies when transforming data, this is essential when you have multiple transforms depending on each other. They support the DAG(directed acyclic graph) workflow to manage the transformation dependencies after conducting a transform job using SQL on the data, reading the transformation query and calculating a dependency workflow.\n Another important aspect is their support for incremental loading, where only the differences in data are loaded from source to destination. For example Let’s consider the case of a retail store that tracks its sales data, without an incremental approach the daily sales report would involve extracting sales data from the sales table, which contains millions of records, aggregating the data to calculate the total sales, revenue and units sold for each product, store and day, and load the aggregated data into a new table called sales_daily.\nThis is a resource-intensive approach as the system needs to process all the sales data every time the report is generated. Using an incremental approach, whenever a new sale is recorded in the sales table, a trigger or a background process is used to update the sales_daily table with new data for that day and store.  Whenever every report is generated, only the data for the latest day is extracted from the sales_daily table, which is a much smaller dataset than the entire sales table. The incremental approach helps in improving performance, cost and scalability.\n\n\nMonitoring and Alerting\nMonitoring and Alerting is another important feature of a low-code/no-code ELT tool because It allows you to detect anomalies, bottlenecks and failures in the workflow, provide continuous surveillance and monitor resource utilization. Key important metrics it monitors are; latency, throughput, error rates, resource utilization and pipeline lag. They also give threshold alerts, detect anomalies, and escalate alerts to SMS or phone calls.\nImagine a manufacturing company that collects sensor data from various manufacturing plants across facilities. If analysis, reveals one machine is giving higher vibration levels than others. The ELT tool anomaly detection algorithm should trigger an alert which will prompt the maintenance team to investigate. They might identify a worn-out bearing component and schedule proactive maintenance, preventing equipment failure and uninterrupted production."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#evaluating-low-codeno-code-elt-tools",
    "href": "posts/The Engineer's Guide to Low Code/index.html#evaluating-low-codeno-code-elt-tools",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "Evaluating low-code/no-code ELT Tools",
    "text": "Evaluating low-code/no-code ELT Tools\nThere are various things to consider when choosing a low-code/no-code  ELT tool.\n\nConnectors: When selecting an ELT tool, ensure it supports connections to various data sources and know how many SAAS integrations are available and how effectively the tool can connect to organizational data sources.\nRoadmap: Another important factor, is if the ELT tool can handle the company’s rapidly growing data. Is it responsive and scalable? This will give the organization an idea of the ELT tool’s sustainability in the long run.\nPricing: How does the ELT tool charge? Is it by data flows or data volume and are the features it offers worth its pricing? Some ELT tools offer more connectors at affordable pricing than others.\nSupport: Look for an ELT tool with available customer support, this is very crucial especially when things break. The ELT tools should also offer good documentation that is easy to understand by technical and non-technical users. An online community around the tool is also a plus, users can relate with fellow users and serve as support for each other.\nSecurity: How does the ELT tool prioritise security? Are organizational data safe and is it regulatory compliant with GDPR, SOC2, HIPPA, and other relevant regulations? These are important security questions to look for when selecting an ELT tool. It is also important that the organization knows, how the tool handles privacy and authentication.\nEase of use: A low-code/no-code ELT tool that is user-friendly and easy to customize is another priority to look out for, it makes the process of creating ELT pipelines easy and non-technical for business folks.\nMaintenance: When choosing a low-code/no-code ELT tool, it’s important to know how easy it is to fix problematic data sources, and if it gives informative logs if an execution fails. It is also important to know what skills are required, by team members to keep the ELT process running smoothly."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#implementing-low-codeno-code-elt-tools",
    "href": "posts/The Engineer's Guide to Low Code/index.html#implementing-low-codeno-code-elt-tools",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "Implementing Low-code/No-code ELT Tools",
    "text": "Implementing Low-code/No-code ELT Tools\n\nPlanning the ELT Pipeline\nBefore building an ELT pipeline, you need to get the data from your source using an ELT tool like Airbyte and decide the data warehouse to use, either Google BigQuery, AWS Redshift or Snowflake. Next is to transform the data using dbt, R, Python or a no-code transformation layer such as Boltic, then consider the BI tool for presenting the data to end users.\n\n\nConfiguring Data Source and Destination Connections\nLet’s say for example using a REST API as a data source, Airbyte as the ELT tool and Amazon S3 bucks as the data destination. Create a new S3 bucket in the AWS console, in the bucket, create a folder to store data from the ELT operation in Airbyte. Create another folder in the previous folder to store the data you will extract from the REST API.\nNext, you will configure both the source and the destination connector, and connect the source and the destination. Ensure any API you use, there is a connector for it on Airbyte. If you don’t see a connector for your data source, use Airbyte’s CDK(Connector Development Kit) to create a custom connector.\nNext, you go to AWS S3 to configure the destination connector to connect Airbyte with the destination data lake, after successfully configuring both the source and the destination connector, and passing all the tests. You can now configure the ELT connection between source and destination.\n\n\n\nDesigning Data Transformation Workflows\nBefore transforming your data, you need to explore and understand it, this involves looking at your entity relationship diagrams to see how the data relate with each other, identifying missing values or misleading column names and performing a summary analysis of the data.\nWhile exploring, you might understand what is wrong with your data, and decide to perform your transformation process such as correcting misleading columns, renaming fields appropriately, adding business logic or joining data. The transformation you apply depends on what you have explored in the data. You can use tools like dbt, SQL, Python or R to transform your data or go no-code with tools like Boltic. At this stage,  test your data to meet business standards.\nFinally, you document your transformation process explaining the data model, key fields and metrics making the documentation easy for non-technical users to understand.\n\n\nScheduling and Automating ELT Processes\nBefore releasing the data to end users, you need to push it into a production environment in the data warehouse, these product-ready tables are what the BI analysis will query. With time you will need to update and refresh the data to meet the business needs, using a scheduler or orchestrator.  Using the job scheduler, you can use tools like dbt Cloud or Shipyard to create data transformations and tests within a collaborative IDE.\n\n\nMonitoring and Maintaining the ELT Pipeline\nMonitoring is important to identify bugs in data pipelines, optimise pipelines and gain valuable insights. These ELT tools provide visualisations, logging and observability systems to analyse latency, error rates and data throughput in your ELT pipeline. Most low-code/no-code provide all these out of the box, you will receive custom notifications when data problems occur allowing you to improve the quality of your data sets."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#best-practices-for-low-codeno-code-elt",
    "href": "posts/The Engineer's Guide to Low Code/index.html#best-practices-for-low-codeno-code-elt",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "Best Practices for low-code/no-code ELT",
    "text": "Best Practices for low-code/no-code ELT\n\nEnsuring Data Quality and Integrity\n\nData quality and requirements: The first step before conducting an ELT is to specify the data quality and requirements. These should specify the data accuracy, completeness, consistency, timeliness, validity, and uniqueness. This also includes details of the sources, end users and data quality metrics. This will help to understand the quality of data to expect and how to achieve it.\nValidation: The next step is to validate the data quality using the ELT tools before loading them into the warehouse, to reduce the amount of bad-quality data that gets into the data warehouse.\nData quality checks: In this step, ensure you use the ELT tools to implement quality checks on the data in the data warehouse before making it available to end users so that the data is consistent, complete and correct(3Cs).\nData quality monitoring and auditing: The next step is to monitor and audit the data quality as new data gets updated into the system, this is to resolve issues of data quality when they arise. ELT tools have various tools to get reports, alerts and logs on data quality. This ensures the successful maintenance of the data in the long run.\nData quality documentation and communication: Next is to give a report explaining to the end users or stakeholders the quality of the data, this report should contain the processes, rules, metrics and issues with the data. Doing this ensures trust and transparency of the data quality.\nReview and update the data quality: Constantly use the information from the ELT tool logging system to update the data quality, from time to time. By doing this, you are ensuring that the data remains relevant and meets the organization’s requirements.\n\n\n\nHandling Data Governance and Compliance\nIt is important to define clear roles and objectives for various stakeholders and use data governance tools to automate the tasks of data validation, cleansing, standardization, profiling, auditing, and reporting. Provide a data catalogue of the data assets and other data quality indicators and recommend a staging area for the source data before moving to the data warehouse. Finally, using a schema-on-read approach allows users to apply different views to the same data, based on their needs.\n\n\nIntegrating low-code/no-code ELT with Existing Systems\nDue to the visually driven approach of low-code/no-code ELT tools, you must thoroughly assess your existing data sources, formats and structures to identify potential compatibility issues and develop appropriate mapping strategies. Check, if a connector for your data source exists before building a custom connector.\n\n\nScaling and Optimizing ELT Workflows\nOne of the reasons for scaling and optimizing an ELT process is to reduce data loss and ELT run downtime. The following are ways to optimize an ELT workflow effectively:\n\nParallelize Data Flow: Simultaneous data flow saves more time than sequential data flow, you can load a group of unrelated tables from your data source to the data warehouse, by grouping the tables into a batch and running each batch simultaneously, instead of loading the tables one by one.\nApplying data quality checks: You should ensure each data table is tested with predefined checks such as schema-based to ensure that when a test fails, the data is rejected and when it passes, it produces an output.\nCreate Generic Pipelines: Generic pipelines ensure team members reuse a code without developing one from scratch. Practices like parameterizing the values can ease the job, when one wants to use a code/pipeline, they change the values of the parameters such as database connections.\nUse of streaming instead of batching: Streaming data from the source to the data destination is the best approach, this ensures the system is up to date whenever new data is added to allow end users access to recent data."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#future-of-low-codeno-code-elt",
    "href": "posts/The Engineer's Guide to Low Code/index.html#future-of-low-codeno-code-elt",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "Future of low-code/no-code ELT",
    "text": "Future of low-code/no-code ELT\nLow-code/no-code ELT tools have become popular, offering cloud-based options and reduced infrastructure costs. Most of them now have pre-built connectors covering most data sources and automated scheduling. Advanced low-code/no-code ELT tools now have data mapping and transformation capabilities using machine learning to detect data patterns and transformation. Some low-code/no-code tools now integrate data governance features such as profiling, data lineage tracking and automated data validation rules to ensure data integrity. These innovations aim to simplify data integration and empower non-technical users.\nEven with the innovation and simplicity low-code/no-code ELT tool offers, it has some of its limitations:\n\nLow-code/no-code ELT make it difficult to collaborate as a team, unlike a code-heavy pipeline with version control and collaborative features.\nSome low-code/no-code ELT tools give poor developer experience which can hamper productivity, such as bad and non-intuitive UI design and limited customization options.\nAnother challenge with low-code/no-code ELT is the security of the applications built on them, even though the platform is secure. This is an issue since the tool was developed for non-technical individuals with no expertise in security best practices.\nThey do not offer the source code to pipelines built and this is a challenge when an organization wants to migrate to another platform, they are forced to develop from scratch on another platform or stick with that same platform."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#conclusion",
    "href": "posts/The Engineer's Guide to Low Code/index.html#conclusion",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "Conclusion",
    "text": "Conclusion\nWhy learn data engineering since anyone can now use low-code/no-code tools? Data engineering is not just about building pipelines but also solving business problems such as designing schemas. It’s important that the data you give to your end users can answer their questions. In summary, developing your soft skills is more important to make you stand out.\nLow-code/no-code have a lot of abstraction which when things go wrong it’s difficult to find out, some organizations will not want to give out the control they have over their data pipeline by hiding away some of the pipeline core functionality using a low-code/no-code ELT tool."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#references",
    "href": "posts/The Engineer's Guide to Low Code/index.html#references",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "References",
    "text": "References\n\nUnleashing the power of ELT in AWS using Airbyte\nHow do you ensure data quality and governance in an ELT process?\n6 Best Practices to Scale and Optimize Data Pipelines\n\n\nNeed Help with Data? Let’s Make It Simple.\nAt LearnData.xyz, we’re here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we’ve got your back.\n📧 Shoot us an email at admin@learndata.xyz—let’s chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to learndata.xyz",
    "section": "",
    "text": "Are you ready to transform your data into actionable insights? At learndata.xyz, we specialize in turning complex data into clear, powerful solutions.\n\n\n\nR & Python Consulting: Harness the full potential of these leading data science languages.\nStatistical Analysis: Make informed decisions with robust statistical methodologies.\nShiny Web Applications: Create interactive, data-driven web apps that captivate and inform.\n\n\n\n\n\nExpert Team: Our statisticians and data scientists bring years of experience to every project.\nTailored Solutions: We craft custom solutions that address your unique challenges.\nCutting-Edge Techniques: Stay ahead with the latest in machine learning and data visualization.\nEducation-Focused: We don’t just deliver results; we empower your team with knowledge.\n\n\n\n\n\nData Analysis and Visualization\nMachine Learning and Predictive Modeling\nShiny Web Application Development\nStatistical Consulting\nTraining and Workshops\nCode Review and Optimization\n\n\n\n\nContact Us Now to discuss how we can elevate your data game.\nExplore Our Services to learn more about our offerings."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html",
    "href": "posts/The Engineer's Guide to Low Code/index.html",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "",
    "text": "subscribe.html"
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "",
    "text": "© 2024 Guiding Tech Media | About | Contact | Advertise | Privacy | Terms of Service"
  },
  {
    "objectID": "posts/How_to_conduct_unit_tests_in_Python_Shiny_with_Pytest/index.html",
    "href": "posts/How_to_conduct_unit_tests_in_Python_Shiny_with_Pytest/index.html",
    "title": "How to Conduct Unit Tests in Python Shiny with Pytest",
    "section": "",
    "text": "You are building a shiny application that processes and visualizes financial data, and a calculation function within the app for calculating financial ratios is incorrect; this could lead to misleading results and poor user decision-making. Unit tests allow you, the developer, to isolate and validate these functions independently, catch errors early, and ensure that changes in other application parts do not unintentionally break a function’s functionality.\nUnit testing, also known as module or component testing, is the process of testing smaller components of an application to ensure they work as expected. Because unit tests are applied to various components and modules, they make it easy to identify and fix bugs in an application. They are also easy to write and don’t require any input or output from your application for you to run them. In this article, you will learn about unit tests and how you can effectively implement unit tests in your Python Shiny applications."
  },
  {
    "objectID": "posts/How_to_conduct_unit_tests_in_Python_Shiny_with_Pytest/index.html#separating-concerns",
    "href": "posts/How_to_conduct_unit_tests_in_Python_Shiny_with_Pytest/index.html#separating-concerns",
    "title": "How to Conduct Unit Tests in Python Shiny with Pytest",
    "section": "Separating Concerns",
    "text": "Separating Concerns\nIf you have ever been in a situation where you wrote some lines of code, only to come back months later and find it difficult to fix a bug in a component because you don’t know which component controls which. You can solve this problem by writing good unit tests for your application, and you need to separate concerns to achieve that. That is why, in Python Shiny applications, it’s important to separate business logic from reactive logic to ensure that the application is maintainable and scalable in the long run.\nFor example, look at the financial application you are building; the business logic is the computations and data processing of the app’s user interface, such as data validation, financial calculations, and error handling. The reactive logic is user inputs and reactive outputs, such as file uploads and selecting the financial ratio to compute, display of calculated ratios, generating and updating graphs, etc. Separating the business logic from reactive logic ensures:\n\nAccuracy and Reliability: By separating the business logic from reactive logic. You can rigorously test the business logic to ensure the calculations are correct.\nMaintainability: In the future, you can change business logic without affecting the reactive logic and breaking the user interface.\nTestability: Isolating each calculation in the business logic ensures that every function is tested independently and its value is computed before it is used in the Shiny environment.\nReusability: One might want to use the business logic in one or more contexts or applications; this reduces redundancy and enhances code efficiency."
  },
  {
    "objectID": "posts/How_to_conduct_unit_tests_in_Python_Shiny_with_Pytest/index.html#choosing-a-python-unit-testing-framework",
    "href": "posts/How_to_conduct_unit_tests_in_Python_Shiny_with_Pytest/index.html#choosing-a-python-unit-testing-framework",
    "title": "How to Conduct Unit Tests in Python Shiny with Pytest",
    "section": "Choosing a Python Unit Testing Framework",
    "text": "Choosing a Python Unit Testing Framework\nMany Python unit testing frameworks exist, but you need to consider your project requirements, budget, and tech stack before choosing one. I recommend open-source unit testing frameworks, which are usually free and have strong, supportive communities with many resources to learn from.\nMany Python unit testing frameworks are out there, such as unitest, behave, robot, etc. However, one of the most popular and widely used is the Pytest framework. Pytest is simple, scalable, and powerful, especially when working on projects with external dependencies."
  },
  {
    "objectID": "posts/How_to_conduct_unit_tests_in_Python_Shiny_with_Pytest/index.html#build-a-demo-application",
    "href": "posts/How_to_conduct_unit_tests_in_Python_Shiny_with_Pytest/index.html#build-a-demo-application",
    "title": "How to Conduct Unit Tests in Python Shiny with Pytest",
    "section": "Build a Demo Application",
    "text": "Build a Demo Application\nLet me demonstrate how Pytest works by building a simple sales dashboard based on supermarket sales data. First, ensure you have installed Shiny and Pytest.\npip install shiny pytest\nPaste the following code into a file utilis.py.\nimport pandas as pd\n\n# Business Logic\ndef fetch_data():\n    data = pd.read_csv(\"supermarket_sales.csv\")\n    return data\n\ndef total_profit(data):\n    profit = sum(data[\"gross income\"])\n    return profit\n\ndef total_revenue(data):\n    revenue = sum(data[\"cogs\"])\n    return revenue\nThis is our business logic for the Python Shiny application. To keep it simple, let’s restrict it to three functions.\n\nfetch_data() to load the sales data.\ntotal_profit() to calculate the total profit.\ntotal_revenue() to calculate the total revenue.\n\nNext, let’s build the application’s user interface. Copy and paste the following code into a separate file, main.py.\nfrom shiny import reactive\n\nfrom shiny.express import input, render, ui\n\nfrom utilis import fetch_data, total_profit, total_revenue\n\nwith ui.card():\n    ui.h2(\"Sales Dashboard\"),\n    ui.input_select(\n        \"calculation\", \"Select calculation:\", choices=[\"Total Profit\", \"Total Revenue\"]\n    )\n    ui.input_action_button(\"compute\", \"Compute!\")\n\nwith ui.card():\n    with ui.h1():\n        @render.text\n        @reactive.event(input.compute)\n        def result():\n            if input.calculation() == \"Total Profit\":\n                value = total_profit(fetch_data())\n            else:\n                value = total_revenue(fetch_data())\n\n            return value\nRun this using the command.\nshiny run --reload main.py\n\n\n\nLive Application. Image by Author\n\n\nIf you are building a toy project, you could deploy the application straightaway, but in production, you need to test for each and every component, especially your business logic. If you run the tests in the future and it fails, the unit test will help you track which function has a bug.\n\nUsing Pytest with Shiny\nWhen writing tests with Pytest, your test files must start with either the prefix test_ prefix or end with the suffix_test. This ensures that Pytest identifies the files and functions to run the unit tests.\nCopy and paste the following into a file,\ntest_main.py.\nfrom utilis import fetch_data, total_profit, total_revenue\nimport pandas as pd\n\n# Business Logic\ndata = fetch_data()\n\ndef test_total_profit():\n    assert total_profit(data) == 15379.369\n\ndef test_total_revenue():\n    assert total_revenue(data) == 307587.38\n\nFirst of all, we have to import the data that loads the dataset and store it in the variable data\nThe test_total_profit() function tests if the total_profit function is giving the correct profit sum.\nThe same applies to the test_total_revenue() function, which must be checked to ensure that it returns the correct revenue sum.\n\nTo run the test, call pytest on your terminal.\npytest \n\n\n\nTwo unit tests successfully passed. Image by Author.\n\n\nChange the test_total_profit value in the test function to 16000 and run the test again.\ndef test_total_profit():\n    assert total_profit(data) == 16000\n\n\n\nOne successful test passed, and one failed . Image by Author.\n\n\nYou will notice that not only did the test fail, but it also pointed out the function where it failed. This is very handy when your application has many functions.\nNow, revert the test_total_profit()to its original value, change the name to total_test_profit(), and run the test.\n\n\n\nOne unit test ran and passed. Image by Author.\n\n\nYou can see that only one of the tests ran, and this is due to the absence of the test_ prefix or _test suffix."
  },
  {
    "objectID": "posts/How_to_conduct_unit_tests_in_Python_Shiny_with_Pytest/index.html#using-pytest-with-shiny",
    "href": "posts/How_to_conduct_unit_tests_in_Python_Shiny_with_Pytest/index.html#using-pytest-with-shiny",
    "title": "How to Conduct Unit Tests in Python Shiny with Pytest",
    "section": "Using Pytest with Shiny",
    "text": "Using Pytest with Shiny\nWhen writing tests with Pytest, your test files must start with either the prefix test_ prefix or end with the suffix_test. This ensures that Pytest identifies the files and functions to run the unit tests.\nCopy and paste the following into a file,\ntest_main.py.\nfrom utilis import fetch_data, total_profit, total_revenue\nimport pandas as pd\n\n# Business Logic\ndata = fetch_data()\n\ndef test_total_profit():\n    assert total_profit(data) == 15379.369\n\ndef test_total_revenue():\n    assert total_revenue(data) == 307587.38\n\nFirst of all, we have to import the data that loads the dataset and store it in the variable data\nThe test_total_profit() function tests if the total_profit function is giving the correct profit sum.\nThe same applies to the test_total_revenue() function, which must be checked to ensure that it returns the correct revenue sum.\n\nTo run the test, call pytest on your terminal.\npytest \n\n\n\nTwo unit tests successfully passed. Image by Author.\n\n\nChange the test_total_profit value in the test function to 16000 and run the test again.\ndef test_total_profit():\n    assert total_profit(data) == 16000\n\n\n\nOne successful test passed, and one failed . Image by Author.\n\n\nYou will notice that not only did the test fail, but it also pointed out the function where it failed. This is very handy when your application has many functions.\nNow, revert the test_total_profit()to its original value, change the name to total_test_profit(), and run the test.\n\n\n\nOne unit test ran and passed. Image by Author.\n\n\nYou can see that only one of the tests ran, and this is due to the absence of the test_ prefix or _test suffix."
  },
  {
    "objectID": "posts/How_to_conduct_unit_tests_in_Python_Shiny_with_Pytest/index.html#python-unit-testing-best-practices",
    "href": "posts/How_to_conduct_unit_tests_in_Python_Shiny_with_Pytest/index.html#python-unit-testing-best-practices",
    "title": "How to Conduct Unit Tests in Python Shiny with Pytest",
    "section": "Python Unit Testing Best Practices",
    "text": "Python Unit Testing Best Practices\nHere are some best practices for conducting unit tests on your Python Shiny applications.\n\nOne Assertion Per Test: Avoid combining different assertions in a single test function. This will make it difficult to pinpoint issues when a test fails. Always make sure each of your test functions has a single assertion.\nMaintain Independence: Make sure your test functions are not reliant on the outcome of other test functions.\nDocumentation: Document your tests to make them easier to maintain and update in the future. This will be handy for you or others who want to work on the project.\nCI Integration: You will notice earlier that we ran the test manually on the terminal; you can use continuous integration systems like GitHub Actions to automate the test periodically and even send alerts when something goes wrong.\nMaintenance: Don’t just write a test and forget about it. Regularly revisit the test and update it as you update your application codebase.\nSimplicity: Keep your test functions simple, so each test should test for a function in your application.\nProper Naming Conventions: Adopt proper and descriptive naming conventions. Naming your test functions right will avoid overcrowding your test files with comments.\nPay Attention to Code Coverage: High code coverage reduces the chances of having bugs in your application. When writing your test function, always account for every possible condition and error you might expect."
  },
  {
    "objectID": "posts/How_to_conduct_unit_tests_in_Python_Shiny_with_Pytest/index.html#conclusion",
    "href": "posts/How_to_conduct_unit_tests_in_Python_Shiny_with_Pytest/index.html#conclusion",
    "title": "How to Conduct Unit Tests in Python Shiny with Pytest",
    "section": "Conclusion",
    "text": "Conclusion\nWriting tests, especially unit tests, might seem daunting initially, but it pays off in the long run. When a codebase grows large, it becomes difficult to debug, and these tests make it easy to pinpoint the location of bugs. As a developer who develops data applications, adopting sound software engineering principles, such as testing, is a good skill. In this article, you learned about Python unit testing and how to apply Pytest to your Python Shiny application. If you want to read more, here are some additional resources.\n\nA Beginner’s Guide to Unit Tests in Python (2023)\nPython’s unittest: Writing Unit Tests for Your Code\nA Gentle Introduction to Unit Testing in Python\nTop 9 Python Unit Testing Frameworks In 2024\nAssert statement for unit testing\nUnit testing: Python Shiny documentation.\n\n\nNeed Help with Data? Let’s Make It Simple.\nAt LearnData.xyz, we’re here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we’ve got your back.\n📧 Shoot us an email at admin@learndata.xyz—let’s chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "navs/courses.html#advanced-data-wrangling-with-pandas",
    "href": "navs/courses.html#advanced-data-wrangling-with-pandas",
    "title": "Courses",
    "section": "Advanced Data Wrangling with Pandas",
    "text": "Advanced Data Wrangling with Pandas"
  },
  {
    "objectID": "navs/courses.html#building-interactive-shiny-web-apps-with-r-programming",
    "href": "navs/courses.html#building-interactive-shiny-web-apps-with-r-programming",
    "title": "Courses",
    "section": "Building Interactive Shiny Web Apps with R Programming",
    "text": "Building Interactive Shiny Web Apps with R Programming"
  },
  {
    "objectID": "navs/courses.html#data-wrangling-and-exploratory-data-analysis-with-r",
    "href": "navs/courses.html#data-wrangling-and-exploratory-data-analysis-with-r",
    "title": "Courses",
    "section": "Data Wrangling and Exploratory Data Analysis with R",
    "text": "Data Wrangling and Exploratory Data Analysis with R"
  },
  {
    "objectID": "navs/courses.html#power-bi-dax-practice-test-and-solutions",
    "href": "navs/courses.html#power-bi-dax-practice-test-and-solutions",
    "title": "Courses",
    "section": "Power BI DAX Practice Test and Solutions",
    "text": "Power BI DAX Practice Test and Solutions"
  },
  {
    "objectID": "posts/E2E/index.html",
    "href": "posts/E2E/index.html",
    "title": "End-to-end Testing with Playwright and Python Shiny",
    "section": "",
    "text": "When an application’s code base grows large, it becomes difficult to maintain. Anytime a change is made in the code, some app functionality can break and go unnoticed. This is why writing tests in your Python Shiny applications is essential.\nOne such test is end-to-end tests (E2E). End-to-end tests help mimic user interactions on an application, such as clicking buttons, file uploads, and browser variations, to ensure that the application’s user interface is working as expected.\nThis article will teach you how to conduct end-to-end tests on your Python Shiny applications using Playwright, an open-source automation framework for testing web applications."
  },
  {
    "objectID": "posts/E2E/index.html#what-is-playwright",
    "href": "posts/E2E/index.html#what-is-playwright",
    "title": "End-to-end Testing with Playwright and Python Shiny",
    "section": "What is Playwright",
    "text": "What is Playwright\nPlaywright is an automation framework used to test web applications across different browsers. It automates browser interactions with an application, just as a user would if they were to use the application. Playwright was built on Node.js but is also available as a Python library.\n\nBenefits of End-to-end testing with Playwright.\n\nWide Support and Compatibility: Playwright works with almost all operating systems and major browsers and has libraries in major programming languages.\nResilience and Dynamic Wait Times: Some elements or interactions can take time to load. Playwright features auto-wait for these elements or interactions, hence preventing unnecessary timeouts.\nTest Isolation: Playwright simulates a browser environment in a new tab. This allows you to run multiple tests on different sessions of the applications."
  },
  {
    "objectID": "posts/E2E/index.html#a-basic-python-shiny-example",
    "href": "posts/E2E/index.html#a-basic-python-shiny-example",
    "title": "End-to-end Testing with Playwright and Python Shiny",
    "section": "A Basic Python Shiny Example",
    "text": "A Basic Python Shiny Example\nLet’s build a simple web application using the affairs dataset from the statsmodels library. Ensure you have the following libraries installed.\n\nstatsmodels\npandas\nshiny\n\npip install statsmodels pandas shiny\nIn your project directory, create a new file called utilis.py. This file will contain the function that filters the affairs dataset based on an individual’s age and number of children.\nimport statsmodels.api as sm\n\n# Load the dataset from statsmodels (Affair dataset on extramarital affairs)\ndata = sm.datasets.fair.load_pandas().data\n\n# Function to filter data based on inputs\ndef filter_data(age_range, kids_range):\n    return data[(data[\"age\"] >= age_range) & (data[\"children\"] <= kids_range)]\nCreate another file, main.py, that will contain the code for the Python Shiny application.\nfrom shiny.express import input, render, ui\nfrom utilis import filter_data\n\n# Add UI elements\nwith ui.card():\n    ui.h2(\"Extramarital Affairs Dataset (Fair)\"),\n    ui.input_slider(\"age\", \"Age Filter\", 15, 60, 20)\n    ui.input_slider(\"kids\", \"Number of Kids\", 0, 5, 2)\n\n# Table output\nwith ui.card():\n\n    @render.table\n    def filtered_table():\n        filtered = filter_data(input.age(), input.kids())\n        return filtered.head(10)  # Display first 10 rows\nThe code above creates a shiny application that takes in two user inputs: age and kids. This now returns the first ten rows of the filtered dataset. Run the following code to view the application\nshiny run --reload main.py\n\n\n\nLive Python Shiny application. Image by Author"
  },
  {
    "objectID": "posts/E2E/index.html#integrating-playwright-with-python-shiny",
    "href": "posts/E2E/index.html#integrating-playwright-with-python-shiny",
    "title": "End-to-end Testing with Playwright and Python Shiny",
    "section": "Integrating Playwright with Python Shiny",
    "text": "Integrating Playwright with Python Shiny\nTo use Playwright, you must install the Playwright Python library and Pytest.\npip install pytest pytest-playwright\nJust like unit tests, all end-to-end test files must have a test prefix or suffix. Create a test file test_app.py, and paste the following code to see if the table output in the application appears as expected.\nfrom shiny.playwright import controller\nfrom shiny.run import ShinyAppProc\nfrom playwright.sync_api import Page\nfrom shiny.pytest import create_app_fixture\nfrom utilis import filter_data\n\napp = create_app_fixture(\"./main.py\")\n\ntest_data = filter_data(28, 4).head(10)\n\nn_row = test_data.shape[0]\nn_col = test_data.shape[1]\ncolumns_names = [*test_data.columns]\n\ndef test_table(page: Page, app: ShinyAppProc):\n    page.goto(app.url)\n    table = controller.OutputTable(page, \"filtered_table\")\n    slider_1 = controller.InputSlider(page, \"age\")\n    slider_2 = controller.InputSlider(page, \"kids\")\n    slider_1.set(\"28\")\n    slider_2.set(\"4\")\n    table.expect_column_labels(columns_names)\n    table.expect_ncol(n_col)\n    table.expect_nrow(n_row)\nHere is a breakdown of the above code:\n\nFirst of all, the controller module is imported. This controls the shiny components and mimics browser interactions on the Python shiny application.\npage is an instance of the Page class, representing a single tab on the browser.\napp is an instance of the ShinyAppProc class, representing the Python Shiny application.\nThe function test_table is designed to mimic user interaction with the inputs to generate an output table. It is always good practice to ensure that your tests cover a single functionality.\nThe controller.OutputTable and controller.InputSlider are methods from the controller module that mimic human interaction. Here is a list of all the available shiny controller classes and methods.\nThe .set method sets a value for each controller, just like a user will set it on the browser.\nThe .expect_column_labels, expect_ncol, and .expect_nrow methods are all specific to the controller, InputSlider. They check whether a table has the proper column labels and dimensions.\n\nType pytest on the command line, and click enter to run.\n\n\n\nTest result passed. Image by Author.\n\n\nYou can see that the test was successful. If the number of expected columns, rows, or label names provided does not match what the application is displaying, the test will fail and also give a reason for the failure.\n\n\n\nTest result fails. Image by Author."
  },
  {
    "objectID": "posts/E2E/index.html#end-to-end-testing-best-practices",
    "href": "posts/E2E/index.html#end-to-end-testing-best-practices",
    "title": "End-to-end Testing with Playwright and Python Shiny",
    "section": "End-to-end Testing Best Practices",
    "text": "End-to-end Testing Best Practices\n\nDefine Workflows to Test: When writing E2E tests, you need to define the parts of the application that you want to cover. It is unrealistic to say you want to write a test covering 100% of the application.\nSimulate Real-World User Experience: Ensure that any test you write simulates what users will realistically do, such as button clicks, file uploads, and so on.\nUse Descriptive Names: When creating test files and test functions, ensure you use descriptive names. This ensures anyone viewing the code understands what’s going on. Make sure the names of the test functions relate to the function of the application being tested.\nConduct Cross-browser Testing: Ensure you test all relevant browsers to ensure users can access your applications from various browsers.\nAutomate Tests: Ensure you integrate CI/CD workflows into your code, using tools like GitHub Actions, Jenkins, Circle CI, and others to run automated tests anytime a commit is made."
  },
  {
    "objectID": "posts/E2E/index.html#conclusion",
    "href": "posts/E2E/index.html#conclusion",
    "title": "End-to-end Testing with Playwright and Python Shiny",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, you have learned about E2E tests and how to write E2E tests with Playwright in your Python Shiny applications. E2E tests only covers user interactions, what if you want to test your business logic, such as calculations in your applications. This is where unit tests come into place, Check out How to Conduct Unit Tests in Python Shiny with Pytest to learn more.\nIf you want to learn more about Playwright, here are some resources that are of help.\n\nPlaywright Best Practices\nHow to perform End to End Testing using Playwright\nPlaywright End to End Testing: Complete Guide\nPlaywright: Fast and reliable end-to-end testing for modern web apps\n\n\nNeed Help with Data? Let’s Make It Simple.\nAt LearnData.xyz, we’re here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we’ve got your back.\n📧 Shoot us an email at admin@learndata.xyz—let’s chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "navs/services.html",
    "href": "navs/services.html",
    "title": "",
    "section": "",
    "text": "At learndata.xyz, we offer a comprehensive suite of data-driven solutions to help businesses and individuals harness the power of their data. Our expertise in R, Python, and statistical analysis, combined with our proficiency in Shiny web applications, allows us to deliver tailored services that meet your unique needs.\n\n\n\nAdvanced statistical analysis using R and Python\nCustom data visualization and dashboards\nExploratory data analysis (EDA) to uncover insights\nTime series analysis and forecasting\n\n\n\n\n\nDevelopment of predictive models and algorithms\nImplementation of supervised and unsupervised learning techniques\nModel evaluation, validation, and optimization\nNatural Language Processing (NLP) solutions\n\n\n\n\n\nCreation of interactive, data-driven web applications\nCustom dashboard design and implementation\nReal-time data processing and visualization\nIntegration of statistical models into user-friendly interfaces\n\n\n\n\n\nExpert advice on experimental design and sampling methods\nHypothesis testing and statistical inference\nPower analysis and sample size determination\nInterpretation and reporting of statistical results\n\n\n\n\n\nCustomized R and Python programming workshops\nData analysis and visualization best practices\nIntroduction to machine learning and predictive modeling\nShiny app development training\n\n\n\n\n\nAssessment and improvement of existing R and Python code\nPerformance optimization for data processing pipelines\nBest practices implementation for maintainable code\nVersion control and collaborative development guidance\n\nWhether you’re looking to gain insights from your data, build predictive models, or create interactive web applications, our team at learndata.xyz is here to support your data science journey. Contact us today to discuss how we can help you transform your data into actionable intelligence."
  },
  {
    "objectID": "navs/about.html#our-mission",
    "href": "navs/about.html#our-mission",
    "title": "About",
    "section": "Our Mission",
    "text": "Our Mission\nOur mission is to empower businesses and individuals to harness the full potential of their data. We believe that data, when properly analyzed and visualized, can drive innovation, inform decision-making, and unlock new opportunities across all sectors."
  },
  {
    "objectID": "navs/about.html#our-expertise",
    "href": "navs/about.html#our-expertise",
    "title": "About",
    "section": "Our Expertise",
    "text": "Our Expertise\nAt the heart of learndata.xyz is our deep expertise in:\n\nR and Python Programming: We leverage the power of these leading data science languages to deliver robust, efficient, and scalable solutions.\nStatistical Analysis: Our team of statisticians brings rigorous methodologies to every project, ensuring reliable and meaningful results.\nShiny Web Applications: We specialize in creating interactive, data-driven web applications that make complex data accessible and actionable.\nMachine Learning: From predictive modeling to natural language processing, we implement cutting-edge machine learning techniques to solve real-world problems."
  },
  {
    "objectID": "navs/about.html#our-approach",
    "href": "navs/about.html#our-approach",
    "title": "About",
    "section": "Our Approach",
    "text": "Our Approach\nWe believe in a collaborative, client-centered approach. Every project begins with a thorough understanding of your unique needs and goals. We then apply our technical expertise and industry knowledge to deliver tailored solutions that drive real value for your organization."
  },
  {
    "objectID": "navs/about.html#why-choose-us",
    "href": "navs/about.html#why-choose-us",
    "title": "About",
    "section": "Why Choose Us?",
    "text": "Why Choose Us?\n\nExpertise: Our team combines academic rigor with practical industry experience.\nInnovation: We stay at the forefront of data science developments, constantly exploring new techniques and technologies.\nCustomization: We don’t believe in one-size-fits-all solutions. Every project is tailored to meet your specific needs.\nEducation: We’re committed to not just delivering results, but also to empowering our clients with knowledge and skills.\nResults-Driven: Our focus is always on delivering actionable insights and measurable outcomes.\n\nAt learndata.xyz, we’re more than just consultants – we’re your partners in navigating the complex world of data. Whether you’re looking to optimize your processes, predict market trends, or transform your data into compelling visualizations, we’re here to guide you every step of the way.\nReady to unlock the power of your data? Contact us today to start your data-driven journey."
  },
  {
    "objectID": "navs/privacy.html",
    "href": "navs/privacy.html",
    "title": "",
    "section": "",
    "text": "Last updated: 2nd October, 2024\n\n\nWelcome to learndata.xyz. We respect your privacy and are committed to protecting your personal data. This privacy policy will inform you about how we look after your personal data when you visit our website and tell you about your privacy rights and how the law protects you.\n\n\n\n\n\nThis privacy policy aims to give you information on how learndata.xyz collects and processes your personal data through your use of this website, including any data you may provide through this website when you sign up for our newsletter, purchase a product or service, or take part in a survey.\n\n\n\nlearndata.xyz is the controller and responsible for your personal data (collectively referred to as “learndata.xyz”, “we”, “us” or “our” in this privacy policy).\n\n\n\n\nWe may collect, use, store and transfer different kinds of personal data about you which we have grouped together as follows:\n\nIdentity Data\nContact Data\nTechnical Data\nUsage Data\nMarketing and Communications Data\n\n\n\n\nWe will only use your personal data when the law allows us to. Most commonly, we will use your personal data in the following circumstances:\n\nWhere we need to perform the contract we are about to enter into or have entered into with you.\nWhere it is necessary for our legitimate interests and your interests and fundamental rights do not override those interests.\nWhere we need to comply with a legal obligation.\n\n\n\n\nWe have put in place appropriate security measures to prevent your personal data from being accidentally lost, used or accessed in an unauthorized way, altered or disclosed.\n\n\n\nWe will only retain your personal data for as long as reasonably necessary to fulfil the purposes we collected it for, including for the purposes of satisfying any legal, regulatory, tax, accounting or reporting requirements.\n\n\n\nUnder certain circumstances, you have rights under data protection laws in relation to your personal data, including the right to:\n\nRequest access to your personal data\nRequest correction of your personal data\nRequest erasure of your personal data\nObject to processing of your personal data\nRequest restriction of processing your personal data\nRequest transfer of your personal data\nRight to withdraw consent\n\n\n\n\nWe may update our privacy policy from time to time. We will notify you of any changes by posting the new privacy policy on this page.\n\n\n\nIf you have any questions about this privacy policy or our privacy practices, please contact us at:\nEmail: admin@learndata.xyz"
  },
  {
    "objectID": "navs/tos.html",
    "href": "navs/tos.html",
    "title": "",
    "section": "",
    "text": "Last updated: 2nd October, 2024\nPlease read these Terms of Service (“Terms”, “Terms of Service”) carefully before using the https://learndata.xyz website (the “Service”) operated by learndata.xyz (“us”, “we”, or “our”).\nYour access to and use of the Service is conditioned on your acceptance of and compliance with these Terms. These Terms apply to all visitors, users and others who access or use the Service.\nBy accessing or using the Service you agree to be bound by these Terms. If you disagree with any part of the terms then you may not access the Service.\n\n\nlearndata.xyz provides data science consulting services, including but not limited to data analysis, statistical modeling, machine learning, and Shiny web application development. The specific services to be provided will be agreed upon in separate contracts or statements of work.\n\n\n\nWhen you create an account with us, you must provide us information that is accurate, complete, and current at all times. Failure to do so constitutes a breach of the Terms, which may result in immediate termination of your account on our Service.\nYou are responsible for safeguarding the password that you use to access the Service and for any activities or actions under your password, whether your password is with our Service or a third-party service.\n\n\n\nThe Service and its original content, features and functionality are and will remain the exclusive property of learndata.xyz and its licensors. The Service is protected by copyright, trademark, and other laws of both the United States and foreign countries. Our trademarks and trade dress may not be used in connection with any product or service without the prior written consent of learndata.xyz.\n\n\n\nOur Service may contain links to third-party web sites or services that are not owned or controlled by learndata.xyz.\nlearndata.xyz has no control over, and assumes no responsibility for, the content, privacy policies, or practices of any third party web sites or services. You further acknowledge and agree that learndata.xyz shall not be responsible or liable, directly or indirectly, for any damage or loss caused or alleged to be caused by or in connection with use of or reliance on any such content, goods or services available on or through any such web sites or services.\n\n\n\nWe may terminate or suspend access to our Service immediately, without prior notice or liability, for any reason whatsoever, including without limitation if you breach the Terms.\nAll provisions of the Terms which by their nature should survive termination shall survive termination, including, without limitation, ownership provisions, warranty disclaimers, indemnity and limitations of liability.\n\n\n\nYour use of the Service is at your sole risk. The Service is provided on an “AS IS” and “AS AVAILABLE” basis. The Service is provided without warranties of any kind, whether express or implied, including, but not limited to, implied warranties of merchantability, fitness for a particular purpose, non-infringement or course of performance.\n\n\n\nThese Terms shall be governed and construed in accordance with the laws of [Your State/Country], without regard to its conflict of law provisions.\nOur failure to enforce any right or provision of these Terms will not be considered a waiver of those rights. If any provision of these Terms is held to be invalid or unenforceable by a court, the remaining provisions of these Terms will remain in effect.\n\n\n\nWe reserve the right, at our sole discretion, to modify or replace these Terms at any time. If a revision is material we will try to provide at least 30 days notice prior to any new terms taking effect. What constitutes a material change will be determined at our sole discretion.\n\n\n\nIf you have any questions about these Terms, please contact us at:\nEmail: admin@learndata.xyz"
  },
  {
    "objectID": "navs/courses.html#ml-model-deployment-with-fastapi-and-streamlit",
    "href": "navs/courses.html#ml-model-deployment-with-fastapi-and-streamlit",
    "title": "Courses",
    "section": "ML Model Deployment with FastAPI and Streamlit",
    "text": "ML Model Deployment with FastAPI and Streamlit"
  },
  {
    "objectID": "navs/home.html",
    "href": "navs/home.html",
    "title": "",
    "section": "",
    "text": "Are you ready to transform your data into actionable insights? At learndata.xyz, we specialize in turning complex data into clear, powerful solutions.\n\n\n\nR & Python Consulting: Harness the full potential of these leading data science languages.\nStatistical Analysis: Make informed decisions with robust statistical methodologies.\nShiny Web Applications: Create interactive, data-driven web apps that captivate and inform.\n\n\n\n\n\nExpert Team: Our statisticians and data scientists bring years of experience to every project.\nTailored Solutions: We craft custom solutions that address your unique challenges.\nCutting-Edge Techniques: Stay ahead with the latest in machine learning and data visualization.\nEducation-Focused: We don’t just deliver results; we empower your team with knowledge.\n\n\n\n\n\nData Analysis and Visualization\nMachine Learning and Predictive Modeling\nShiny Web Application Development\nStatistical Consulting\nTraining and Workshops\nCode Review and Optimization\n\n\n\n\nContact Us Now to discuss how we can elevate your data game.\nExplore Our Services to learn more about our offerings."
  },
  {
    "objectID": "posts/CICD with Python Shiny and GitHub Actions/index.html",
    "href": "posts/CICD with Python Shiny and GitHub Actions/index.html",
    "title": "CI/CD with Python Shiny and GitHub Actions",
    "section": "",
    "text": "When working on a project with many collaborators, running tests before merging is essential to ensure that none of the application functionality is broken. This is handled by automating an application’s testing, building, and deployment process.\nCI stands for continuous integration, which is the automation of an application’s testing and deployment process, while CD stands for continuous deployment, which is the automation of an application’s deployment.\nGitHub Actions make it easy to implement CI/CD on applications, primarily if the application repository is hosted on GitHub. In a short period of time, you can set up a workflow that automates your test, build, and deployment process.\nIn this article, you will learn how to use GitHub Actions to create a workflow in your Python Shiny code repository that only merges pull requests from collaborators if tests pass, using Pytest for unit testing."
  },
  {
    "objectID": "posts/CICD with Python Shiny and GitHub Actions/index.html#what-are-github-actions",
    "href": "posts/CICD with Python Shiny and GitHub Actions/index.html#what-are-github-actions",
    "title": "CI/CD with Python Shiny and GitHub Actions",
    "section": "What are GitHub Actions?",
    "text": "What are GitHub Actions?\nMany CI/CD platforms exist, such as Jenkins, Circle CI, and so on. However, GitHub Actions is one of the easiest to work with since it integrates well with projects hosted on GitHub. GitHub Actions is a feature within GitHub that enables you to automate tasks within your software development lifecycle. By using GitHub Actions, you also benefit from GitHub security and code scanning features. GitHub Action uses YAML to create an automation workflow, with the option to run it on the GitHub cloud or your local server.\nGitHub Actions is made up of the following core components:\n\nWorkflow: A configurable automated process comprising one or more jobs. Workflows are defined in YAML files within your repository’s .github/workflows/ directory.\nEvent: A specific activity that triggers the workflow. Events can be GitHub events (like push, pull_request), scheduled events (cron), or manual triggers.\nJob: A set of steps that execute on the same runner. Jobs can run sequentially or in parallel and depend on other jobs.\nStep: An individual task within a job. Steps can run commands or use actions.\nAction: A reusable extension that can simplify your workflow by performing specific tasks such as checking out code, setting up languages, or deploying.\nRunner: A server that runs your workflows when they’re triggered. GitHub provides hosted runners, or you can host your own.\n\n\n\n\nGitHub Action Workflow. Image by Author."
  },
  {
    "objectID": "posts/CICD with Python Shiny and GitHub Actions/index.html#a-basic-python-shiny-application",
    "href": "posts/CICD with Python Shiny and GitHub Actions/index.html#a-basic-python-shiny-application",
    "title": "CI/CD with Python Shiny and GitHub Actions",
    "section": "A Basic Python Shiny Application",
    "text": "A Basic Python Shiny Application\nLet’s build a simple Python Shiny application that lets users compute the total profit and revenue from supermarket sales data and implement some unit tests.\n\nStep 1 - Install dependencies\nCreate a project folder, and copy and paste the following library names into a requirements.txt file.\nshiny\npandas\npytest\npytest-cov\n\nshiny library for building web applications.\npandas library for loading and wrangling data.\npytest library for unit testing.\npytest-cov library for producing pytest coverage reports.\n\nRun the following code on your terminal to install the required libraries.\npython3 pip install -r requirements.txt\n\n\nStep 2 - Setup utility functions\nCreate a new folder, utilis.py. This folder will contain the application’s business logic.\nimport pandas as pd\n\n# Business Logic\ndef fetch_data():\n    data = pd.read_csv(\"supermarket_sales.csv\")\n    return data\n\ndef total_profit(data):\n    profit = sum(data[\"gross income\"])\n    return profit\n\ndef total_revenue(data):\n    revenue = sum(data[\"cogs\"])\n    return revenue\n\nThe fetch_data() function loads the dataset.\nThe total_profit(data) calculates the total profit.\nThe total_revenue(data) calculates the total revenue.\n\n\n\nStep 3 - Create the application user interface\nCreate a simple user interface that will give users options regarding the type of aggregate they want from the sales data. Copy and paste the following into a file named main.py.\nfrom shiny import reactive\n\nfrom shiny.express import input, render, ui\n\nfrom utilis import fetch_data, total_profit, total_revenue\n\nwith ui.card():\n    ui.h2(\"Sales Dashboard\"),\n    ui.input_select(\n        \"calculation\", \"Select calculation:\", choices=[\"Total Profit\", \"Total Revenue\"]\n    )\n    ui.input_action_button(\"compute\", \"Compute!\")\n\nwith ui.card():\n    with ui.h1():\n        @render.text\n        @reactive.event(input.compute)\n        def result():\n            if input.calculation() == \"Total Profit\":\n                value = total_profit(fetch_data())\n            else:\n                value = total_revenue(fetch_data())\n\n            return value\n\n\nStep 4 - Run the application\nRun the application to ensure that it is working as expected.\n shiny run --reload main.py\n\n\n\nLive Python Shiny Application. Image by Author.\n\n\n\n\nStep 5 - Add unit tests\nAdd the following unit tests into a file test_main.py\nfrom utilis import fetch_data, total_profit, total_revenue\nimport pandas as pd\n\n# Business Logic\ndata = fetch_data()\n\ndef test_total_profit():\n    assert total_profit(data) == 15379.369\n\ndef test_total_revenue():\n    assert total_revenue(data) == 307587.38\ntest_total_profit()and test_total_revenue() check if the values on the Python Shiny application are equal to 1539.369 and 307587.38, respectively.\nRun pytest on your terminal to ensure that the test passes.\n\n\n\nCheck if all unit tests are passed. Image by Author."
  },
  {
    "objectID": "posts/CICD with Python Shiny and GitHub Actions/index.html#setting-up-an-automation-workflow-with-github-actions",
    "href": "posts/CICD with Python Shiny and GitHub Actions/index.html#setting-up-an-automation-workflow-with-github-actions",
    "title": "CI/CD with Python Shiny and GitHub Actions",
    "section": "Setting Up an Automation Workflow with GitHub Actions",
    "text": "Setting Up an Automation Workflow with GitHub Actions\nLet’s set up an automation workflow that is triggered anytime a push is made. Before proceeding, ensure your project is hosted on GitHub."
  },
  {
    "objectID": "posts/CICD with Python Shiny and GitHub Actions/index.html#step-1---set-up-workflow-directory",
    "href": "posts/CICD with Python Shiny and GitHub Actions/index.html#step-1---set-up-workflow-directory",
    "title": "CI/CD with Python Shiny and GitHub Actions",
    "section": "Step 1 - Set up workflow directory",
    "text": "Step 1 - Set up workflow directory\nIn your project directory, create a new folder, .github; in it, create another folder, workflows. Inside the workflows folder, create a file called run_test.yml. Copy and paste the following code into it.\nname: Run Unit Test via Pytest\n\non: push\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: 3.12.1\n\n      - name: Install dependencies and lint\n        run: |\n          python -m pip install --upgrade pip\n          pip install -r requirements.txt\n          pip install ruff\n          ruff --format=github --target-version=py310 . || true\n\n      - name: Test with pytest\n        run: |\n          coverage run -m pytest -v -s\n          coverage report -m\nHere is a breakdown of the above YAML file.\n\nname: Run Unit Test via Pytest is the label identifying what the workflow does.\non: push means the workflow will start automatically whenever new changes are pushed to the GitHub repository.\njobs contain tasks or steps that the workflow will execute. You can specify various numbers of jobs, here we only have one job named build.\nruns-on: ubuntu-latest means that the job should run on the latest version of Ubuntu, which is the environment in which the code is tested.\nuses: actions/checkout@v3, the first step, clones your code onto the virtual machine where the test will run.\nThe next step, which is name: Set up Python, installs Python on the virtual machine, specifically version 3.12.1, to ensure the code runs in the correct Python environment.\nThe next step name: Install dependencies and lint, install the project dependencies, and check for linting.\n\npython -m pip install --upgrade pip upgrades pip.\npip install -r requirements.txt installs the required libraries.\npip install ruff installs Ruff, a tool that checks your code for potential errors and enforces linting.\nruff --format=github --target-version=py310 . || true runs Ruff to analyze your code. If Ruff detects an issue, ||true ensures that the workflow continues running, allowing you to see listing results without stopping the testing process.\n\nThe last step name: Test with pytest runs the unit tests and checks how much code was covered by the tests.\n\n\nStep 2 - Push to GitHub to run the workflow\nPush your project to GitHub to run the workflow, then go to the Actions tab under the project repository. This will give you a list of all workflows with runners under that specific repository.\n\n\n\nGo to the Actions tab to check running workflows. Image by Author.\n\n\nGo to the Actions tab to check running workflows. Image by Author.\nA green check means the workflow run was successful, while a red cross means it failed. You can click on a specific workflow run and then click on build to get more details about the workflow.\n\n\n\nClick the build under a workflow run to view various tasks under the workflow run. Image by Author.\n\n\n\n\nStep 3 - Create a branch to make a merge request.\nLet’s create a branch and break some code functionality to ensure the test fails. Create a new branch, change any numbers used in the test function of the test_main.py file, and push to GitHub under the new branch.\n\n\n\nDifference between the main and new branch. Image by Author.\n\n\nClick Create Pull request on GitHub to merge the branch into the main branch. The workflow will be executed, and an error message will show that the unit tests failed.\n\n\n\nThe message shows that the unit test failed in the new branch. Image by Author.\n\n\nNow, return to the branch and fix it as before. If you make a pull request, you will see that all tests have been successfully passed.\n\n\n\nAll checks were successfully passed. Image by Author.\n\n\nCollaborators can still make merge requests even if the tests fail. You need to enforce a rule that prevents any pull request from merging if all tests have not been passed. Go to the GitHub settings and select branch. Under the branch section, give a name to the rule you are about to set and check the option Require status checks to pass.\n\n\n\nGo to Branch rules to enforce a rule. Image by Author."
  },
  {
    "objectID": "posts/CICD with Python Shiny and GitHub Actions/index.html#conclusion",
    "href": "posts/CICD with Python Shiny and GitHub Actions/index.html#conclusion",
    "title": "CI/CD with Python Shiny and GitHub Actions",
    "section": "Conclusion",
    "text": "Conclusion\nThere is more to GitHub actions than just automating the testing of pull requests. You can integrate GitHub Actions with various tools such as Mail, Slack, and Discord to get notifications on GitHub Actions events. Many of these tools are available on the GitHub Marketplace.\nIn his article, you learned how to set up a simple CI/CD workflow with GitHub Actions and Python Shiny. I hope this motivates you to build on this in your next project. Here are some valuable resources on testing and GitHub Actions.\nHow to Conduct Unit Tests in Python Shiny with Pytest\nEnd-to-end Testing with Playwright and Python Shiny\nGitHub Actions and MakeFile: A Hands-on Introduction\nCI/CD in Data Engineering: A Guide for Seamless Deployment\nHow to use Github Actions for Data Science\nHow to Send Detailed Slack Notifications from GitHub Actions?\n\nNeed Help with Data? Let’s Make It Simple.\nAt LearnData.xyz, we’re here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we’ve got your back.\n📧 Shoot us an email at admin@learndata.xyz—let’s chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html",
    "href": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html",
    "title": "A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn",
    "section": "",
    "text": "Working with numerical data helps us understand the distribution of values in a numerical variable. This gives us a sense of the frequently occurring values and how these values vary from each other. It also highlights the presence of extreme values in a variable, if any.\nIn this article, you will learn how to plot a histogram using Seaborn, a Python library built on Matplotlib for statistical data visualization. You will also learn how to customize and interpret your histogram plots to derive precise insights from your data."
  },
  {
    "objectID": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#what-is-a-histogram",
    "href": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#what-is-a-histogram",
    "title": "A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn",
    "section": "What is a Histogram?",
    "text": "What is a Histogram?\nA histogram is used to plot numeric data. It splits a numeric variable into various equal ranges known as bins and plots the total number of observations that fall under each bin as bars. These bars are adjacent to each other, with no space between them. This differs from a bar plot that plots the frequency of categorical data, with space between the bars.\n\n\n\nDifference between histogram and bar chart. Source: Syncfusion"
  },
  {
    "objectID": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#building-a-histogram-with-seaborn",
    "href": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#building-a-histogram-with-seaborn",
    "title": "A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn",
    "section": "Building a Histogram with Seaborn",
    "text": "Building a Histogram with Seaborn\nWe will build our histogram using the tips dataset in the seaborn library. This dataset contains information about tips received by a waiter over a period of months. Before proceeding, ensure you have imported the pandas, Seaborn, and Matplotib libraries.\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nLet’s view the columns in the dataset.\ntips = sns.load_dataset('tips')\ntips.head()\n\n\n\nPreview of the tips dataset. Image by Author.\n\n\nWe will use the histogram to answer questions from our dataset regarding the tip variable, such as:\n\nWhat is the distribution of the tip?\nWhat is the highest and lowest tip amount received?\nWhat is the tip amount most frequently received by the waiter?\nIs there a difference between the distribution of tips from female and male customers?\n\n\nCreating the histogram\nTo build a seaborn histogram, you need to use the sns.histplot() function. The following are the key arguments to provide:\n\ndata: the pandas dataframe.\nx: the variable of interest in the data.\ncolor: the color of the bars.\nalpha: the transparency of the bars.\nbins: the number of bins in the histogram.\nbinwidth: the width of each bin.\nkde: A boolean to add a kernel density estimation\nhue: to differentiate data subset based on another variable\n\nUse the following code to build a simple histogram for us to start with, adding title and axis labels to the plot.\nsns.histplot(data=tips, x='tip')\nplt.title(\"Distribution of tips received by the waiter\")\nplt.xlabel(\"Tip amount\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\nA simple histogram that shows the distribution of tips given to the waiter. Image by Author."
  },
  {
    "objectID": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#enhancing-the-histogram",
    "href": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#enhancing-the-histogram",
    "title": "A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn",
    "section": "Enhancing the histogram",
    "text": "Enhancing the histogram\nYou can improve the histogram by providing some of the arguments explained in the previous section.\n\nAdjusting bin sizes and width\nCurrently, the default number of bins is set to auto based on the number of observations, and the bandwidth is set to None by default.\n\nNote\nUsing a bin width overrides the bins argument.\n\nLet’s increase the number of bins by giving a higher value, say 100.\nsns.histplot(data=tips, x='tip', bins=100)\nAlternatively, you can set the binwidth to 0.1, you will still get the same plot.\nsns.histplot(data=tips, x='tip', binwidth=0.1)\n\n\n\nAdjust the bin size or bandwidth to spread or thin the distribution. Image by Author.\n\n\n\n\nChanging the bin color\nTo set the bin’s color, add a value to the color argument. Let’s give the histogram the color green.\nsns.histplot(data=tips, x='tip', color=\"green\")\nplt.title(\"Distribution of tips received by the waiter\")\nplt.xlabel(\"Tip amount\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.show()\n\n\n\nAdd a color to the histogram. Image by Author.\n\n\n\n\nAdd a Kernel Density Estimation (KDE)\nTo ease interpretation, we can set the kde argument to True. This will apply a kernel density estimation that smooths the histogram points, revealing the shape of the distribution.\nsns.histplot(data=tips, x='tip', color=\"green\", kde=True)\nplt.title(\"Distribution of tips received by the waiter\")\nplt.xlabel(\"Tip amount\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.show()\n\n\n\nSet kde to true. Image by Author\n\n\n\n\nAdd grouping variables\nYou can add a grouping variable to the histogram to see the distribution for each respective group. For example, let’s group the histogram by sex by adding a sex group to the hue argument to see the distribution of tips given by male and female customers.\n\nNote\nThis is going to override the color argument you specified earlier.\n\nsns.histplot(data=tips, x='tip', color=\"green\", kde=True, hue=\"sex\")\nplt.title(\"Distribution of tips received by the waiter\")\nplt.xlabel(\"Tip amount\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.show()\n\n\n\nAdd a group variable to the histogram. Image by Author."
  },
  {
    "objectID": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#interpreting-histograms",
    "href": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#interpreting-histograms",
    "title": "A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn",
    "section": "Interpreting Histograms",
    "text": "Interpreting Histograms\nInterpreting a histogram is not difficult, especially once you use the kernel density estimation on the plot. The highest bars are the frequently occurring values and the highest points on the kernel density plot are the highest bars. We can deduce from our histogram that the most frequent amount of tips received by the waiter ranges from $2 to $3.\n\n\n\nIdentifying frequently occurring values through the plot peak. Image by Author.\n\n\nThe tail of the kernel density plot indicates extreme values. The longer the tail, the more extreme values present in the data are. This tail is referred to as a skew. A long tail to the left means the distribution is left-skewed. A long tail to the right means the histogram distribution is right-skewed, just like in the case of our plot. This means the waiter received tips above $6 from a few customers.\n\n\n\nIdentifying the skewness of the histogram plot. Image by Author.\n\n\nThe difference between the two kernel density plots also tells us that male customers give higher tips to the waiter than female customers, though the number of male customers is greater than that of female customers. The minimum tip the waiter receives is $1, while the highest is $10.\n\n\n\nDifference between the two KDE plots. Image by Author."
  },
  {
    "objectID": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#best-practices-when-creating-histograms",
    "href": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#best-practices-when-creating-histograms",
    "title": "A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn",
    "section": "Best Practices when Creating Histograms",
    "text": "Best Practices when Creating Histograms\nA histogram is a powerful plot that can tell you much about your numeric variables. Here are some best practices to ensure you craft accurate and precise histograms.\n\nChoose an Appropriate Bin Size: When choosing a bin size, try various values and ensure that the selected bin size conveys as much information as possible.\nLabel Axes: Ensure you provide informative labels when labeling your axes to help your viewers understand the plot.\nUse Contrasting Colors: When comparing groups, use contrasting colors to make it easy to identify group differences.\nUse Legends: When working with group variables, it is advisable to have a legend to identify the group and their respective colors in the plot.\nUse Consistent Bin Widths: When setting bin width size, ensure your bin width is consistent throughout to avoid distorting the information conveyed by the plot."
  },
  {
    "objectID": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#conclusion",
    "href": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#conclusion",
    "title": "A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn",
    "section": "Conclusion",
    "text": "Conclusion\nWhen conducting predictive analysis on a dataset with numerical variables, it is crucial to view the distribution of these variables to identify outliers. This is where a histogram comes into play. You can use a histogram to see how the values in a variable are dispersed from each other, making the histogram a very important plot for exploratory data analysis.\nIn this article, you have learned about histograms and how to build and interpret them using the Seaborn library in Python. If you want to learn more about histograms, here are some valuable resources.\nHow to Read Histograms: 9 Steps (with Pictures)\nInterpreting Histograms\nEverything about Density Plot\nShapes of Distributions: Definitions, Examples\nMeasures of shape\n\nNeed Help with Data? Let’s Make It Simple.\nAt LearnData.xyz, we’re here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we’ve got your back.\n📧 Shoot us an email at admin@learndata.xyz—let’s chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/How to Create and Read a Forest Plot in R/index.html",
    "href": "posts/How to Create and Read a Forest Plot in R/index.html",
    "title": "How to Create and Read a Forest Plot in R",
    "section": "",
    "text": "As a researcher trying to compare the results of a particular intervention or treatment from different studies, a forest plot makes it easy to view results from multiple studies. This makes it easy to see variations and trends in an intervention or treatment. In this article, you will learn how to create and customize a forest plot in R using the forestplot package. I will also show you how to read a forest plot."
  },
  {
    "objectID": "posts/How to Create and Read a Forest Plot in R/index.html#what-is-a-forest-plot",
    "href": "posts/How to Create and Read a Forest Plot in R/index.html#what-is-a-forest-plot",
    "title": "How to Create and Read a Forest Plot in R",
    "section": "What is a Forest plot?",
    "text": "What is a Forest plot?\nA forest plot is a type of graph used in research to show the results of a single topic from multiple studies line by line. Each study is represented by a line and a dot, where the dot shows the main result (how effective a treatment is) while the line shows the range of possible results, also known as the confidence intervals. The forest plot sometimes includes a diamond-shaped plot below, summarizing all the results of the studies. If all the lines are close to each other, then it means the studies agree on that particular topic or there are varying findings on that topic.\n\nComponents of a Forest plot\n\nStudy Labels: Located at the left side of the plot, these labels identify each respective study and contain the study author and year.\nEffect Size: This represents the observed effect of an outcome in each study, such as the prevalence of a disease. The position of the point on the plot indicates whether the effect is negative, neutral, or positive.\nConfidence Intervals: Each study shows a horizontal line showing the range of possible outcomes. The narrower the line, the higher the precision of the effect Size.\nNo Effect Line: This vertical line is either zero or one, representing no effect. If a study confidence interval crosses this line, the study is not statistically significant.\nOverall Effect (Diamond Shape): This represents the pooled effect from all studies at the plot’s bottom. The center of the diamond shows the overall effect, while the width of the diamond represents the confidence interval. The combined result is statistically significant if the diamond does not cross the no-effect line.\n\n\n\n\nForest Plot. Image by Author"
  },
  {
    "objectID": "posts/How to Create and Read a Forest Plot in R/index.html#forestplot-package-in-r",
    "href": "posts/How to Create and Read a Forest Plot in R/index.html#forestplot-package-in-r",
    "title": "How to Create and Read a Forest Plot in R",
    "section": "forestplot package in R",
    "text": "forestplot package in R\nforest plot is an r package for creating publication-ready forest plots and works seamlessly with the pipe (|> or %>%) operator. You can install the package by running the code below.\ninstall.packages(\"forestplot\")\nHere are some critical arguments for plotting a forest plot in R using the forestplot function from the forestplot package.\n\nmean: This takes the column containing the effect size of each study.\nlower: This takes the lower confidence interval of the effect Size.\nupper: This takes the higher confidence interval of the effect Size.\n\nYou can check for additional arguments by running ?forestplot on your R console."
  },
  {
    "objectID": "posts/How to Create and Read a Forest Plot in R/index.html#creating-a-forest-plot-in-r",
    "href": "posts/How to Create and Read a Forest Plot in R/index.html#creating-a-forest-plot-in-r",
    "title": "How to Create and Read a Forest Plot in R",
    "section": "Creating a Forest plot in R",
    "text": "Creating a Forest plot in R\nIn this tutorial, we will use fictional data from various studies studying the effect of a particular cancer treatment. First, load the forest plot package.\nlibrary(forestplot)\nNext, we will load the dataset we will work with.\ncancer <- read.csv(\"https://raw.githubusercontent.com/adejumoridwan/datasets/refs/heads/main/cancer_data\")\nhead(cancer)\n\n\n\nTop 6 rows of the cancer studies dataset. Image by Author.\n\n\nCopy and paste the following code to create the forest plot and run.\ncancer |> \n  forestplot(\n    mean = cancer$EffectSize,\n    lower = cancer$CI_lower,\n    upper = cancer$CI_upper,\n    labeltext = c(Study, EffectSize, CI_lower, CI_upper),\n    xlog = TRUE,\n    boxsize = 0.25,\n    vertices = TRUE,) \nHere are the roles of the arguments specified in the forestplot function:\n\nmean: This argument takes the effect size of the outcome of interest from the dataset.\nlower: This argument takes the lower confidence interval of the effect size.\nupper: This takes the upper confidence interval from the effect size.\nlabeltext: This specifies the columns appearing on the forest plot’s left side.\nxlog : If TRUE, the marks on the x-axis follow a logarithmic scale.\nboxsize: Overrides the default size of the box used in the plot.\nvertices: If TRUE, apply vertices to the end of the confidence intervals for each study.\n\n\n\n\nPlot the forest plot. Image by Author"
  },
  {
    "objectID": "posts/How to Create and Read a Forest Plot in R/index.html#customizing-forest-plot",
    "href": "posts/How to Create and Read a Forest Plot in R/index.html#customizing-forest-plot",
    "title": "How to Create and Read a Forest Plot in R",
    "section": "Customizing Forest Plot",
    "text": "Customizing Forest Plot\nAdding headers, changing the colors, or changing the theme are various ways of improving the forest plot. All these are done by piping additional functions to the main forestplot() function.\n\nAdd column labels\nLet’s add column labels to the plot above to create an identification for the columns. Use the pipe operator and pass the following function to the plot.\n|> \nfp_add_header(Study = c(\"Study\"),\n                EffectSize = c(\"Effect Size\"),\n                CI_lower  = c(\"Lower CI\"),\n                CI_upper = c(\"Upper CI\"))\nThe fp_add_header() function assigns new column labels to the dataset column names.\n\n\n\nAdd column labels. Image by Author.\n\n\n\n\nAdd a summary\nYou can add a summary at the bottom of the plot summarizing the whole plot by taking the mean of the effect sizes and the confidence intervals. This will also add a diamond-shaped overall effect below the plot.\n|>\nfp_append_row(mean  = mean(cancer$EffectSize),\n              lower = mean(cancer$CI_lower),\n              upper = mean(cancer$CI_upper),\n              Study = \"Summary\",\n              EffectSize = mean(cancer$EffectSize),\n              is.summary = TRUE)\nThe fp_append_row() adds another row to the forest plot, the mean of the effect size, lower, and upper lower are calculated, and a graph is plotted based on these values. The EffectSize argument adds the value of the overall effect size to the table; you can add the lower and upper confidence intervals by adding the following arguments to the fp_append_row() function.\n\nCI_lower = mean(cancer$CI_lower)\nCI_upper = mean(cancer$CI_upper)\n\n\n\n\nAdd summary row. Image by Author.\n\n\n\n\nAdd colors and demarcate lines\nLet’s add a line to the plot demarcating the column headers from the observations and change the dots’ colors to blue. Then, pass the following function to the forest plot code.\n  |>\n  fp_add_lines() |> \n  fp_set_style(box = \"royalblue\",\n               line = \"darkblue\",\n               summary = \"royalblue\")\n\nfp_add_lines() adds a line demarcating the headers and summary from the observation rows.\nfp_set_style sets a color to the forest plot.\nThe box argument gives a color to the forest plot boxes.\nThe line argument gives a color to the confidence interval lines.\nThe summary gives a color to the diamond-shaped plot under the summary row.\n\n\n\n\nAdd colors and demarcate lines. Image by Author.\n\n\n\n\nAdd a theme\nYou can add a theme to the forest plot to make it easy to read. Add a zebra-styled theme by passing the following function to the forest plot function.\n|>\nfp_set_zebra_style(\"#EFEFEF\")\n\n\n\nAdd a zebra-styled theme. Image by Author."
  },
  {
    "objectID": "posts/How to Create and Read a Forest Plot in R/index.html#reading-a-forest-plot",
    "href": "posts/How to Create and Read a Forest Plot in R/index.html#reading-a-forest-plot",
    "title": "How to Create and Read a Forest Plot in R",
    "section": "Reading a Forest Plot",
    "text": "Reading a Forest Plot\nEach study has a blue square that represents its effect size. An effect size greater than 1 means the study found a positive effect, while less than 1 means a negative effect. The horizontal lines around each square represent the range of values (“confidence interval”) within which we expect the actual effect size to fall 95% of the time. If this line crosses 1, the effect could be neutral (neither positive nor negative).\nSome studies, like (Smith et al. 2015) and (Brown et al. 2017), have effect sizes above 1, and their confidence intervals do not cross 1. This implies that they found a significant positive effect.\n\n\n\nEffect size and CI above 1. Image by Author.\n\n\nStudies like (Wilson et al. 2020) have effect sizes below one and confidence intervals that do not cross 1, indicating a negative or weaker effect.\n\n\n\nEffect size and CI below 1. Image by Author.\n\n\nStudies such as (Davis et al. 2018) and (Jackson et al. 2023) have confidence intervals that cross 1, meaning the effect might not be significant.\n\n\n\nCI crossing 1. Image by Author.\n\n\nAt the bottom, a diamond shape represents the “summary effect” of all studies combined. In this plot, the summary effect size is about 1.14, with a confidence interval that crosses 1. This means that, overall, when we look at all the studies together, there is no effect\n\n\n\nDiamond shaped summary. Image by Author"
  },
  {
    "objectID": "posts/How to Create and Read a Forest Plot in R/index.html#conclusion",
    "href": "posts/How to Create and Read a Forest Plot in R/index.html#conclusion",
    "title": "How to Create and Read a Forest Plot in R",
    "section": "Conclusion",
    "text": "Conclusion\nThe forest plot is not limited to comparing results between studies. You can also use it to compare predictor variables in a regression analysis, where instead of study labels, you have predictor variables and their respective estimates and confidence intervals.\nIn this article, you have learned how to create a forest plot in R using the forest plot package and how to customize and derive insights from it. To learn more, check out the package vignette. There are other packages such as forestploter, forester, and if you want a custom forestplot, you can build yours using ggplot2. Here are further resources that dive deep into interpreting a forest plot. Hope you find them helpful.\nForest Plot Generation in R\nTutorial: How to read a forest plot\nThe 5 min meta-analysis: understanding how to read and interpret a forest plot\nSeeing the Forest by Looking at the Trees: How to Interpret a Meta-Analysis Forest Plot\n\nNeed Help with Data? Let’s Make It Simple.\nAt LearnData.xyz, we’re here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we’ve got your back.\n📧 Shoot us an email at admin@learndata.xyz—let’s chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn",
    "section": "Your next breakthrough could be one email away. Let’s make it happen!",
    "text": "Your next breakthrough could be one email away. Let’s make it happen!\n\nNeed Help with Data? Let’s Make It Simple.\nAt LearnData.xyz, we’re here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we’ve got your back.\n📧 Shoot us an email at admin@learndata.xyz—let’s chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen-1",
    "href": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen-1",
    "title": "A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn",
    "section": "Your next breakthrough could be one email away. Let’s make it happen!",
    "text": "Your next breakthrough could be one email away. Let’s make it happen!"
  },
  {
    "objectID": "posts/CICD with Python Shiny and GitHub Actions/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/CICD with Python Shiny and GitHub Actions/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "CI/CD with Python Shiny and GitHub Actions",
    "section": "Your next breakthrough could be one email away. Let’s make it happen!",
    "text": "Your next breakthrough could be one email away. Let’s make it happen!"
  },
  {
    "objectID": "posts/E2E/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/E2E/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "End-to-end Testing with Playwright and Python Shiny",
    "section": "Your next breakthrough could be one email away. Let’s make it happen!",
    "text": "Your next breakthrough could be one email away. Let’s make it happen!"
  },
  {
    "objectID": "posts/How to Create and Read a Forest Plot in R/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/How to Create and Read a Forest Plot in R/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "How to Create and Read a Forest Plot in R",
    "section": "Your next breakthrough could be one email away. Let’s make it happen!",
    "text": "Your next breakthrough could be one email away. Let’s make it happen!"
  },
  {
    "objectID": "posts/How to Create and Read a Forest Plot in R/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen-1",
    "href": "posts/How to Create and Read a Forest Plot in R/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen-1",
    "title": "How to Create and Read a Forest Plot in R",
    "section": "Your next breakthrough could be one email away. Let’s make it happen!",
    "text": "Your next breakthrough could be one email away. Let’s make it happen!"
  },
  {
    "objectID": "posts/How_to_conduct_unit_tests_in_Python_Shiny_with_Pytest/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/How_to_conduct_unit_tests_in_Python_Shiny_with_Pytest/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "How to Conduct Unit Tests in Python Shiny with Pytest",
    "section": "Your next breakthrough could be one email away. Let’s make it happen!",
    "text": "Your next breakthrough could be one email away. Let’s make it happen!"
  },
  {
    "objectID": "posts/language_translator/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/language_translator/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "How to Build a Language Translator Application with Strapi, Streamlit, and Hugging Face Models",
    "section": "Your next breakthrough could be one email away. Let’s make it happen!",
    "text": "Your next breakthrough could be one email away. Let’s make it happen!"
  },
  {
    "objectID": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html",
    "href": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html",
    "title": "Linear Regression with Python Statsmodels: Assumptions and Interpretation",
    "section": "",
    "text": "Let’s say you are a real estate agent and want to know the price of houses based on their characteristics. You will need records of available homes, their features and prices, and you will use this data to estimate the price of a house based on those features.\nThis technique is known as regression analysis, and this article will focus specifically on linear regression. You will also learn about the requirements your data should meet, before you can perform a linear regression analysis using the Python library statsmodels , how to conduct the linear regression analysis, and interpret the results."
  },
  {
    "objectID": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html#what-is-linear-regression",
    "href": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html#what-is-linear-regression",
    "title": "Linear Regression with Python Statsmodels: Assumptions and Interpretation",
    "section": "What is Linear Regression?",
    "text": "What is Linear Regression?\nLinear regression is a statistical technique used to model the relationship between a continuous dependent variable(outcome) and one or more independent variables (predictors) by fitting a linear equation to the observed data. This allows us to understand how the outcome variable changes to the predictor variables.\n\nTypes of linear regression\nWe have various types of linear regression.\n\nSimple Linear Regression: This examines the relationship between a single outcome variable and a single predictor variable.\nMultiple Linear Regression: This examines the relationship between a single outcome variable and multiple predictor variables.\n\n\n\nAssumptions of linear regression\nBefore conducting a linear regression, our data should meet some assumptions:\n\nLinearity: The relationship between the outcome and predictor variables is linear. You can check this by plotting a graph of the predictor variable against the outcome variable and ensuring that the points on the graph form a straight line.\nIndependence: Observations are independent of each other. That is, the occurrence of subsequent observations does not depend on the occurrence of previous observations. This is checked by observing the structure of the data or how the data was collected.\nHomoscedasticity: The variance of the errors is constant across all levels of the independent variables. The difference between predicted and actual values is almost the same in all observations. The Breusch-Pagan test is one way to check for this assumption.\nNormality: The outcome variable is normally distributed. That is, its graph should have a curve similar to a bell-shaped curve. You can check for this assumption by plotting the graph of the outcome variable.\nNo Multicollinearity: In the case of multiple linear regression, independent variables are not highly correlated. If you have two predictors having a strong relationship, you need to drop one and select the other for your model. There are various ways of checking for this, such as using the correlation matrix or checking the variance inflation factor (VIF)."
  },
  {
    "objectID": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html#linear-regression-with-statsmodels",
    "href": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html#linear-regression-with-statsmodels",
    "title": "Linear Regression with Python Statsmodels: Assumptions and Interpretation",
    "section": "Linear Regression with Statsmodels",
    "text": "Linear Regression with Statsmodels\nstatsmodels is a Python library for statistical modeling, hypothesis testing, and data analysis. Take it as a Python library that makes Python feel like a statistical software. We will use it to demonstrate how to develop a linear regression model.\nBefore starting, ensure you have installed the following libraries:\n\npandas for data wrangling\nmatplotlib for data visualization\nseaborn to use the mpg dataset\nstatsmodels for regression analysis\n\nIn this tutorial, we will analyze the mpg dataset and perform a regression analysis to predict mpg (miles per gallon), measuring a car’s fuel efficiency. We will use the following variables as our predictor variables.\n\ndisplacement - is the total volume of all cylinders in the engine, measured in cubic inches.\nweight - is the weight of the car measured in pounds.\nacceleration is the time a car takes from 0 to 60 mph.\n\nLet’s load the needed libraries.\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport statsmodels.api as sm\nimport seaborn as sns\n\n%matplotlib inline\nLet’s load the mpg dataset and drop the row with missing values.\n# Load the 'mpg' dataset from seaborn\ndf = sns.load_dataset('mpg')\n\n# Drop rows with missing values\ndf = df.dropna()\n\ndf.head()\n\n\n\nLoad the mpg dataset. Image by Author\n\n\nLet’s define the outcome and predictor variables,\n# Define the dependent variable (y) and independent variables (X)\ny = df['mpg']\nX = df[['displacement', 'weight', 'acceleration']]"
  },
  {
    "objectID": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html#checking-for-assumptions",
    "href": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html#checking-for-assumptions",
    "title": "Linear Regression with Python Statsmodels: Assumptions and Interpretation",
    "section": "Checking for assumptions",
    "text": "Checking for assumptions\nBefore we perform the regression analysis, we need to check for some of the assumptions stated earlier.\n\nLinearity\nWe can use a scatterplot to check the linearity of the outcome and each independent variable.\n# Plot the X and y axis\nfig, axes = plt.subplots(nrows=1, ncols=len(X.columns), figsize=(20, 5))\nfig.suptitle(\"Scatter Plots with Linear Regression Lines\", fontsize=16)\n\n# Loop through each column in X and create a scatter plot with regression line\nfor i, col in enumerate(X.columns):\n    sns.regplot(ax=axes[i], x=X[col], y=y, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n    axes[i].set_title(f'{col} vs mpg')\n    axes[i].set_xlabel(col)\n    axes[i].set_ylabel('mpg')\n\nplt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit the subtitle\nplt.show()\n\n\n\nCheck for linearity of relationship between outcome and predictor variables. Image by Author.\n\n\nThe image above shows that mpg is linearly related to each predictor. If one of the plots were to show a curve shape, we would have to drop that predictor since it violates the assumption of linearity.\n\n\nIndependence\nWe can find this out from the nature of the data. For example, you have observations recorded based on time, such as hourly, daily, and so on. We can say these observations depend on each other, as we can’t have the next observation unless the previous one has already been recorded. However, in the case of the mpg dataset, the observations are drawn from various vehicles and are independent of each other. We can say that this particular assumption of independence is satisfied.\n\n\nNormality\nYou can check this assumption by plotting a density plot of the outcome variable.\nsns.kdeplot(y)\nplt.xlabel('mpg')\nplt.ylabel('Density')\nplt.title('Density Plot of mpg')\nplt.show()\n\n\n\nCheck for normality of the outcome variable. Image by Author.\n\n\nThe above plot shows that the variable mpg has a shape similar to a bell-shaped curve, which implies that this assumption is satisfied.\n\n\nNo Multicollinearity\nYou can investigate multicollinearity by calculating each predictor variable’s variance inflation factor (VIF). If you have predictors with VIF values greater or equal to five, you can plot a correlation matrix further to check the relationship between these variables.\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['features'] = X.columns\nprint(vif)\n\n\n\nCheck the VIF of each predictor. Image by Author.\n\n\nThe results show that all predictors have VIF values above five. Let’s plot a correlation matrix to see the relationship between these predictors.\n# Calculate the correlation matrix\ncorrelation_matrix = df[['displacement', 'weight', 'acceleration']].corr()\n\n# Plot the correlation matrix as a heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", cbar=True)\nplt.title(\"Correlation Matrix of Independent Variables\")\nplt.show()\n\n\n\nCorrelation matrix. Image by Author.\n\n\nYou can see that displacement and weight are highly correlated. Let’s drop the weight variable from our predictors and check the VIF values again.\n# Drop the weight variable\nX = df[['displacement', 'acceleration']]\n\n# Calculate VIF for each independent variable\n\nvif = pd.DataFrame()\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['features'] = X.columns\nprint(vif)\n\n\n\nRemove weight variable, and check VIF values again. Image by Author.\n\n\nThe VIF values from above show that there is no multicollinearity present in the predictor variables. We can now proceed to build the regression model."
  },
  {
    "objectID": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html#building-the-regression-model",
    "href": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html#building-the-regression-model",
    "title": "Linear Regression with Python Statsmodels: Assumptions and Interpretation",
    "section": "Building the regression model",
    "text": "Building the regression model\nBefore proceeding to the regression analysis, let’s add a constant to the independent variables. This is done to account for the regression model’s intercept, which is the value of the dependent variable when all independent variables are zero.\n# Add a constant to the independent variables\nX = sm.add_constant(X)\nLet’s fit the linear regression model.\n# Create and fit the OLS model\nmodel = sm.OLS(y, X)\nresults = model.fit()\n\n# Print the model summary\nprint(results.summary())\n\n\n\nSummary statistics of the regression model. Image by Author.\n\n\n\nHomoscedasticity\nTo check this assumption, we need the results, which we can only get from the summary results. According to the Breusch-Pagan test, if the p-value we get is greater than 0.05, we reject the null hypothesis and conclude that the variance of the errors is constant in all observations.\nfrom statsmodels.stats.diagnostic import het_breuschpagan\n\nbp_test = het_breuschpagan(results.resid, results.model.exog)\nbp_pvalue = bp_test[-1]\nbp_pvalue\n\n>> 1.0127670189356358e-05\nAlthough our data failed the test of homoscedasticity, we can still proceed with the regression model since most of the assumptions of linear regression are already satisfied."
  },
  {
    "objectID": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html#interpretation",
    "href": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html#interpretation",
    "title": "Linear Regression with Python Statsmodels: Assumptions and Interpretation",
    "section": "Interpretation",
    "text": "Interpretation\nThere are a lot of metric in the regression summary, but the most important are the regression coefficients and the R-squared values.\n\nIntercept and coefficients\nThe intercept coefficient 36.1882, is when all predictor variables are zero, that is, the mpg of a car with zero displacement and acceleration.\nThe displacement coefficient -0.0609 means that if a car’s displacement increases by a unit, its mpg will decrease by -0.0609 units. The p-value of 0.000, less than 0.05, means the relationship is statistically significant. This suggests that displacement and mpg are negatively related.\nIf the acceleration increases by one unit, the mpg is expected to decrease by 0.0582 units. However, the p-value is greater than 0.05, signifying that the relationship is not statistically significant.\n\n\n\nCoefficients of regression. Image of Author.\n\n\n\n\nCoefficient of determination\nThe R-squared indicates the amount of variability explained by the model, while the adjusted R-squared adjusts for the number of predictors in the model. It measures how well the model fits, and choosing the adjusted R-squared over the R-squared is advisable. The adjusted R-squared is 0.647, meaning the model explains 64.7% of the variability in the outcome variable mpg. This indicates a strong fit.\n\n\n\nR-squared and Adjusted R-squared. Image by Author."
  },
  {
    "objectID": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html#conclusion",
    "href": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html#conclusion",
    "title": "Linear Regression with Python Statsmodels: Assumptions and Interpretation",
    "section": "Conclusion",
    "text": "Conclusion\nLinear regression is just one of the many regression analyses, but it’s easy to conduct and interpret as long as all the model assumptions are met. With what you have learned in this article, I am sure you can apply linear regression to any data you choose and accurately interpret it. Here are some extra resources that explain more of what we have touched in this article.\nAdjusted R-Squared: A Clear Explanation with Examples\nR-squared vs Adjusted R-squared for Regression Analysis\nHomoscedasticity and heteroscedasticity\nHeteroskedasticity vs. Homoskedasticity→ Assumption of Linear Regression\nThe normality assumption in linear regression analysis\n\nNeed Help with Data? Let’s Make It Simple.\nAt LearnData.xyz, we’re here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we’ve got your back.\n📧 Shoot us an email at admin@learndata.xyz—let’s chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "Linear Regression with Python Statsmodels: Assumptions and Interpretation",
    "section": "Your next breakthrough could be one email away. Let’s make it happen!",
    "text": "Your next breakthrough could be one email away. Let’s make it happen!"
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Your next breakthrough could be one email away. Let’s make it happen!",
    "text": "Your next breakthrough could be one email away. Let’s make it happen!"
  },
  {
    "objectID": "posts/Securing ML APIs with FastAPI/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/Securing ML APIs with FastAPI/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "Securing ML APIs with FastAPI",
    "section": "Your next breakthrough could be one email away. Let’s make it happen!",
    "text": "Your next breakthrough could be one email away. Let’s make it happen!"
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/The Engineer's Guide to Low Code/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "Your next breakthrough could be one email away. Let’s make it happen!",
    "text": "Your next breakthrough could be one email away. Let’s make it happen!"
  },
  {
    "objectID": "posts/Forecasting Time Series Data With Facebook Prophet/index.html",
    "href": "posts/Forecasting Time Series Data With Facebook Prophet/index.html",
    "title": "Forecasting Time Series Data With Facebook Prophet in R",
    "section": "",
    "text": "Have you ever wondered how future prices of currencies, cryptocurrencies, or any other measures are predicted and accurately obtained? Well, this is forecasting at large: forecasting lets you get future values of a measure based on historical data. This historical data is also referred to as time series data, and this article will explain how to use the Facebook Prophet package in R to forecast future values of a measure."
  },
  {
    "objectID": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#prerequisites",
    "href": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#prerequisites",
    "title": "Forecasting Time Series Data With Facebook Prophet in R",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis tutorial will use India’s daily climate data from 2013 to 2017 to build a facebook prophet model. The data has four parameters;\n\nmeantemp\nhumidity\nwind_speed\nmeanpressure\n\nWe will limit the focus of this article to the meantemp measured in Celsius; you can try for other parameters as an exercise after going through this tutorial. The following are the packages we will need\n\ndplyr for data transformation\nggplot2 for data visualization\nprophet for building the forecasting model"
  },
  {
    "objectID": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#what-is-time-series-data",
    "href": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#what-is-time-series-data",
    "title": "Forecasting Time Series Data With Facebook Prophet in R",
    "section": "What is Time Series Data?",
    "text": "What is Time Series Data?\nTime series data is a collection of data points collected sequentially or chronologically, often with equal time intervals. These intervals are either hourly, daily, weekly, and so on. An example is the yearly child mortality in a country or the daily close price of Bitcoin. Time series data is applied in various fields, from forecasting mortality rates in healthcare to fraud detection in finance and pattern recognition.\n\nKey features of time series data\nBefore calling data, a time series data must have some features. Here is an outline of these features.\n\nChronological Order: Observations are ordered in time.\nTime Dependency: The current value in the series depends on past values.\nStationarity: The mean and variance are constant over time, indicating a consistent pattern.\nTrend: Long-term upward or downward movement.\nSeasonality: Regular and repeating patterns over specific intervals.\nNoise: Random variation or irregular patterns."
  },
  {
    "objectID": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#preparing-the-dataset",
    "href": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#preparing-the-dataset",
    "title": "Forecasting Time Series Data With Facebook Prophet in R",
    "section": "Preparing the Dataset",
    "text": "Preparing the Dataset\nLet’s load the libraries and data into our R session.\nlibrary(prophet)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nclimate_train <- read_csv(\"/kaggle/input/daily-climate-time-series-data/DailyDelhiClimateTrain.csv\")\nclimate_test <- read_csv(\"/kaggle/input/daily-climate-time-series-data/DailyDelhiClimateTest.csv\")\nprint(head(climate_train))\nprint(head(climate_test))\n\n\n\nPreview of the training and testing set. Image by Author.\n\n\nWe will use the climate_train to train the model and the climate_test to test it. The climate_train model contains data from 2013-01-01 to 2017-01-01, while the climate_test data ranges from 2017-01-01 to 2017-04-24.\nBefore building the Facebook prophet model, we need to subset the date and mean temp variables and rename them ds and y, respectively, as required by the prophet() function.\nclimate_train <- climate_train |> \n        select(date, meantemp) |>\n        rename(ds = date, y = meantemp) |>\n        mutate(ds = as.Date(ds))\n        \nclimate_test <- climate_test |> \n        select(date, meantemp) |>\n        rename(ds = date, y = meantemp) |>\n        mutate(ds = as.Date(ds))\n        \nprint(head(climate_train))\nprint(head(climate_test))\n\n\n\nPreview of the renamed training and test sets. Image by Author."
  },
  {
    "objectID": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#visualizing-time-series-data",
    "href": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#visualizing-time-series-data",
    "title": "Forecasting Time Series Data With Facebook Prophet in R",
    "section": "Visualizing Time Series Data",
    "text": "Visualizing Time Series Data\nLet’s plot the training data to see how the average daily temperature changes with time.\nggplot(climate_train, aes(x = ds, y = y)) +\n  geom_line(color = \"darkblue\") +\n  labs(\n    title = \"Average Daily Temperature\", \n    x = \"Time\",                          \n    y = \"Temperature\"                    \n  ) +\n  theme_minimal()\n\n\n\nAverage daily temperature, from 2013 - 2017. Image by Author.\n\n\nThe plot above shows temperatures ranging from around 10°C to 40°C. It shows a clear periodic pattern, indicating that temperatures rise and fall consistently yearly, typical of seasonal climate changes. Peaks represent summer months with higher temperatures around 30-40°C, while valleys represent winter months with lower temperatures around 10-15°C. The lowest temperature was recorded in the early months of 2013."
  },
  {
    "objectID": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#building-the-forecasting-model",
    "href": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#building-the-forecasting-model",
    "title": "Forecasting Time Series Data With Facebook Prophet in R",
    "section": "Building the Forecasting Model",
    "text": "Building the Forecasting Model\nLet’s build a forecasting model to predict future temperature values. We can achieve this by calling the prophet() function on the climate_train train dataset.\nmodel <- prophet(climate_train)\nNext, we need to predict future values. But before doing that, we need to define the future dates we want to predict, which are the dates on the climate_test dataset.\nfuture <- climate_test |>\n        select(ds)\n        \nhead(future)\n\n\n\nCreate future dates variable to use for forecast. Image by Author.\n\n\nThen, we use the predict() function to predict future temperature values based on the model we built earlier.\nforecast <- predict(model, future)\nThe predict() function returns a data frame with several variables, but we will subset the ones we need.\nforecast <- forecast[c('ds', 'yhat', 'yhat_lower', 'yhat_upper')]\nhead(forecast)\n\n\n\nPreview of the forecast values. Image by Author.\n\n\nds - is the date column\nyhat - is the predicted temperature value\nyhat_lower and yhat_upper - represent the lower and upper intervals where the actual temperature value should fall."
  },
  {
    "objectID": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#visualizing-future-forecast",
    "href": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#visualizing-future-forecast",
    "title": "Forecasting Time Series Data With Facebook Prophet in R",
    "section": "Visualizing Future Forecast",
    "text": "Visualizing Future Forecast\nYou can also visualize the forecast values.\nplot(model, forecast)\n\n\n\nVisualization of the forecast values. Image by Author.\n\n\nThe plot above shows that the forecast values exhibit a similar pattern to the trained values.\nThe Facebook prophet model also breaks down forecasts into components, where you can view the model trend and seasonality.\nprophet_plot_components(model, forecast)\n\n\n\nComponents of the forecast values. Image by Author.\n\n\nThe top panel in the plot above shows that the overall trend in average daily temperature increases from January to April, suggesting that the average temperature is gradually rising over this period.\nThe middle panel, which displays the weekly seasonality, illustrates how the average temperature varies by day of the week. Temperature peaks midweek (around Wednesday) and decreases slightly towards the weekend.\nThe bottom pane captures the average temperature across the year, indicating a seasonal cycle. The temperature rises from January, peaking around the middle of the year (likely summer), and then declines towards the end of the year (winter)."
  },
  {
    "objectID": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#measuring-forecast-accuracy",
    "href": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#measuring-forecast-accuracy",
    "title": "Forecasting Time Series Data With Facebook Prophet in R",
    "section": "Measuring Forecast Accuracy",
    "text": "Measuring Forecast Accuracy\nThere are various ways of measuring forecast accuracy, but we will use the Root Mean Square Error (RMSE) approach for this tutorial. RMSE measures forecasting accuracy by taking the mean of the squared difference of the actual value from the subtracted value and taking the square root.\n\\[\nRMSE = \\sqrt{\\frac{\\sum{(y - \\hat{y})^2}}{n}}\n\\]\nLet’s create a new data frame called comparison, which joins our forecast values with those of the climate_test data frame and subsets just ds, y, and yhat.\ncomparison <- climate_test |>\n        left_join(forecast, by=\"ds\") |>\n        select(ds, y, yhat)\n        \nhead(comparison)\n\n\n\nPreview of the comparison dataframe. Image by Author.\n\n\nWe are going to use the formula above to calculate the RMSE.\ncomparison$error <- comparison$y - comparison$yhat\n\n# Calculate performance RMSE\nrmse <- sqrt(mean(comparison$error^2))\nprint(paste(\"RMSE: \", rmse))\n[1] \"RMSE:  2.76289062761507\"\nThe RMSE value is approximately 2.76, which means that, on average, the model’s prediction deviates from the actual observed values by approximately 2.76°C. This indicates that the model is performing well."
  },
  {
    "objectID": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#conclusion",
    "href": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#conclusion",
    "title": "Forecasting Time Series Data With Facebook Prophet in R",
    "section": "Conclusion",
    "text": "Conclusion\nForecasting lets us answer many questions based on historical data. Unlike other prediction models, which are feature-based, forecasting is time-based and considers the change of a variable with time.\nThis tutorial taught you how to visualize and forecast time series data. Using what you learned in this tutorial, you can consider forecasting other weather parameters in the dataset or use any existing time series data. If you want to go further and have a deeper understanding of forecasting and time series, here are some invaluable resources.\nCreating Time Series Visualizations in R\nR Dygraphs: How to Visualize Time Series Data in R and R.\nTime Series Analysis in R: How to Read and Understand Time Series Data\nFacebook Prophet Documentation\nUsing R for Time Series Analysis\nFundamentals of time series analysis with R\n14 Time Series Analysis\n\nNeed Help with Data? Let’s Make It Simple.\nAt LearnData.xyz, we’re here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we’ve got your back.\n📧 Shoot us an email at admin@learndata.xyz—let’s chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "Forecasting Time Series Data With Facebook Prophet in R",
    "section": "Your next breakthrough could be one email away. Let’s make it happen!",
    "text": "Your next breakthrough could be one email away. Let’s make it happen!"
  },
  {
    "objectID": "posts/Introduction to Kaplan-Meier Survival Analysis Estimation with Python/index.html",
    "href": "posts/Introduction to Kaplan-Meier Survival Analysis Estimation with Python/index.html",
    "title": "Introduction to Kaplan-Meier Survival Analysis Estimation with Python",
    "section": "",
    "text": "Traditional linear and logistic regression methods have been shown to predict outcomes with minimal bias over time. But there is a caveat: These methods don’t account for time-dependent outcomes. In this article, you will learn more about Kaplan-Meier survival analysis estimation, its applications, and how to use it to analyze data using the survival analysis Python library lifelines."
  },
  {
    "objectID": "posts/Introduction to Kaplan-Meier Survival Analysis Estimation with Python/index.html#what-is-survival-analysis",
    "href": "posts/Introduction to Kaplan-Meier Survival Analysis Estimation with Python/index.html#what-is-survival-analysis",
    "title": "Introduction to Kaplan-Meier Survival Analysis Estimation with Python",
    "section": "What is Survival Analysis?",
    "text": "What is Survival Analysis?\nImagine a clinical study where patients are given a new cancer treatment, and you want to analyze how long it takes for them to relapse.\nLet’s say the key variables are the type of treatment, age of the patient, time to relapse, and censoring; that is, patients might relapse at the end of the study with no idea of their relapse time, or some might die or drop out before the end of the study.\nUnlike traditional regression models, we might want to model the time to relapse using the treatment type and age. However, this approach has a lot of setbacks:\n\nIf a patient has not relapsed by the end of the study, their time is censored. We have to either exclude these patients from the study, which can lead to biased estimates, or treat these censored patients as if they had relapsed before the study’s end, which is incorrect.\nThe risk of relapse is not constant over time, which means the model does not account for the fact that patients might be at different risks at different times.\nThe data may or may not be normally distributed, and linear regression assumes the normality of errors.\nFor logistic regression, it only tells us if a patient relapsed or not but does not tell us when the relapse happened.\n\nSurvival analysis is designed explicitly for time-dependent data and addresses all the above setbacks of traditional regression models. It is a branch of statistics that focuses on analyzing the time until an event of interest occurs. This event can be death, disease relapse, equipment failure, customer churn, or any other time-dependent event."
  },
  {
    "objectID": "posts/Introduction to Kaplan-Meier Survival Analysis Estimation with Python/index.html#terminologies-in-survival-analysis",
    "href": "posts/Introduction to Kaplan-Meier Survival Analysis Estimation with Python/index.html#terminologies-in-survival-analysis",
    "title": "Introduction to Kaplan-Meier Survival Analysis Estimation with Python",
    "section": "Terminologies in Survival Analysis",
    "text": "Terminologies in Survival Analysis\nBefore conducting a survival analysis, there are some terminologies you should be familiar with:\n\nSurvival Time: This is the duration until an event of interest occurs.\nEvent: This is the occurrence of the outcome of interest.\nCensoring: This is incomplete information about a subject, either because the subject experienced the event of interest after the end of the study or the subject dropped out of the study. For example, a patient changing hospital during a cancer study on treatment relapse or the patient relapsing after the end of the study. There are three types of censoring:\n\nRight Censoring: The event of interest did not occur before the end of the study.\nLeft Censoring: The event of interest occurred before the start of the observation period or the subject dropped out of the study.\nInterval Censoring: Events of interest occur within a specific time interval, but the exact time is unknown.\n\nSurvival Function: The probability that a subject will survive beyond a specific time t.\nSurvival Curve: This graphical representation of the survival function over time."
  },
  {
    "objectID": "posts/Introduction to Kaplan-Meier Survival Analysis Estimation with Python/index.html#what-is-kaplan-meier-estimation",
    "href": "posts/Introduction to Kaplan-Meier Survival Analysis Estimation with Python/index.html#what-is-kaplan-meier-estimation",
    "title": "Introduction to Kaplan-Meier Survival Analysis Estimation with Python",
    "section": "What is Kaplan-Meier Estimation?",
    "text": "What is Kaplan-Meier Estimation?\nKaplan-Meier is a statistical methodology used to estimate the survival function from time-to-event data. The survival probability at the time \\(t_i\\) is given as:\n\\[\nS(t_i) = \\prod_{j=1}^{i}(1-\\frac{d_j}{n_j})\n\\]\nWhere:\n\n\\(t_i\\) - Time of the \\(i_{th}\\) event\n\\(d_j\\) - Number of events (e.g., deaths) at \\(t_i\\)\n\\(n_j\\) - Number of individuals at risk just before \\(t_i\\).\n\nDon’t worry; you won’t have to calculate this by hand. The lifelines Python library has classes and methods to help you perform a Kaplan-Meier Estimation."
  },
  {
    "objectID": "posts/Introduction to Kaplan-Meier Survival Analysis Estimation with Python/index.html#case-study-employee-churn",
    "href": "posts/Introduction to Kaplan-Meier Survival Analysis Estimation with Python/index.html#case-study-employee-churn",
    "title": "Introduction to Kaplan-Meier Survival Analysis Estimation with Python",
    "section": "Case Study: Employee Churn",
    "text": "Case Study: Employee Churn\nIn this article, we will use the Telco customer churn dataset to demonstrate how to perform a Kaplan-Meier estimation by estimating customer retention and also find out which categories of customer type are likely to churn. Before loading the data, ensure the following libraries are installed.\n\npandas\nmatplotlib\nlifelines\n\nLet’s add the following imports, load the data, and preview the main variables of interest.\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\ntelco_df = pd.read_csv(\"/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\ntelco_df.loc[:,[\"tenure\",\"Churn\"]].head()\n\n\n\nPreview of the tenure and Churn variables. Image by Author.\n\n\nFor a survival analysis, we are interested in two key variables, event and time, which are Churn and tenure in the dataset, where tenure is measured in months.\nRecode the Churn variable, so 1 represents churn, while 0 represents censoring. Censoring, in this case, means that a customer might churn in the future.\ntelco_df[\"Churn\"] = telco_df[\"Churn\"].map({\"No\":0,\"Yes\":1})\ntelco_df.loc[:,[\"tenure\",\"Churn\"]].head() \n\n\n\nRecode Churn variable into numeric format. Image by Author.\n\n\nRename the tenure and Churn variables to time and event. This is unnecessary, but it is just to make it easy to relate to the concepts discussed earlier.\ntime = telco_df[\"tenure\"]\nevent = telco_df[\"Churn\"]\n\nKaplan-Meier Estimation\nImport the KaplanMeierFitter() class from the lifelines library to perform a Kaplan-Meier estimation. Then, we create an instance of the class and fit it into our data.\nfrom lifelines import KaplanMeierFitter\n\nkmf = KaplanMeierFitter()\nkmf.fit(time, event_observed=event)\nYou can get the Kaplan-Meier estimate for each timeline by calling the survival_function_ method on the kmf object.\nkmf.survival_function_\n\n\n\nKaplan-Meier estimate for each timeline. Image by Author.\n\n\nYou can also get a plot of these values by calling the plot_survival_function method.\nkmf.plot_survival_function()\nplt.title(\"Survival function of customer churn\")\n\n\n\nSurvival function of customer churn. Image by Author.\n\n\n\n\nSurvival Estimate by Group\nWhat if we are interested in seeing the survival plots of various groups in the dataset? You can achieve this by filtering and fitting a survival function for each group and combining it into a single plot. For example, let’s see the survival plot of customers based on whether they have partners or not.\npartner = telco_df[\"Partner\"]\nix = (partner == \"Yes\")\n\nkmf.fit(time[~ix], event[~ix], label = \"No\")\nax = kmf.plot_survival_function()\n\nkmf.fit(time[ix], event[ix], label = \"Yes\")\nax = kmf.plot_survival_function()\n\n\n\nSurvival plot based on if customers have partners or not. Image by Author.\n\n\nFor variables with more than two groups, you can follow the following approach. Let’s examine each group based on PaymentMethod and see which payment method group will have the highest retention.\npayment_methods = telco_df[\"PaymentMethod\"].unique()\n\nfor i, payment_method in enumerate(payment_methods):\n    ax = plt.subplot(2, 3, i + 1)\n\n    ix = telco_df['PaymentMethod'] == payment_method\n    kmf.fit(time[ix], event[ix], label=payment_method)\n    kmf.plot_survival_function(ax=ax, legend=False)\n\n    plt.title(payment_method)\n    plt.xlim(0, 50)\n\nplt.tight_layout()\n\n\n\nSurvival estimates by payment method. Image by Author.\n\n\n\n\nAdd a Descriptive Table Below the Plot\nWhen publishing results, following the plots with descriptive tables is encouraged. This table shows the number of customers who will churn at various time intervals, censored customers, and customers at risk of churning.\npartner = telco_df[\"Partner\"]\nix = (partner == \"Yes\")\n\nax = plt.subplot(111)\n\nkmf_yes = KaplanMeierFitter()\nax = kmf_yes.fit(telco_df.loc[ix][\"tenure\"], telco_df.loc[ix][\"Churn\"],label = \"Yes\").plot_survival_function(ax = ax)\n\nkmf_no = KaplanMeierFitter()\nax = kmf_no.fit(telco_df.loc[~ix][\"tenure\"], telco_df.loc[~ix][\"Churn\"],label = \"No\").plot_survival_function(ax = ax)\n\nfrom lifelines.plotting import add_at_risk_counts\nadd_at_risk_counts(kmf_yes, kmf_no, ax=ax)\nplt.tight_layout()\n\n\n\nSurvival estimate by partners with descriptive table. Image by Author.\n\n\n\n\nInterpreting Kaplan-Meier Plots\nInterpreting the Kaplan-Meier plot is easy since it’s a descriptive statistical methodology. Let’s take a look at the first plot we had.\n\n\n\nSurvival estimate at mid-duration and end of timeline. Image by Author.\n\n\nIn the plot above, the y-axis represents the number of customers remaining at a given time. The survival probability starts at 1 (100%) at time 0, meaning all customers are present initially. As time progresses, the likelihood of customer retention decreases. The downward trend shows that customer retention is continuous over time.\nThe shaded area indicates the confidence interval for the Kaplan-Meier survival analysis estimate, where a narrower band implies higher confidence and a wider band suggests more uncertainty. Around 75% of customers remain by mid-duration (1). At the end of the timeline, 60% of customers are still active (2), showing that the business retains a significant portion of customers over the period.\n\n\n\nSurvival estimate by partners. Image by Author.\n\n\nThe plot above groups the previous plot into two categories: those with and without partners. The orange curve represents those with partners, while the blue represents those without partners.\nThose with partners have a higher retention probability than those without partners. Their survival probability is around 70% by the end of the timeline.\nThose without partners exhibit a steeper decline in survival probability, indicating they are more likely to churn faster. By the end of the timeline, their survival probability is approximately 45%.\nThis gap between the two curves highlights that the presence of a partner is a significant factor influencing customer retention.\nThe table below the plot further explains the plot. At time zero, the starting population was 3,393 customers with partners and 3,639 without partners. Nine were censored with zero events for customers with partners, while two were censored with zero events for those without partners.\nAt the end of the timeline, 452 customers with partners are at risk of churning, 2,292 have been censored, and 658 churned. For those without partners, 80 are at risk of churning, 2,362 have been censored, and 1,199 have churned.\nThese results show that customers with partners have a higher retention rate than those without partners. This could be due to shared decision-making or factors like excellent financial stability or dual service usage.\n\n\n\nSurvival estimate by payment methods. Image by Author.\n\n\nThe plot above also compares customer retention across four different payment methods. Electronic checks show the poorest retention rate, with a sharp initial and continuous decline. By timeline 50, with a customer retention rate of about 45%.\nMailed check customers show better retention than electronic check users but still demonstrate concerning churn patterns. Starting with a sharp initial drop, the curve continues to decline more gradually, reaching about 75% retention by the end of the period. This suggests that while mailed check users are more stable than electronic check users, they still represent a higher risk group than automatic payment methods.\nBank transfer (automatic) shows strong retention patterns. The curve declines gradually, maintaining approximately 85% retention by the end of the observation period. This suggests that customers who set up automatic bank transfers are significantly more likely to remain loyal customers.\nCredit card (automatic) payments show similarly strong retention patterns to automatic bank transfers. The survival curve remains high, ending at roughly 85% retention. The gradual decline and narrow confidence intervals suggest that automatic credit card payments, like automatic bank transfers, are associated with more stable, long-term customer relationships.\nOverall, a clear pattern shows that automatic payment methods (bank transfer and credit card) are associated with significantly better customer retention than manual payment methods (electronic and mailed checks)."
  },
  {
    "objectID": "posts/Introduction to Kaplan-Meier Survival Analysis Estimation with Python/index.html#add-a-descriptive-table-below-the-plot",
    "href": "posts/Introduction to Kaplan-Meier Survival Analysis Estimation with Python/index.html#add-a-descriptive-table-below-the-plot",
    "title": "Introduction to Kaplan-Meier Survival Analysis Estimation with Python",
    "section": "Add a Descriptive Table Below the Plot",
    "text": "Add a Descriptive Table Below the Plot\nWhen publishing results, following the plots with descriptive tables is encouraged. This table shows the number of customers who will churn at various time intervals, censored customers, and customers at risk of churning.\npartner = telco_df[\"Partner\"]\nix = (partner == \"Yes\")\n\nax = plt.subplot(111)\n\nkmf_yes = KaplanMeierFitter()\nax = kmf_yes.fit(telco_df.loc[ix][\"tenure\"], telco_df.loc[ix][\"Churn\"],label = \"Yes\").plot_survival_function(ax = ax)\n\nkmf_no = KaplanMeierFitter()\nax = kmf_no.fit(telco_df.loc[~ix][\"tenure\"], telco_df.loc[~ix][\"Churn\"],label = \"No\").plot_survival_function(ax = ax)\n\nfrom lifelines.plotting import add_at_risk_counts\nadd_at_risk_counts(kmf_yes, kmf_no, ax=ax)\nplt.tight_layout()\n\n\n\nSurvival estimate by partners with descriptive table. Image by Author.\n\n\n\nInterpreting Kaplan-Meier Plots\nInterpreting the Kaplan-Meier plot is easy since it’s a descriptive statistical methodology. Let’s take a look at the first plot we had.\n\n\n\nSurvival estimate at mid-duration and end of timeline. Image by Author.\n\n\nIn the plot above, the y-axis represents the number of customers remaining at a given time. The survival probability starts at 1 (100%) at time 0, meaning all customers are present initially. As time progresses, the likelihood of customer retention decreases. The downward trend shows that customer retention is continuous over time.\nThe shaded area indicates the confidence interval for the Kaplan-Meier survival analysis estimate, where a narrower band implies higher confidence and a wider band suggests more uncertainty. Around 75% of customers remain by mid-duration (1). At the end of the timeline, 60% of customers are still active (2), showing that the business retains a significant portion of customers over the period.\n\n\n\nSurvival estimate by partners. Image by Author.\n\n\nThe plot above groups the previous plot into two categories: those with and without partners. The orange curve represents those with partners, while the blue represents those without partners.\nThose with partners have a higher retention probability than those without partners. Their survival probability is around 70% by the end of the timeline.\nThose without partners exhibit a steeper decline in survival probability, indicating they are more likely to churn faster. By the end of the timeline, their survival probability is approximately 45%.\nThis gap between the two curves highlights that the presence of a partner is a significant factor influencing customer retention.\nThe table below the plot further explains the plot. At time zero, the starting population was 3,393 customers with partners and 3,639 without partners. Nine were censored with zero events for customers with partners, while two were censored with zero events for those without partners.\nAt the end of the timeline, 452 customers with partners are at risk of churning, 2,292 have been censored, and 658 churned. For those without partners, 80 are at risk of churning, 2,362 have been censored, and 1,199 have churned.\nThese results show that customers with partners have a higher retention rate than those without partners. This could be due to shared decision-making or factors like excellent financial stability or dual service usage.\n\n\n\nSurvival estimate by payment methods. Image by Author.\n\n\nThe plot above also compares customer retention across four different payment methods. Electronic checks show the poorest retention rate, with a sharp initial and continuous decline. By timeline 50, with a customer retention rate of about 45%.\nMailed check customers show better retention than electronic check users but still demonstrate concerning churn patterns. Starting with a sharp initial drop, the curve continues to decline more gradually, reaching about 75% retention by the end of the period. This suggests that while mailed check users are more stable than electronic check users, they still represent a higher risk group than automatic payment methods.\nBank transfer (automatic) shows strong retention patterns. The curve declines gradually, maintaining approximately 85% retention by the end of the observation period. This suggests that customers who set up automatic bank transfers are significantly more likely to remain loyal customers.\nCredit card (automatic) payments show similarly strong retention patterns to automatic bank transfers. The survival curve remains high, ending at roughly 85% retention. The gradual decline and narrow confidence intervals suggest that automatic credit card payments, like automatic bank transfers, are associated with more stable, long-term customer relationships.\nOverall, a clear pattern shows that automatic payment methods (bank transfer and credit card) are associated with significantly better customer retention than manual payment methods (electronic and mailed checks)."
  },
  {
    "objectID": "posts/Introduction to Kaplan-Meier Survival Analysis Estimation with Python/index.html#conclusion",
    "href": "posts/Introduction to Kaplan-Meier Survival Analysis Estimation with Python/index.html#conclusion",
    "title": "Introduction to Kaplan-Meier Survival Analysis Estimation with Python",
    "section": "Conclusion",
    "text": "Conclusion\nWell, that was a quick introduction to Kapler-Meier survival analysis estimation. I hope by now you can pick any dataset of your choice and implement what you have learned in this article, like estimating the survival function at different timelines and plotting the survival curve. If you want to know more, here are some resources I hope you find helpful.\n12. Survival analysis\nWhat is survival analysis, and when should I use it? - PMC\nThe Ultimate Guide to Survival Analysis\nIntroduction to Survival Analysis with scikit-survival\nThe Kaplan Meier estimate in survival analysis\n\nNeed Help with Data? Let’s Make It Simple.\nAt LearnData.xyz, we’re here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we’ve got your back.\n📧 Shoot us an email at admin@learndata.xyz—let’s chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/Introduction to Kaplan-Meier Survival Analysis Estimation with Python/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/Introduction to Kaplan-Meier Survival Analysis Estimation with Python/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "Introduction to Kaplan-Meier Survival Analysis Estimation with Python",
    "section": "Your next breakthrough could be one email away. Let’s make it happen!",
    "text": "Your next breakthrough could be one email away. Let’s make it happen!"
  },
  {
    "objectID": "posts/Model Deployment in R with Plumber/index.html",
    "href": "posts/Model Deployment in R with Plumber/index.html",
    "title": "Model Deployment in R with Plumber",
    "section": "",
    "text": "The primary aim of machine learning is to learn certain patterns from available data and to make decisions based on reliable facts.\nImagine building a machine learning model, and you get good accuracy. That does not end there; you must deploy this model for clients to interact with. Maybe it’s a classification or a regression model. What use do you think the model is if it’s just on your PC?\nDeploying machine learning models puts them into real-life scenarios, allowing you to monitor them in real-time. This is also important as you can easily track anomalies through the model’s accuracy metrics.\nIn this article, you will learn how to deploy a machine-learning model using Plumber, an R package for developing APIs. You will build a Facebook prophet model forecasting the closing price of Bitcoin and deploy it as a Plumber API such that when a date is given, you can get the closing price for that particular date."
  },
  {
    "objectID": "posts/Model Deployment in R with Plumber/index.html#prerequisites",
    "href": "posts/Model Deployment in R with Plumber/index.html#prerequisites",
    "title": "Model Deployment in R with Plumber",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo follow this tutorial, download the Bitcoin dataset containing Bitcoin’s closing price information from January 1st, 2020, to April 19th, 2021. You also need to install the following Python libraries.\n\nplumber - for generating APIs\nprophet - for time series analysis\nlubridate - for date manipulation\ndplyr - for data manipulation"
  },
  {
    "objectID": "posts/Model Deployment in R with Plumber/index.html#what-is-plumber",
    "href": "posts/Model Deployment in R with Plumber/index.html#what-is-plumber",
    "title": "Model Deployment in R with Plumber",
    "section": "What is Plumber?",
    "text": "What is Plumber?\nR plumber is an R package for creating web APIs by decorating your R functions. For example, you can have a normal function that calculates the area of a rectangle.\narea_of_rectangle <- function(length, breadth) {\n  return(length * breadth)\n}\nAnd convert the function into an API by decorating it with roxygen2comments #*.\nlibrary(plumber)\n\n#* Return the Area of a rectangle\n#* @param length numeric\n#* @param breadth numeric\n#* @get /area_of_rectangle\nfunction(length, breadth) {\n  return(as.numeric(length) * as.numeric(breadth))\n}\n\n#* Return the Area of a rectangle is a comment describing what the function does.\n#* @param length numeric indicates that the function takes a numeric parameter called length.\n#* @param length numeric also shows that the function takes another numeric parameter called breadth.\n#* @get/area_of_rectangle tells Plumber to create a web endpoint at the URL path /area_of_rectangle. The function is triggered when this URL is visited with the correct parameters.\n\nTo run the newly created API, click Run API just above the R script to open the API endpoint docs automatically.\n\n\n\nExample of an R API. Image by Author.\n\n\nYou can interact with the API’s automatic documentation and see if they return valid results or errors.\n\n\n\nAutomatic API docs. Image by Author.\n\n\nYou can also assess the value of the area directly on your browser through the following URL <your_local_host>/area_of_rectangle?length=5&breadth=12. Because the endpoint is a GET verb, the function parameters are known as query parameters and are provided in the URL path after a question mark.\n\n\n\nSample API URL. Image by endpoint.\n\n\n\nCreating a Plumber project\nTo create a plumber project, go to File, then New Project. Under the New Project Wizard, select New Directory and the New Plumber API Project.\n\n\n\nCreate a Plumber project. Image by Author.\n\n\nOnce you have created the Plumber API Project, you will see a file called plumber.R . This is where you will write all the logic needed to create the API."
  },
  {
    "objectID": "posts/Model Deployment in R with Plumber/index.html#building-the-forecasting-model",
    "href": "posts/Model Deployment in R with Plumber/index.html#building-the-forecasting-model",
    "title": "Model Deployment in R with Plumber",
    "section": "Building the Forecasting Model",
    "text": "Building the Forecasting Model\nRun the following code in the plumber.R to load the libraries and data.\nlibrary(plumber)\nlibrary(prophet)\nlibrary(dplyr)\nlibrary(lubridate)\n\n# Load and preprocess the data\nbitcoin_data <- read.csv(\"gemini_BTCUSD_2020_1min.csv\")\n\nhead(bitcoin_data)\n\n\n\nimage.png\n\n\nCopy and paste the following code below to build the forecasting model.\n# Ensure the Date column is in datetime format\nbitcoin_data$Date <- mdy_hm(bitcoin_data$Date)\n\n# Extract only the date part (ignoring time)\nbitcoin_data$Date <- floor_date(bitcoin_data$Date, unit = \"day\")\n\n# Calculate the daily average of the 'Close' prices\ndaily_data <- bitcoin_data |>\n  group_by(Date) |>\n  summarise(Close = mean(Close))\n\n# Prepare the data for Prophet\nprophet_data <- data.frame(ds = daily_data$Date, y = daily_data$Close)\n\n# Train the Prophet model\nmodel <- prophet(prophet_data)\nCheck out the following article to learn more about building forecasting models with the Facebook Prophet."
  },
  {
    "objectID": "posts/Model Deployment in R with Plumber/index.html#creating-the-plumber-api",
    "href": "posts/Model Deployment in R with Plumber/index.html#creating-the-plumber-api",
    "title": "Model Deployment in R with Plumber",
    "section": "Creating the Plumber API",
    "text": "Creating the Plumber API\nInside the plumber.R file, copy and paste the following code.\n#* Predict closing prices for a given date\n#* @param date A future date (format: YYYY-MM-DD) to predict closing price for\n#* @get /predict\nfunction(date) {\n  # Convert the input date to a Date object\n  input_date <- ymd(date)\n\n  # Get the last available date in the dataset\n  last_date <- ymd(max(prophet_data$ds))\n\n  # Validate the input date\n  if (is.na(input_date)) {\n    return(list(\n      error = \"Invalid date format. Please provide a date in YYYY-MM-DD format.\"\n    ))\n  }\n\n  if (input_date <= last_date) {\n    return(list(\n      error = \"Please provide a future date beyond the last available date in the dataset.\",\n      last_available_date = as.character(last_date)\n    ))\n  }\n\n  # Calculate the number of days to forecast\n  days_to_forecast <- input_date - last_date\n\n  # Check if the number of days to forecast is valid\n  if (days_to_forecast <= 0) {\n    return(list(\n      error = \"The calculated forecast period is negative. Please check your input.\"\n    ))\n  }\n\n  # Generate future predictions\n  future <- make_future_dataframe(model, periods = days_to_forecast)\n  forecast <- predict(model, future)\n\n  # Find the prediction for the input date\n  forecast_for_date <- forecast |>\n    filter(ds == input_date) |>\n    select(ds, yhat, yhat_lower, yhat_upper)\n\n  # Return the prediction for the given date\n  if (nrow(forecast_for_date) == 0) {\n    return(list(\n      error = \"Unable to generate forecast for the given date. Please try a valid future date.\"\n    ))\n  } else {\n    return(forecast_for_date)\n  }\n}\nThe above function predicts the closing price of Bitcoin for a specified future date. It uses a GET verb at the endpoint /predict and takes the argument date in the format YYYY-MM-DD.\nFirst, input_date is converted to a date-time object using the lubridate function ymd(). The same is done for the last_date in the dataset.\nThe input_date is also validated in case no date was given, or the date given is less than or equal to the last_date in the dataset.\nThe days_to_forecast is calculated by subtracting the last date from the input date, which is validated if it is less than or equal to 0.\nThe predict function makes future predictions by taking the model and the future data frame as arguments.\nFinally, the date, actual predictions of the input data, and confidence intervals are filtered from the forecast data frame and returned.\nLet’s run the API to interact with the forecast model API docs.\n\n\n\nRun and open the API automatically generated docs. Image by Author."
  },
  {
    "objectID": "posts/Model Deployment in R with Plumber/index.html#hosting",
    "href": "posts/Model Deployment in R with Plumber/index.html#hosting",
    "title": "Model Deployment in R with Plumber",
    "section": "Hosting",
    "text": "Hosting\nR Plumber provides various ways to host your API. You can use Posit Connect, which is recommended as the most straightforward deployment method, Digital Ocean, or deploy using Docker. Once you have hosted your API, clients can interact with it to get real-time predictions. You can further set up tests and CI/CD for automated testing to ensure that as your application grows large, it does not break."
  },
  {
    "objectID": "posts/Model Deployment in R with Plumber/index.html#conclusion",
    "href": "posts/Model Deployment in R with Plumber/index.html#conclusion",
    "title": "Model Deployment in R with Plumber",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, you have learned how to use R Plumber to deploy your machine-learning models as an API. You can integrate these APIs into web applications and allow users to interact with the model. This way, you solve business problems and see how your model performs on real-world data. If you want to dive more into developing machine learning APIs, here are useful resources from which you can learn.\nRecreating the Shiny App tutorial with a Plumber API + React\nHow to put an R model in production\nDeploying a Machine Learning Model Using Plumber and Docker\nHow to deploy a Tensorflow Model in R with Plumber\nDeploying to RStudio Connect: A Beginner’s Guide\n**How to Make an R REST API: A Beginners Guide to Plumber**\n\nNeed Help with Data? Let’s Make It Simple.\nAt LearnData.xyz, we’re here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we’ve got your back.\n📧 Shoot us an email at admin@learndata.xyz—let’s chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/Model Deployment in R with Plumber/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/Model Deployment in R with Plumber/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "Model Deployment in R with Plumber",
    "section": "Your next breakthrough could be one email away. Let’s make it happen!",
    "text": "Your next breakthrough could be one email away. Let’s make it happen!"
  }
]