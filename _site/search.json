[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Monitoring Model Performance and Data Drift for Diabetes Classification\n\n\n\nmlops\n\n\ndata drift\n\n\nclassification\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "navs/about.html",
    "href": "navs/about.html",
    "title": "About",
    "section": "",
    "text": "In an era where there is a huge and vast amount of data, the need for professionals who can manage and make sense of this data is needed, either by describing, modelling or identifying patterns in data.\nAs someone who has worked as a freelance Data Scientist for over 5 years in the data industry, I aim to train and educate you on how you can become one of the best data professionals out there either as a Data Analyst, Data Scientist or Machine Learning Engineer.\nI graduated with a BSc in Statistics and currently working as a Biostatistician at a public hospital assisting medical researchers in analysing health data. As a technical writer, I have written various articles and have thousands of reads on medium.\nI am proficient in Mathematics, Statistics and Programming (R and Python) and have a strong domain knowledge of the health sector.\nAs a data scientist, my day-to-day activities range from performing exploratory data analysis to building machine learning or deep learning models.\nMy biggest happiness comes when students benefit from my courses and I hope as you are reading this, you have either enrolled or are about to enroll in one of my courses.\nI wish you all the best in your path to becoming a world-class data professional!\nStart your journey today!"
  },
  {
    "objectID": "navs/courses.html",
    "href": "navs/courses.html",
    "title": "Courses",
    "section": "",
    "text": "ML Model Deployment with FastAPI and Streamlit\nBuilding Interactive Shiny Web Apps with R Programming\nData Wrangling and Exploratory Data Analysis with R\nPower BI DAX Practice Test and Solutions"
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "",
    "text": "According to WHO, the number of people with diabetes rose from 108 million in 1980 to 422 million in 2014. Diabetes is a serious disease that leads to blindness, kidney failure, heart attacks, strokes, and lower limb amputations. It is mostly prevalent in low- and middle-income countries.\nBuilding a healthcare system that uses machine learning to predict patients with diabetes will help in early detection making it easy for healthcare providers to screen patients with diabetes at an early stage, before diagnosis.\n\nMachine learning models tend to degrade with time, highlighting the need for effective and constant monitoring of the model to know when its performance is declining. Often, this arises as a result of the change in the distribution of the data compared to the data the model was trained upon, this phenomenon is known as Data Drift.\nIn this article, you will learn how to monitor a diabetes classifier and detect data drifts in the data received from patients in a health information management system or mobile application using NannyML.\nNannyML is an open-source library for monitoring machine learning model performance in production, even without the predicted values being ready. It allows you to track your machine-learning model over time and see checkpoints where the model degrades."
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#estimating-model-performance-with-nannyml",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#estimating-model-performance-with-nannyml",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Estimating Model Performance with NannyML",
    "text": "Estimating Model Performance with NannyML\nNannyMl offers binary class classification that one could use to estimate the model’s performance, even without targets. Model estimation performance with NannyML involves:\n\nGetting the reference and analysis sets ready: The reference set is the data where the model behaves as expected, usually the test data. The analysis set is the latest production data, either with target features or not.\nTraining a performance estimator on the reference set: NannyML uses the reference set to train a performance estimator, it’s advisable to use the test data as reference data to prevent overfitting.\nUsing the estimator to predict performance on the analysis set (simulating real-world data): NannyML estimates the model performance on the analysis data using the trained performance estimator. One can use various classification metrics, such as accuracy or F1-score. Since misclassifying a patient (false negative) is more severe than misclassifying a healthy patient as diabetic (false positive), the AUC-ROC is the most appropriate metric to use in this case."
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#detecting-data-drift-with-nannyml",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#detecting-data-drift-with-nannyml",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Detecting Data Drift with NannyML",
    "text": "Detecting Data Drift with NannyML\nLet’s say you deployed a machine-learning model. As time goes on, the model tends to degrade, This is due to the nature of the data changing. If you have an application that you initially designed for kids and you train most of your machine learning models using your current user’s data, then all of a sudden middle-aged people and the elderly start using your application, and they become more of your users than the kids you designed it for. This will change the age distribution of your data, If age is one important feature in your machine learning model, your model will get worse with time. This is where you need to monitor when such changes happen in your data so that you can update the ML model.\n\nNannyML uses various algorithms to detect data drift, either using Univariate drift detection or Multivariate drift detection methods.\n\nUnivariate drift detection: In this approach, NannyML looks at each feature used in classifying if a patient is diabetic, and compares the chunks with those created from the analysis period. The result of the comparison is called a drift metric, and it is the amount of drift between the reference and analysis chunks, which is calculated for each chunk.\nMultivariate Drift Detection Instead of taking every feature one by one, NannyML provides a single summary metric explaining the drift between the reference and the analysis sets. Although this approach can detect slight changes in the data, it is difficult to explain compared to univariate drift.\n\nIn the case of classifying diabetic patients, undetected drift is dangerous and can lead to wrong model classifications. This is worse if the number of false negatives is high, the classifier might not detect some patients with diabetes, this can lead to late diagnosis."
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#estimating-model-performance-in-the-diabetes-classifier",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#estimating-model-performance-in-the-diabetes-classifier",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Estimating Model Performance in the Diabetes Classifier",
    "text": "Estimating Model Performance in the Diabetes Classifier\nNannyML uses two main approaches to estimate model performance, Confidence-based Performance estimation (CBPE) and Direct Loss estimation (DLE). In this case, we are interested in using the CBPE, since we are dealing with a classification task.\nThe CBPE uses the confidence score of the predictions to estimate the model performance, the confidence score is a value that the diabetes classifier gives for each predicted observation, expressing its confidence in predicting if a patient is diabetic., with values ranging from 0 to 1 and the closer it is to 1, the more confident the classifier is with it’s prediction.\nThe diabetes data contains 253,680 responses and 21 features. In this section, you will learn how to use this data to build an ML model, estimate your model’s performance, and detect data drift on updated data.\n\nProject Requirements\nTo get started, ensure you have installed NannyML on your JupyterNotebook. Download the analysis and diabetes data. The diabetes data is the data you will train the machine learning model on, and the analysis data is what you would take as the production data from the patients, which you will use to estimate model performance and detect data drift later on.\n\n\nBuilding the ML Model\nLet’s build a simple random forest classifier to classify respondents as diabetic.\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load your data\ndiabetes = pd.read_csv(\"binary_diabetes.csv\")\n\n# Split the data into features (X) and target (y)\nX = diabetes.drop('Diabetes_binary', axis=1)\ny = diabetes['Diabetes_binary']\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Create and train the model\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Get the predicted probabilities\ny_pred_proba = model.predict_proba(X_test)[:, 1]  \n\n# Print the classification report\nprint(classification_report(y_test, y_pred))\n\n# Create a dataframe with the test data, predicted classes, and predicted probabilities\ntest_results = X_test.copy()\ntest_results['y_true'] = y_test\ntest_results['y_pred'] = y_pred\ntest_results['y_pred_prob'] = y_pred_proba\nHere is the output of the classification model.\n                                precision    recall  f1-score   support\n\n         0.0       0.88      0.97      0.92     65605\n         1.0       0.48      0.18      0.26     10499\n\n    accuracy                           0.86     76104\n   macro avg       0.68      0.57      0.59     76104\nweighted avg       0.83      0.86      0.83     76104\nFrom the report above, the classifier can classify those with diabetes with a precision of 0.88 accurately. This means the chances of the classifier missing a diabetic patient are low. An overall accuracy of 0.86 indicates the model is performing well.\n\n\nEstimating Model Performance\nUsing CBPE, you can estimate the model performance when in production, and the analysis data does not necessarily have to contain the target feature.\nimport nannyml as nml\n\nreference_df = test_results\nanalysis_df = pd.read_csv(\"/content/analysis_df.csv\")\n\nestimator = nml.CBPE(\n    y_pred_proba =\"y_pred_prob\",\n    y_pred = \"y_pred\",\n    y_true = \"y_true\",\n    problem_type = \"classification_binary\",\n    metrics = [\"roc_auc\"],\n    chunk_size = 3000\n)\n\nestimator.fit(reference_df)\nresults = estimator.estimate(analysis_df)\n\nmetric_fig = results.plot()\nmetric_fig.show()\n\nnml.CBPE calculates the CBPE and takes the argument, y_pred_proba the predicted probability, y_pred the predicted classification, y_true the actual classification.\nThe problem_type argument takes the type of classification one is interested in, binary classification in this case.\nResults in NannyML are presented per chunk, by aggregating the data to a single data point on the monitoring results; here I used a chunk of 3000.\n\nYou can use any other metric, but the roc-auc metric is preferred in this situation because this is a health problem and it tells us more about the ability of the classifier to detect true positives, to minimize the chance of missing any diabetic patient.\n\nThe plot above shows that the model ROC-AUC metric started to fail at some point, which led to inaccurate predictions. The blue marker indicates the model metric, if this line exceeds the upper or lower threshold in red, it raises an alert signifying a drift.\nThe next step is to see what causes the poor model performance in these chunks by applying various drift detection methods.\n\n\nDetecting Data Drift\nLet’s use a multivariate drift detection method to get a summary number that detects any changes in our data using the domain classifier approach. This provides a measure of discriminating the reference data from the examined chunk data. If there is no data drift, the datasets are not different giving a value of 0.5. The higher the drift, the higher the returned measure, with a value as high as 1. This method provides a general overview of data drift in the analysis data.\nnon_feature_columns = ['y_pred_proba', 'y_pred', 'y_true']\nfeature_column_names = [\n    col for col in reference_df.columns\n    if col not in non_feature_columns\n]\ncat_features = [feature for feature in feature_column_names if feature not in ['BMI', 'MentHlth', 'PhysHlth', 'Age']]\n\ncalc = nml.DomainClassifierCalculator(\n    feature_column_names=feature_column_names,\n    chunk_size=3000\n)\n\ncalc.fit(reference_df)\nresults = calc.calculate(analysis_df)\n\nfigure = results.plot()\nfigure.show()\n\nThe plot above shows the domain classifier values in the reference data are either a little below or above 0.5, but that of the analysis data reaches a value of 1, indicating the presence of data drift in the analysis data. Now that we know we have data drift in the analysis data, let’s see which features are causing this drift.\n\n\nRanking\nWe need to know the features contributing to drift in the analysis data. The ranking method uses the results of univariate drift detection to rank features based on alert counts or correlation ranking.\nLet’s use the alert count’s approach to know which features are causing the drift in data.\nuniv_calc = nml.UnivariateDriftCalculator(\n    column_names=feature_column_names,\n    treat_as_categorical=['y_pred', *cat_features],\n    continuous_methods=['kolmogorov_smirnov', 'jensen_shannon'],\n    categorical_methods=['chi2', 'jensen_shannon'],\n    chunk_size=3000\n)\n\nuniv_calc.fit(reference_df)\nunivariate_results = univ_calc.calculate(analysis_df)\n\nalert_count_ranker = nml.AertCountRanker()\nalert_count_ranked_features = alert_count_ranker.rank(\n    univariate_results.filter(methods=['jensen_shannon']),\n    only_drifting = False)\ndisplay(alert_count_ranked_features)\nkolmogorov_smirnov, jensen_shannonand chi2 are various univariate drift detection methods you can always choose from.\n\nThe table above shows the top 10 features likely to cause drifts based on the alert counts. Next, let’s investigate further the contribution to data drift for each variable using the univariate drift detection method.\n\n\nUnivariate Drift Detection Method\nUnivariate drift detection allows you to see the amount of drift in the suspected features, which was used earlier to rank the features.\nuniv_calc = nml.UnivariateDriftCalculator(\n    column_names=feature_column_names,\n    treat_as_categorical=['y_pred', *cat_features],\n    continuous_methods=['jensen_shannon'],\n    categorical_methods=['jensen_shannon'],\n    chunk_size=3000\n)\n\nuniv_calc.fit(reference_df)\nunivariate_results = univ_calc.calculate(analysis_df)\n\nfigure = univariate_results.filter(column_names=univariate_results.continuous_column_names, methods=['jensen_shannon']).plot(kind='drift')\nfigure.show()\n\nThe plots above show the amount of drift in some of the features using Jensen-Shannon distance, which you can apply to both continuous and categorical features. You can also see that the red dotted points exceed the dotted line in each plot, this signifies drift in the variable.\nYou can also go further into each feature to see the distribution, this lets you know how large this drift is Let’s take a critical look at PhyHlth, this is the first feature on the ranked list.\nfigure = univariate_results.filter(column_names=[\"PhysHlth\"], methods=['jensen_shannon']).plot(kind='distribution')\nfigure.show() \n\nFrom the plot, you can see in highlight chunks with data drift, the plot is wider and bigger compared to that of the reference data, the plot also tells the presence of negative values in the analysis data. You can follow this procedure for each feature in the model indicating data drift using the univariate drift detection method. From the above results, you can understand the change in model prediction and the cause of those changes."
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#conclusion",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#conclusion",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, you learned about NannyML an open-source tool for monitoring model performance and detecting data drifts. You also learned how to use NannyML on a diabetes classifier and how to apply both univariate and multivariate drift detection methods in detecting data drift using NannyML.\nWhat’s next after detecting data drift? Check out this article, to know what to do when you detect drifts in your data."
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#recommended-reads",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#recommended-reads",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Recommended Reads",
    "text": "Recommended Reads\n\nMonitoring a Hotel Booking Cancellation Model Part 1: Creating Reference and Analysis Set\nTutorial: Monitoring an ML Model with NannyML and Google Colab\nHow to Estimate Performance and Detect Drifting Images for a Computer Vision Model?"
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#linkedin-summary",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#linkedin-summary",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "LinkedIn Summary",
    "text": "LinkedIn Summary\nMonitoring machine learning models for performance and data drift is crucial to ensure accurate predictions. In this article you will learn how to use the open-source NannyML library to estimate model performance and detect data drift for a diabetes classification model."
  }
]