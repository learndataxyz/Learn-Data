[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Monitoring Model Performance and Data Drift for Diabetes Classification\n\n\n\nmlops\n\n\ndata drift\n\n\nclassification\n\n\ntutorial\n\n\npython\n\n\ndata science\n\n\nmachine learning\n\n\nhealthcare\n\n\ndeployment\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecuring ML APIs with FastAPI\n\n\n\ndata engineering\n\n\nlow code\n\n\nguide\n\n\nno code\n\n\nELT\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Engineer’s Guide to Low Code/No Code ELT Tools\n\n\n\ndata engineering\n\n\nlow code\n\n\nguide\n\n\nno code\n\n\nELT\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "navs/about.html",
    "href": "navs/about.html",
    "title": "About",
    "section": "",
    "text": "Feeling lost in the world of data? Let LearnData be your guide. Our team lives and breathes all things data - from the fundamentals to cutting-edge techniques. We’ll break it down for you step-by-step with real examples and hands-on practice. By the time you’re done, you’ll be a data dynamo ready to take on any challenge."
  },
  {
    "objectID": "navs/courses.html",
    "href": "navs/courses.html",
    "title": "Courses",
    "section": "",
    "text": "ML Model Deployment with FastAPI and Streamlit\nBuilding Interactive Shiny Web Apps with R Programming\nData Wrangling and Exploratory Data Analysis with R\nPower BI DAX Practice Test and Solutions"
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#introduction",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#introduction",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Introduction",
    "text": "Introduction\nAccording to WHO, the number of people with diabetes rose from 108 million in 1980 to 422 million in 2014. Diabetes is a serious disease that leads to blindness, kidney failure, heart attacks, strokes, and lower limb amputations. It is mostly prevalent in low- and middle-income countries.\nBuilding a healthcare system that uses machine learning to predict patients with diabetes will help in early detection making it easy for healthcare providers to screen patients with diabetes at an early stage, before diagnosis.\n\nMachine learning models tend to degrade with time, highlighting the need for effective and constant monitoring of the model to know when its performance is declining. Often, this arises as a result of the change in the distribution of the data compared to the data the model was trained upon, this phenomenon is known as Data Drift.\nIn this article, you will learn how to monitor a diabetes classifier and detect data drifts in the data received from patients in a health information management system or mobile application using NannyML.\nNannyML is an open-source library for monitoring machine learning model performance in production, even without the predicted values being ready. It allows you to track your machine-learning model over time and see checkpoints where the model degrades."
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#estimating-model-performance-with-nannyml",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#estimating-model-performance-with-nannyml",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Estimating Model Performance with NannyML",
    "text": "Estimating Model Performance with NannyML\nNannyMl offers binary class classification that one could use to estimate the model’s performance, even without targets. Model estimation performance with NannyML involves:\n\nGetting the reference and analysis sets ready: The reference set is the data where the model behaves as expected, usually the test data. The analysis set is the latest production data, either with target features or not.\nTraining a performance estimator on the reference set: NannyML uses the reference set to train a performance estimator, it’s advisable to use the test data as reference data to prevent overfitting.\nUsing the estimator to predict performance on the analysis set (simulating real-world data): NannyML estimates the model performance on the analysis data using the trained performance estimator. One can use various classification metrics, such as accuracy or F1-score. Since misclassifying a patient (false negative) is more severe than misclassifying a healthy patient as diabetic (false positive), the AUC-ROC is the most appropriate metric to use in this case."
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#detecting-data-drift-with-nannyml",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#detecting-data-drift-with-nannyml",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Detecting Data Drift with NannyML",
    "text": "Detecting Data Drift with NannyML\nLet’s say you deployed a machine-learning model. As time goes on, the model tends to degrade, This is due to the nature of the data changing. If you have an application that you initially designed for kids and you train most of your machine learning models using your current user’s data, then all of a sudden middle-aged people and the elderly start using your application, and they become more of your users than the kids you designed it for. This will change the age distribution of your data, If age is one important feature in your machine learning model, your model will get worse with time. This is where you need to monitor when such changes happen in your data so that you can update the ML model.\n\nNannyML uses various algorithms to detect data drift, either using Univariate drift detection or Multivariate drift detection methods.\n\nUnivariate drift detection: In this approach, NannyML looks at each feature used in classifying if a patient is diabetic, and compares the chunks with those created from the analysis period. The result of the comparison is called a drift metric, and it is the amount of drift between the reference and analysis chunks, which is calculated for each chunk.\nMultivariate Drift Detection Instead of taking every feature one by one, NannyML provides a single summary metric explaining the drift between the reference and the analysis sets. Although this approach can detect slight changes in the data, it is difficult to explain compared to univariate drift.\n\nIn the case of classifying diabetic patients, undetected drift is dangerous and can lead to wrong model classifications. This is worse if the number of false negatives is high, the classifier might not detect some patients with diabetes, this can lead to late diagnosis."
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#estimating-model-performance-in-the-diabetes-classifier",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#estimating-model-performance-in-the-diabetes-classifier",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Estimating Model Performance in the Diabetes Classifier",
    "text": "Estimating Model Performance in the Diabetes Classifier\nNannyML uses two main approaches to estimate model performance, Confidence-based Performance estimation (CBPE) and Direct Loss estimation (DLE). In this case, we are interested in using the CBPE, since we are dealing with a classification task.\nThe CBPE uses the confidence score of the predictions to estimate the model performance, the confidence score is a value that the diabetes classifier gives for each predicted observation, expressing its confidence in predicting if a patient is diabetic., with values ranging from 0 to 1 and the closer it is to 1, the more confident the classifier is with it’s prediction.\nThe diabetes data contains 253,680 responses and 21 features. In this section, you will learn how to use this data to build an ML model, estimate your model’s performance, and detect data drift on updated data.\n\nProject Requirements\nTo get started, ensure you have installed NannyML on your JupyterNotebook. Download the analysis and diabetes data. The diabetes data is the data you will train the machine learning model on, and the analysis data is what you would take as the production data from the patients, which you will use to estimate model performance and detect data drift later on.\n\n\nBuilding the ML Model\nLet’s build a simple random forest classifier to classify respondents as diabetic.\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load your data\ndiabetes = pd.read_csv(\"binary_diabetes.csv\")\n\n# Split the data into features (X) and target (y)\nX = diabetes.drop('Diabetes_binary', axis=1)\ny = diabetes['Diabetes_binary']\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Create and train the model\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Get the predicted probabilities\ny_pred_proba = model.predict_proba(X_test)[:, 1]  \n\n# Print the classification report\nprint(classification_report(y_test, y_pred))\n\n# Create a dataframe with the test data, predicted classes, and predicted probabilities\ntest_results = X_test.copy()\ntest_results['y_true'] = y_test\ntest_results['y_pred'] = y_pred\ntest_results['y_pred_prob'] = y_pred_proba\nHere is the output of the classification model.\n                                precision    recall  f1-score   support\n\n         0.0       0.88      0.97      0.92     65605\n         1.0       0.48      0.18      0.26     10499\n\n    accuracy                           0.86     76104\n   macro avg       0.68      0.57      0.59     76104\nweighted avg       0.83      0.86      0.83     76104\nFrom the report above, the classifier can classify those with diabetes with a precision of 0.88 accurately. This means the chances of the classifier missing a diabetic patient are low. An overall accuracy of 0.86 indicates the model is performing well.\n\n\nEstimating Model Performance\nUsing CBPE, you can estimate the model performance when in production, and the analysis data does not necessarily have to contain the target feature.\nimport nannyml as nml\n\nreference_df = test_results\nanalysis_df = pd.read_csv(\"/content/analysis_df.csv\")\n\nestimator = nml.CBPE(\n    y_pred_proba =\"y_pred_prob\",\n    y_pred = \"y_pred\",\n    y_true = \"y_true\",\n    problem_type = \"classification_binary\",\n    metrics = [\"roc_auc\"],\n    chunk_size = 3000\n)\n\nestimator.fit(reference_df)\nresults = estimator.estimate(analysis_df)\n\nmetric_fig = results.plot()\nmetric_fig.show()\n\nnml.CBPE calculates the CBPE and takes the argument, y_pred_proba the predicted probability, y_pred the predicted classification, y_true the actual classification.\nThe problem_type argument takes the type of classification one is interested in, binary classification in this case.\nResults in NannyML are presented per chunk, by aggregating the data to a single data point on the monitoring results; here I used a chunk of 3000.\n\nYou can use any other metric, but the roc-auc metric is preferred in this situation because this is a health problem and it tells us more about the ability of the classifier to detect true positives, to minimize the chance of missing any diabetic patient.\n\nThe plot above shows that the model ROC-AUC metric started to fail at some point, which led to inaccurate predictions. The blue marker indicates the model metric, if this line exceeds the upper or lower threshold in red, it raises an alert signifying a drift.\nThe next step is to see what causes the poor model performance in these chunks by applying various drift detection methods.\n\n\nDetecting Data Drift\nLet’s use a multivariate drift detection method to get a summary number that detects any changes in our data using the domain classifier approach. This provides a measure of discriminating the reference data from the examined chunk data. If there is no data drift, the datasets are not different giving a value of 0.5. The higher the drift, the higher the returned measure, with a value as high as 1. This method provides a general overview of data drift in the analysis data.\nnon_feature_columns = ['y_pred_proba', 'y_pred', 'y_true']\nfeature_column_names = [\n    col for col in reference_df.columns\n    if col not in non_feature_columns\n]\ncat_features = [feature for feature in feature_column_names if feature not in ['BMI', 'MentHlth', 'PhysHlth', 'Age']]\n\ncalc = nml.DomainClassifierCalculator(\n    feature_column_names=feature_column_names,\n    chunk_size=3000\n)\n\ncalc.fit(reference_df)\nresults = calc.calculate(analysis_df)\n\nfigure = results.plot()\nfigure.show()\n\nThe plot above shows the domain classifier values in the reference data are either a little below or above 0.5, but that of the analysis data reaches a value of 1, indicating the presence of data drift in the analysis data. Now that we know we have data drift in the analysis data, let’s see which features are causing this drift.\n\n\nRanking\nWe need to know the features contributing to drift in the analysis data. The ranking method uses the results of univariate drift detection to rank features based on alert counts or correlation ranking.\nLet’s use the alert count’s approach to know which features are causing the drift in data.\nuniv_calc = nml.UnivariateDriftCalculator(\n    column_names=feature_column_names,\n    treat_as_categorical=['y_pred', *cat_features],\n    continuous_methods=['kolmogorov_smirnov', 'jensen_shannon'],\n    categorical_methods=['chi2', 'jensen_shannon'],\n    chunk_size=3000\n)\n\nuniv_calc.fit(reference_df)\nunivariate_results = univ_calc.calculate(analysis_df)\n\nalert_count_ranker = nml.AertCountRanker()\nalert_count_ranked_features = alert_count_ranker.rank(\n    univariate_results.filter(methods=['jensen_shannon']),\n    only_drifting = False)\ndisplay(alert_count_ranked_features)\nkolmogorov_smirnov, jensen_shannonand chi2 are various univariate drift detection methods you can always choose from.\n\nThe table above shows the top 10 features likely to cause drifts based on the alert counts. Next, let’s investigate further the contribution to data drift for each variable using the univariate drift detection method.\n\n\nUnivariate Drift Detection Method\nUnivariate drift detection allows you to see the amount of drift in the suspected features, which was used earlier to rank the features.\nuniv_calc = nml.UnivariateDriftCalculator(\n    column_names=feature_column_names,\n    treat_as_categorical=['y_pred', *cat_features],\n    continuous_methods=['jensen_shannon'],\n    categorical_methods=['jensen_shannon'],\n    chunk_size=3000\n)\n\nuniv_calc.fit(reference_df)\nunivariate_results = univ_calc.calculate(analysis_df)\n\nfigure = univariate_results.filter(column_names=univariate_results.continuous_column_names, methods=['jensen_shannon']).plot(kind='drift')\nfigure.show()\n\nThe plots above show the amount of drift in some of the features using Jensen-Shannon distance, which you can apply to both continuous and categorical features. You can also see that the red dotted points exceed the dotted line in each plot, this signifies drift in the variable.\nYou can also go further into each feature to see the distribution, this lets you know how large this drift is Let’s take a critical look at PhyHlth, this is the first feature on the ranked list.\nfigure = univariate_results.filter(column_names=[\"PhysHlth\"], methods=['jensen_shannon']).plot(kind='distribution')\nfigure.show() \n\nFrom the plot, you can see in highlight chunks with data drift, the plot is wider and bigger compared to that of the reference data, the plot also tells the presence of negative values in the analysis data. You can follow this procedure for each feature in the model indicating data drift using the univariate drift detection method. From the above results, you can understand the change in model prediction and the cause of those changes."
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#conclusion",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#conclusion",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, you learned about NannyML an open-source tool for monitoring model performance and detecting data drifts. You also learned how to use NannyML on a diabetes classifier and how to apply both univariate and multivariate drift detection methods in detecting data drift using NannyML.\nWhat’s next after detecting data drift? Check out this article, to know what to do when you detect drifts in your data."
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#recommended-reads",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#recommended-reads",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Recommended Reads",
    "text": "Recommended Reads\n\nMonitoring a Hotel Booking Cancellation Model Part 1: Creating Reference and Analysis Set\nTutorial: Monitoring an ML Model with NannyML and Google Colab\nHow to Estimate Performance and Detect Drifting Images for a Computer Vision Model?"
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#introduction-to-low-codeno-code-elt-tools",
    "href": "posts/The Engineer's Guide to Low Code/index.html#introduction-to-low-codeno-code-elt-tools",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "Introduction to Low-code/No-code ELT Tools",
    "text": "Introduction to Low-code/No-code ELT Tools\nLow-code/No-code tools are tools used in building applications using drag-and-drop components, reducing or eliminating the amount of code used in development. It provides an interactive graphical user interface, making it easy for non-technical users to start developing.\nLow-code tools help developers quickly get started, writing little or no code. It helps professional developers quickly deliver applications, letting them focus on the business side of the applications. Some features of low-code tools are:\n\nIt offers an interactive user interface with high-level functions, eliminating the need to write complex code.\nIt is easy to modify or adapt.\nMostly developed for a specific use case or audience\nIt is easy to scale.\n\nNo-code, on the other hand, is a method of developing applications that allows non-technical business users; business analysts, admin officers, small business owners, and others, to build applications without the need to write a single line of code. Some features of no-code tools are:\n\nPurely visual development, that is, users develop using drag-and-drop interfaces.\nIt offers limited customization, users use what is provided in the tool and can’t extend its capabilities.\nSuited for non-technical individuals\nMostly suited for simple use cases\n\nLow-code tools require some basic coding skills, unlike no-code which does not require any programming knowledge. Low-code/no-code is based on visual programming and automatic code generation. The emergence of low-code/no-code was the fact that domain experts know how difficult it is to impart to the IT team, with the help of low-code/no-code, they can take part in the development process, coupled with the fact that the shortage of skilled developers and loads of workload on the IT professionals is another reason for the emergence of low-code/no-code."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#understanding-the-elt-process",
    "href": "posts/The Engineer's Guide to Low Code/index.html#understanding-the-elt-process",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "Understanding the ELT Process",
    "text": "Understanding the ELT Process\nELT stands for Extract, Load and Transform, which are the three stages involved in the process.\nIt’s a data preprocessing technique that involves moving raw data from a source to a data storage area, either a data lake or a data warehouse. Sources are either social media platforms, streaming platforms or any other place data is stored. During extraction, data is copied in raw form from the source location to a staging area, this is either in a structured or unstructured format from sources such as:\n\nSQL or NoSQL servers\nText and document files\nEmail\nWeb pages\n\nExtraction is either full which involves pulling all rows and columns from a particular data source, using an automated partial extraction with update notifications when data is added to the system, or incremental where update records are extracted as data is added into the data source.\nLoading involves moving the data from the staging area to the data storage area, either a data lake or a data warehouse. This process is automated, continuous and done in batch. One can load all the available data in the data source to the data storage area, load modified data from the source between certain intervals or load data into the storage area in real time.\nIn the transform stage, a pre-written schema is run on the data using SQL for analysis. This stage involves filtering, removing duplications, currency conversions, removal of encryptions, joining data into tables or performing calculations.\nUnlike in ETL where raw data is transformed before being loaded into a destination source, in ELT the data is loaded into a destination source before it’s transformed for analysis as needed.\n\nBy allowing transformation to occur after loading, data is moved quickly to the destination for availability. Because data is transformed after arrival at the destination, ELT allows the data recipient to control data manipulation. This ensures that coding errors when transforming do not affect another stage. ELT uses the powerful big data warehouse and lakes allowing transformation and scalable computation.\nData warehouses use MPP architecture (Massively Parallel Processing), and data lake processes also support the application of schema or transformation models as soon as the data is received, this makes the process flexible, especially for large amounts of data. ELT is suited for data that are in cloud environments, this provides a seamless integration since ELT is cloud-native and allows for the continuous flow of data from sources to storage destinations, hence making them on demand.\nELT is used for instant access to huge volumes of data, for example in IOT, it loads data from IOT devices making it readily available for data scientists or analysts to access raw data and work collaboratively.\nDespite its advantages, ELT has some of its limitations:\n\nData privacy is a challenge in ELT, this is because when transferring data, a breach can occur from the source to the destination storage which poses security and privacy risks.\nWhile in transit, if care is not taken sensitive information is exposed, and extra security measures have to be taken to ensure the confidentiality of the data.\nELT handle large volumes of data, making it computationally intensive, leading to delays in gaining insights.\nBecause the data is loaded into the data storage area without transformation, it makes it challenging to transform the data when compared with ETL. This requires strong technical and domain knowledge of the data scientist and analyst who will write the queries transforming the data."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#low-codeno-code-elt-tools",
    "href": "posts/The Engineer's Guide to Low Code/index.html#low-codeno-code-elt-tools",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "Low-Code/No-Code ELT Tools",
    "text": "Low-Code/No-Code ELT Tools\nLow-code /No-code ELT are used to extract, load and transform data. Here are some of the benefits of using low-code/no-code ELT tools:\n\nIt is easier compared to writing scripts to automate the ELT process.\nIt makes development faster, developers can spend more time solving business problems instead of fixing bugs that result from writing lines of code.\nIt increases automation, a lot of processes that would have to be set up manually are handled automatically, such as monitoring, logging, setting notifications when there is a problem with the pipeline and so on.\nMost ELT tools support a lot of data connectors, making it easy for an organization to connect to any data source with provisions to create custom connectors.\nELT tools lower the cost of building an ELT pipeline by ensuring the whole process is conducted with a single tool from start to finish.\nThey provide better customer experience, by ensuring that even business folks are involved in building the ELT pipeline.\n\nThere are various low-code/no-code  ELT tools out there, each with its strengths and limitations, here are some you can consider for building an ELT pipeline:\n\nAirbyte\nAirbyte is an open-source data movement platform with over 300+ open-source structured and unstructured data sources. Airbyte is made up of two components; platform and connectors. The platform provides all the services required to configure and run data movement operations while the connectors are the modules that pull data from sources or push data to destinations. Airbyte also has a connector builder, a drag-and-drop interface and a low-code YAML format for building data source connectors.\nAirbyte has two plans, Cloud and Enterprise, but it is free to use if you can self-host the open-source version. Airbyte offers real-time and batch data synchronization, tracks the status of data sync jobs, monitors the data pipeline and views logs with a notifications system in case things go wrong. Airbyte also allows you to add custom transformations using dbt.\n\n\nFivetran\nFivetran is an automated data movement platform used for ELT. It offers automated data movement, transformations, security and governance with over 400+ no-code source connectors. You can use the function connector in Fivetran to write the cloud function to extract the data from your source, while it takes care of loading and processing the data into your destination. Fivetran gives you options to manage Fivetran connectors, to have more control over your data integration process runs. It offers a “pay-as-you-use” model, with five Free pricing plans, Starter, Standard, Enterprise and Business Critical.\n\n\nIntegrate.io\nFormerly known as Xplenty, this is another low-code platform for data movement, unlike the others it doesn’t have many connections, it offers both low-code ETL, Reverse ETL and an ELT platform. Pricing on Integrate.io is based on data volume and increases as the number of rows in your data increases. It offers both Starter, Professional and Enterprise plans, with an extra charge for additional connectors.\n\n\nStitch\nStitch is also another data movement platform owned by Qlik. It replicates historical data from your database for free and allows you to add multiple user accounts across your organization to manage and authenticate data sources. It is extensible and has several hundreds of connectors. It offers various pricing models such as standard, advanced and premium which are all charged based on data volume.\n\n\nMatillion\nMatillion is another ELT platform, that uses LLM components to unlock unstructured data and offers custom connectors to build your connectors. It has a complete pushdown architecture supporting SQL, Python, LLMs, Snowflake, Databricks, AWS, Azure and Google. It supports both low-code and no-code for both programmers and business users. You can create an ELT process using either SQL, Python or dbt. It also gives you Auto-Documentation to generate pipeline documentation automatically. It offers three pricing models Basic, Advanced and Enterprise which you can pay only for pipelines run and not, those on development or sampling."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#key-features-of-low-codeno-code-elt-tools",
    "href": "posts/The Engineer's Guide to Low Code/index.html#key-features-of-low-codeno-code-elt-tools",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "Key Features of low-code/no-code ELT Tools",
    "text": "Key Features of low-code/no-code ELT Tools\n\nAvailability of Connectors\nELT connectors are components of an ELT tool that allow the tool to connect to a data source and make it possible for extraction and loading. When trying to go for ELT tools it is important to go for the ELT tool with the highest number of connectors, that is why an organization needs to have a list of all the data sources it uses, this will let the organization know the ELT tool to choose based on organizational data sources, most importantly connectors for revenue-generating applications. Let’s say your organization uses Zoho as a CRM. It’s important to compare various ELT tools with a connector for Zoho and see which offers the best service at the most affordable price.\n\n\nDrag-and-Drop Interfaces\nLow-code/no-code ELT tools offer an intuitive user interface with a drag-and-drop functionality, allowing non-technical users to perform ELT by dragging and dropping components without encountering any challenges. This makes the user experience seamless and users can focus on the application’s business logic. This reduces the workload on the IT team, allowing the organization’s domain experts to partake in the development process.\n\n\nAutomated Scheduling\nDue to their ability to schedule extracting and loading, automating the ELT process is very simple. This can involve creating tasks that can be run using a specified SQL schema at specific intervals. One can easily automate documentation, document process automation, and show the manipulations occurring to data from source to data storage, enabling organizations to save time and costs.\n\n\nData Transformation Capabilities\nLow-code/no-code ELT tools manage dependencies when transforming data, this is essential when you have multiple transforms depending on each other. They support the DAG(directed acyclic graph) workflow to manage the transformation dependencies after conducting a transform job using SQL on the data, reading the transformation query and calculating a dependency workflow.\n Another important aspect is their support for incremental loading, where only the differences in data are loaded from source to destination. For example Let’s consider the case of a retail store that tracks its sales data, without an incremental approach the daily sales report would involve extracting sales data from the sales table, which contains millions of records, aggregating the data to calculate the total sales, revenue and units sold for each product, store and day, and load the aggregated data into a new table called sales_daily.\nThis is a resource-intensive approach as the system needs to process all the sales data every time the report is generated. Using an incremental approach, whenever a new sale is recorded in the sales table, a trigger or a background process is used to update the sales_daily table with new data for that day and store.  Whenever every report is generated, only the data for the latest day is extracted from the sales_daily table, which is a much smaller dataset than the entire sales table. The incremental approach helps in improving performance, cost and scalability.\n\n\nMonitoring and Alerting\nMonitoring and Alerting is another important feature of a low-code/no-code ELT tool because It allows you to detect anomalies, bottlenecks and failures in the workflow, provide continuous surveillance and monitor resource utilization. Key important metrics it monitors are; latency, throughput, error rates, resource utilization and pipeline lag. They also give threshold alerts, detect anomalies, and escalate alerts to SMS or phone calls.\nImagine a manufacturing company that collects sensor data from various manufacturing plants across facilities. If analysis, reveals one machine is giving higher vibration levels than others. The ELT tool anomaly detection algorithm should trigger an alert which will prompt the maintenance team to investigate. They might identify a worn-out bearing component and schedule proactive maintenance, preventing equipment failure and uninterrupted production."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#evaluating-low-codeno-code-elt-tools",
    "href": "posts/The Engineer's Guide to Low Code/index.html#evaluating-low-codeno-code-elt-tools",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "Evaluating low-code/no-code ELT Tools",
    "text": "Evaluating low-code/no-code ELT Tools\nThere are various things to consider when choosing a low-code/no-code  ELT tool.\n\nConnectors: When selecting an ELT tool, ensure it supports connections to various data sources and know how many SAAS integrations are available and how effectively the tool can connect to organizational data sources.\nRoadmap: Another important factor, is if the ELT tool can handle the company’s rapidly growing data. Is it responsive and scalable? This will give the organization an idea of the ELT tool’s sustainability in the long run.\nPricing: How does the ELT tool charge? Is it by data flows or data volume and are the features it offers worth its pricing? Some ELT tools offer more connectors at affordable pricing than others.\nSupport: Look for an ELT tool with available customer support, this is very crucial especially when things break. The ELT tools should also offer good documentation that is easy to understand by technical and non-technical users. An online community around the tool is also a plus, users can relate with fellow users and serve as support for each other.\nSecurity: How does the ELT tool prioritise security? Are organizational data safe and is it regulatory compliant with GDPR, SOC2, HIPPA, and other relevant regulations? These are important security questions to look for when selecting an ELT tool. It is also important that the organization knows, how the tool handles privacy and authentication.\nEase of use: A low-code/no-code ELT tool that is user-friendly and easy to customize is another priority to look out for, it makes the process of creating ELT pipelines easy and non-technical for business folks.\nMaintenance: When choosing a low-code/no-code ELT tool, it’s important to know how easy it is to fix problematic data sources, and if it gives informative logs if an execution fails. It is also important to know what skills are required, by team members to keep the ELT process running smoothly."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#implementing-low-codeno-code-elt-tools",
    "href": "posts/The Engineer's Guide to Low Code/index.html#implementing-low-codeno-code-elt-tools",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "Implementing Low-code/No-code ELT Tools",
    "text": "Implementing Low-code/No-code ELT Tools\n\nPlanning the ELT Pipeline\nBefore building an ELT pipeline, you need to get the data from your source using an ELT tool like Airbyte and decide the data warehouse to use, either Google BigQuery, AWS Redshift or Snowflake. Next is to transform the data using dbt, R, Python or a no-code transformation layer such as Boltic, then consider the BI tool for presenting the data to end users.\n\n\nConfiguring Data Source and Destination Connections\nLet’s say for example using a REST API as a data source, Airbyte as the ELT tool and Amazon S3 bucks as the data destination. Create a new S3 bucket in the AWS console, in the bucket, create a folder to store data from the ELT operation in Airbyte. Create another folder in the previous folder to store the data you will extract from the REST API.\nNext, you will configure both the source and the destination connector, and connect the source and the destination. Ensure any API you use, there is a connector for it on Airbyte. If you don’t see a connector for your data source, use Airbyte’s CDK(Connector Development Kit) to create a custom connector.\nNext, you go to AWS S3 to configure the destination connector to connect Airbyte with the destination data lake, after successfully configuring both the source and the destination connector, and passing all the tests. You can now configure the ELT connection between source and destination.\n\n\n\nDesigning Data Transformation Workflows\nBefore transforming your data, you need to explore and understand it, this involves looking at your entity relationship diagrams to see how the data relate with each other, identifying missing values or misleading column names and performing a summary analysis of the data.\nWhile exploring, you might understand what is wrong with your data, and decide to perform your transformation process such as correcting misleading columns, renaming fields appropriately, adding business logic or joining data. The transformation you apply depends on what you have explored in the data. You can use tools like dbt, SQL, Python or R to transform your data or go no-code with tools like Boltic. At this stage,  test your data to meet business standards.\nFinally, you document your transformation process explaining the data model, key fields and metrics making the documentation easy for non-technical users to understand.\n\n\nScheduling and Automating ELT Processes\nBefore releasing the data to end users, you need to push it into a production environment in the data warehouse, these product-ready tables are what the BI analysis will query. With time you will need to update and refresh the data to meet the business needs, using a scheduler or orchestrator.  Using the job scheduler, you can use tools like dbt Cloud or Shipyard to create data transformations and tests within a collaborative IDE.\n\n\nMonitoring and Maintaining the ELT Pipeline\nMonitoring is important to identify bugs in data pipelines, optimise pipelines and gain valuable insights. These ELT tools provide visualisations, logging and observability systems to analyse latency, error rates and data throughput in your ELT pipeline. Most low-code/no-code provide all these out of the box, you will receive custom notifications when data problems occur allowing you to improve the quality of your data sets."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#best-practices-for-low-codeno-code-elt",
    "href": "posts/The Engineer's Guide to Low Code/index.html#best-practices-for-low-codeno-code-elt",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "Best Practices for low-code/no-code ELT",
    "text": "Best Practices for low-code/no-code ELT\n\nEnsuring Data Quality and Integrity\n\nData quality and requirements: The first step before conducting an ELT is to specify the data quality and requirements. These should specify the data accuracy, completeness, consistency, timeliness, validity, and uniqueness. This also includes details of the sources, end users and data quality metrics. This will help to understand the quality of data to expect and how to achieve it.\nValidation: The next step is to validate the data quality using the ELT tools before loading them into the warehouse, to reduce the amount of bad-quality data that gets into the data warehouse.\nData quality checks: In this step, ensure you use the ELT tools to implement quality checks on the data in the data warehouse before making it available to end users so that the data is consistent, complete and correct(3Cs).\nData quality monitoring and auditing: The next step is to monitor and audit the data quality as new data gets updated into the system, this is to resolve issues of data quality when they arise. ELT tools have various tools to get reports, alerts and logs on data quality. This ensures the successful maintenance of the data in the long run.\nData quality documentation and communication: Next is to give a report explaining to the end users or stakeholders the quality of the data, this report should contain the processes, rules, metrics and issues with the data. Doing this ensures trust and transparency of the data quality.\nReview and update the data quality: Constantly use the information from the ELT tool logging system to update the data quality, from time to time. By doing this, you are ensuring that the data remains relevant and meets the organization’s requirements.\n\n\n\nHandling Data Governance and Compliance\nIt is important to define clear roles and objectives for various stakeholders and use data governance tools to automate the tasks of data validation, cleansing, standardization, profiling, auditing, and reporting. Provide a data catalogue of the data assets and other data quality indicators and recommend a staging area for the source data before moving to the data warehouse. Finally, using a schema-on-read approach allows users to apply different views to the same data, based on their needs.\n\n\nIntegrating low-code/no-code ELT with Existing Systems\nDue to the visually driven approach of low-code/no-code ELT tools, you must thoroughly assess your existing data sources, formats and structures to identify potential compatibility issues and develop appropriate mapping strategies. Check, if a connector for your data source exists before building a custom connector.\n\n\nScaling and Optimizing ELT Workflows\nOne of the reasons for scaling and optimizing an ELT process is to reduce data loss and ELT run downtime. The following are ways to optimize an ELT workflow effectively:\n\nParallelize Data Flow: Simultaneous data flow saves more time than sequential data flow, you can load a group of unrelated tables from your data source to the data warehouse, by grouping the tables into a batch and running each batch simultaneously, instead of loading the tables one by one.\nApplying data quality checks: You should ensure each data table is tested with predefined checks such as schema-based to ensure that when a test fails, the data is rejected and when it passes, it produces an output.\nCreate Generic Pipelines: Generic pipelines ensure team members reuse a code without developing one from scratch. Practices like parameterizing the values can ease the job, when one wants to use a code/pipeline, they change the values of the parameters such as database connections.\nUse of streaming instead of batching: Streaming data from the source to the data destination is the best approach, this ensures the system is up to date whenever new data is added to allow end users access to recent data."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#future-of-low-codeno-code-elt",
    "href": "posts/The Engineer's Guide to Low Code/index.html#future-of-low-codeno-code-elt",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "Future of low-code/no-code ELT",
    "text": "Future of low-code/no-code ELT\nLow-code/no-code ELT tools have become popular, offering cloud-based options and reduced infrastructure costs. Most of them now have pre-built connectors covering most data sources and automated scheduling. Advanced low-code/no-code ELT tools now have data mapping and transformation capabilities using machine learning to detect data patterns and transformation. Some low-code/no-code tools now integrate data governance features such as profiling, data lineage tracking and automated data validation rules to ensure data integrity. These innovations aim to simplify data integration and empower non-technical users.\nEven with the innovation and simplicity low-code/no-code ELT tool offers, it has some of its limitations:\n\nLow-code/no-code ELT make it difficult to collaborate as a team, unlike a code-heavy pipeline with version control and collaborative features.\nSome low-code/no-code ELT tools give poor developer experience which can hamper productivity, such as bad and non-intuitive UI design and limited customization options.\nAnother challenge with low-code/no-code ELT is the security of the applications built on them, even though the platform is secure. This is an issue since the tool was developed for non-technical individuals with no expertise in security best practices.\nThey do not offer the source code to pipelines built and this is a challenge when an organization wants to migrate to another platform, they are forced to develop from scratch on another platform or stick with that same platform."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#conclusion",
    "href": "posts/The Engineer's Guide to Low Code/index.html#conclusion",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "Conclusion",
    "text": "Conclusion\nWhy learn data engineering since anyone can now use low-code/no-code tools? Data engineering is not just about building pipelines but also solving business problems such as designing schemas. It’s important that the data you give to your end users can answer their questions. In summary, developing your soft skills is more important to make you stand out.\nLow-code/no-code have a lot of abstraction which when things go wrong it’s difficult to find out, some organizations will not want to give out the control they have over their data pipeline by hiding away some of the pipeline core functionality using a low-code/no-code ELT tool."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#references",
    "href": "posts/The Engineer's Guide to Low Code/index.html#references",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "References",
    "text": "References\n\nUnleashing the power of ELT in AWS using Airbyte\nHow do you ensure data quality and governance in an ELT process?\n6 Best Practices to Scale and Optimize Data Pipelines"
  },
  {
    "objectID": "posts/Securing ML APIs with FastAPI/index.html",
    "href": "posts/Securing ML APIs with FastAPI/index.html",
    "title": "Securing ML APIs with FastAPI",
    "section": "",
    "text": "LearnData Branding(1).png\n\n\nThe end goal of a machine learning model is to serve end users. Still, due to machine learning models requiring regular updates to improve model accuracy and use in other applications, they are exposed as an API. An ML API is an application that serves as a gateway between your client requests or needs and your machine learning model.\nLet’s say you have a recommender model on an e-library platform, that recommends books for users based on user preferences. This recommender model works as an API by getting user preferences and recommending books to the user. The API also makes it easy for you to use the recommender model on another platform.\nDue to the sensitivity of training data in machine learning models, API security is important to avoid data breaches and prevent malicious clients from accessing the model. In this article, I will show you how to secure your machine-learning APIs using FastAPI - an open-source Python framework that allows you to build secured and scalable APIs. As a Python library, the learning curve is low for data scientists and machine learning engineers with Python backgrounds. If you are new to FastAPI check out this course on ML deployment with FastAPI.\n\n\n\nML Model API Workflow. Image by Author\n\n\nML Model API Workflow. Image by Author\n\n\nAPI is usually a target for data breaches and unauthorized access due to the information it contains, making it prone to security attacks, this is why API security is important. API security is a practice set to protect an API from unauthorized access. Here are some of the most common API security threats:\n\nInjection attacks (SQL, command): In this type of attack, someone injects malicious code into the API, using SQL or terminal commands to read or modify the database. These kinds of attacks are usually targeted at the application’s database.\nCross-site scripting (XSS): This is another type of attack where a hacker manipulates a vulnerable site by sending malicious JavaScript to users, which upon execution by a user, the attacker can masquerade as the user and manipulate the user’s data.\nCross-site request forgery (CSRF): In this attack, attackers make users perform actions they don’t intend to do.\nMan-in-the-middle (MITM) attacks: In this attack, hackers eavesdrop between the interaction of clients and the API, to steal relevant credentials such as login details and credit card information.\n\nIn this article, you will learn how to solve these issues and make your machine-learning API secure.\n\n\n\n\nHave basic knowledge of Python and FastAPI framework\nEnsure you have installed scikit-learn , fastapi , pydantic, uvicorn , numpy,and joblib libraries.\n\n\n\n\n\nCreate a project folder and a virtual environment.\nCopy and paste the following code into a new file called utilis.py in your project directory. This will create a classification model and a model.pkl file based on the iris dataset.\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nimport joblib\n\n# Load the iris dataset\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Train a random forest classifier\nmodel = RandomForestClassifier()\nmodel.fit(X, y)\n\n# Save the trained model\njoblib.dump(model, 'model.pkl')\nCreate an API endpoint for the machine-learning model in a file main.py .\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nimport joblib\n\n# Load the trained model\nmodel = joblib.load(\"model.pkl\")\n\n# Define the request body using Pydantic\nclass PredictionRequest(BaseModel):\n    sepal_length: float\n    sepal_width: float\n    petal_length: float\n    petal_width: float\n\napp = FastAPI()\n\n@app.post(\"/predict\")\ndef predict(request: PredictionRequest):\n    # Convert request data to a format suitable for the model\n    data = [\n        [\n            request.sepal_length,\n            request.sepal_width,\n            request.petal_length,\n            request.petal_width,\n        ]\n    ]\n    # Make a prediction\n    prediction = model.predict(data)\n    # Return the prediction as a response\n    return {\"prediction\": int(prediction[0])}\n\n# To run the app, use the command: uvicorn script_name:app --reload\n# where `script_name` is the name of your Python file (without the .py extension)\n\nWe now have our ML model API, let’s see how we can implement security best practices using this API.\n\n\n\nTake API authentication like a passkey that allows a client to access your API, allowing only authorized users to use the API. There are various ways of implementing API authentication in FastAPI, which you will learn subsequently.\n\n\n\nAuthentication vs Authorization. Source: Medium\n\n\nAuthentication vs Authorization. Source: Medium\nAPI authentication is insufficient to protect your API, you also need to implement API authorization. API authentication is like giving someone a key to your house, while API authorization is like giving them access to specific rooms in the house.\n\n\nThis is the most basic and popular form of implementing API security.\n\nTo implement key-based authentication in FastAPI, add the following code before the @app.post(\"/predict\") endpoint in main.py file.\n# Define the API key\nAPI_KEY = \"your_api_key_here\"\n\n# Dependency to verify the API key\ndef get_api_key(api_key: str = Header(...)):\n    if api_key != API_KEY:\n        raise HTTPException(status_code=403, detail=\"Could not validate credentials\")\n\nAPI_KEY is the variable that contains your environment key, which is supposed to be stored as an environment variable in a .env file.\nget_api_key() function gets the API_KEY and verifies if the provided API key matches what’s on the database. If successful, the user is granted access to the API, else an HTTP error 403 is raised, telling the user that the provided credential is invalid.\n\nNext, go to the predict() function and add api_key as an argument to get the api_key from users.\n@app.post(\"/predict\")\ndef predict(request: PredictionRequest, \n                        #added argument to get API key from user\n            api_key: str = Depends(get_api_key)):\n\nDepends function prevents access to the /predict endpoint without the API key.\n\n\n\n\nHow authentication works in FastAPI\n\n\nHow authentication works in FastAPI\n\n\n\n\nUnlike API keys, OAuth2 is an authorization protocol, granting clients access to resources hosted by other web applications on behalf of the user. With OAuth2, users do not need to give out their password to access a resource.\nA practical example is a client accessing your machine learning API using their Google ID without giving away their details, and your API in turn sends a token back to the client to serve as a temporary password for the client to access the API. It’s very secure compared to the API key. Unlike the API key which grants a user access to all resources in an API, OAuth2 only grants the client access to specified resources.\nWhen a user wants to access a machine learning API through a client application, the process typically uses OAuth2 for secure authentication. The client application starts by redirecting the user to an authentication server, where the user grants permission for the application to access their resources. The authentication server then issues an access token, often, in the form of a JWT (JSON Web Token) to the client application. The application uses this token to make requests to the machine learning API. The API verifies the token to ensure the client is authorized to access the requested resources, thus providing secure and controlled access while protecting user data and privacy.\n\n\n\nOAuth2 Workflow: Source: GeeksforGeeks\n\n\nOAuth2 Workflow: Source: GeeksforGeeks\nLet’s implement a simple OAuth2 with JWT on our machine learning API, by updating the main.py file as follows.\n\nEnsure you install pyjwt and import the following Python libraries.\nfrom fastapi import FastAPI, Depends, HTTPException\nfrom fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm\nfrom pydantic import BaseModel\nimport joblib\nfrom typing import Optional\nimport jwt\n\nOAuth2PasswordBearer and OAuth2PasswordRequestForm are used to implement OAuth2 in FastAPI.\njwt is used to create a JSON Web Token.\n\nDefine the user model, to allow the user to provide a username and password using the BaseModel class.\n# Define a user model\nclass User(BaseModel):\n    username: str\n    password: str\nCreate a function to authenticate users.\ndef authenticate_user(username: str, password: str) -> Optional[User]:\n    if username == \"admin\" and password == \"password\":\n        return User(username=username, password=password)\n    return None\n\nThe authenticate_user() function takes in a client username and password to see if it matches what’s in the database and returns a User model.\n\nCreate a SECRET_KEY variable to encode the JWT and create an oauth2_scheme\nSECRET_KEY = \"your-secret-key\"\n\n# OAuth2 scheme using password flow\noauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"token\")\nCreate a function to access the token using JWT.\ndef create_access_token(data: dict):\n    return jwt.encode(data, SECRET_KEY)\n\nThe create_access_token() function takes in the user details and encodes it with the SECRET_KEY\n\nCreate an authentication route to generate the access token.\n@app.post(\"/token\")\nasync def login_for_access_token(form_data: OAuth2PasswordRequestForm = Depends()):\n    user = authenticate_user(form_data.username, form_data.password)\n    if not user:\n        raise HTTPException(\n            status_code=401,\n            detail=\"Incorrect username or password\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n    access_token = create_access_token({\"sub\": user.username})\n    return {\"access_token\": access_token, \"token_type\": \"bearer\"}\n\nThe login_for_access_token()function takes the user inputs; username and password with the OAuth2 flow as an argument to return an access token to give the client application.\nIf user details are right, an access token is created and returned, else a 401 warning is returned\n\nProtect the API route that requires JWT authentication\n@app.post(\"/predict\")\nasync def predict(request: PredictionRequest, token: str = Depends(oauth2_scheme)):\n    try:\n        # Decode JWT token\n        payload = jwt.decode(token, SECRET_KEY, algorithms=[\"HS256\"])\n        # Convert request data to a format suitable for the model\n        data = [\n            [\n                request.sepal_length,\n                request.sepal_width,\n                request.petal_length,\n                request.petal_width,\n            ]\n        ]\n        # Make a prediction\n        prediction = model.predict(data)\n        # Return the prediction as a response\n        return {\"prediction\": int(prediction[0])}\n    except jwt.exceptions.DecodeError:\n        raise HTTPException(\n            status_code=401,\n            detail=\"Could not validate credentials\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n\nThe argument token: str = Depends(oauth2_scheme) means the API endpoint is protected using OAuth2, and receives the access token from the client application.\nThe token is decoded to see if it contains the SECRET_KEY, if it does, access is given to the model prediction, else a warning is given stating that the provided credentials are invalid.\n\n\n\n\n\n\nUser logs in and their data is encoded with a secret key to create an access token\nThe secured API endpoint decodes this access token to see if it contains the secret key before providing access to the resource.\n\n\n\nHow OAuth2 works in FastAPI.\n\n\nHow OAuth2 works in FastAPI.\n\n\n\n\nRBAC is an approach where users are given various roles that provide access to specific API resources. It is an efficient way of ensuring API security, instead of granting all users privileges, users are granted privileges based on their needs in an API.\n\nLet’s implement an RBAC into the OAuth we created recently, by creating a dummy user data inside main.py.\n# Dummy user data\nusers_db = {\n    \"admin\": {\"username\": \"admin\", \"password\": \"password\", \"role\": \"admin\"},\n    \"user\": {\"username\": \"user\", \"password\": \"password\", \"role\": \"user\"}\n}\nTo demonstrate RBAC, admin will have access to our model prediction API endpoint while user will not have access to it. Update the User model to have a role field.\n# Define a user model with role\nclass User(BaseModel):\n    username: str\n    password: str\n    role: str\nJust before the API endpoint, add the following function\n# Role-based access control dependency\ndef role_checker(required_role: str):\n    def role_dependency(current_user: User = Depends(get_current_user)):\n        if current_user.role != required_role:\n            raise HTTPException(\n                status_code=403,\n                detail=\"Operation not permitted\",\n            )\n        return current_user\n    return role_dependency\n\nThe function role_checker() checks for the required role, by taking the required role admin as an argument.\nThe role_dependency() function checks if a user meets a required role, by taking the User as an argument.\nIf the user meets the required role, then the user is granted access, else a 403 status code is returned with a warning \"Operation not permitted\"\n\nUpdate the API endpoint by adding a user argument.\n@app.post(\"/predict\")\nasync def predict(request: PredictionRequest, \n                  token: str = Depends(oauth2_scheme), \n                  current_user: User = Depends(role_checker(\"admin\"))):\n\nThe current_user argument ensures that no User can access an API endpoint unless given permission.\n\n\n\nHow RBAC works in FastAPI.\n\n\nHow RBAC works in FastAPI.\n\n\n\n\n\n\nInput validation involves checking all inputs in an API to ensure that they meet certain requirements, while sanitization is input modification to ensure validity. Validation checks involve checking for allowed characters, length, format, and range, at the same time, sanitization is the changing of the input to ensure it is valid, such as shortening an input, or the removal of HTML tags in an input.\nInput validation and sanitization help to prevent common attacks like SQL injection and Cross-site scripting, most times you use input validation when your user is to give a particular input type, for example, a mobile number which is all digits. Sanitization is used when the user is expected to provide varying input types such as a user’s profile.\n\n\npydantic is a Python library that allows you to define and validate user inputs. It makes it easy to perform schema validation and serialization using type annotations. Earlier on, we used Pyndantic to validate our User and PredictionRequest.\nclass PredictionRequest(BaseModel):\n    sepal_length: float\n    sepal_width: float\n    petal_length: float\n    petal_width: float\n\nclass User(BaseModel):\n    username: str\n    password: str\n    role: str\n\n\n\n\nWhen exchanging data between systems, it’s important to use data transmission protocols to secure and protect the data from unauthorized access. Data transmission security ensures that only authorized users can transmit data, and protect the system from vulnerabilities. There are various protocols one can force to keep data transmission secured such as HTTPS(Hypertext Transfer Protocol Secure), TLS(Transport Layer Security), SSH(Secure Shell), and FTPS(File Transfer Protocol Secure), we will only talk about HTTPS.\n\n\nHTTPS is a secured version of HTTP, where the data is encrypted when data is exchanged between a client and an API. Especially, when confidential details are shared such as user login credentials or account details. Unlike HTTP which has no security layer and makes data vulnerable, HTTPS adds an SSL/TLS layer to ensure that data is encrypted and secured.\n\n\n\nHTTPS workflow. Source: GeeksForGeeks\n\n\nHTTPS workflow. Source: GeeksForGeeks\n\nTo secure data in the API endpoint we created earlier, let’s generate a self-signed certificate for testing. Copy and paste the following code into your terminal.\nopenssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -days 365 -nodes\nThis will generate a self-signed SSL/TLS certificate with a private key using OpenSSL.\n\nopenssl: This is the command-line tool for using the various cryptography functions of OpenSSL’s library.\nreq: This sub-command is used to create and process certificate requests (CSRs) and, in this case, to create a self-signed certificate.\nx509: This option is used to generate a self-signed certificate instead of a certificate request.\nnewkey rsa:4096: This option does two things:\n\nnewkey: It generates a new private key along with the certificate.\nrsa:4096: This specifies the type of key to create, in this case, an RSA key with a size of 4096 bits.\n\nkeyout key.pem: This specifies the file where the newly generated private key will be saved (key.pem).\nout cert.pem: This specifies the file where the self-signed certificate will be saved (cert.pem).\ndays 365: This sets the certificate to be valid for 365 days (1 year).\nnodes: This option ensures that the private key will not be encrypted with a passphrase. Without this option, OpenSSL would prompt for a passphrase to encrypt the private key.\n\nProvide the necessary information to create the key.pem (private key) and cert.pem (certificate).\n\n\n\nGenerating a self-signed certificate using OpenSSL.\n\n\nGenerating a self-signed certificate using OpenSSL.\nAt the end of the main.py file, add the following code.\nimport uvicorn \n\nif __name__ == \"__main__\":\n    uvicorn.run(\n        app, host=\"127.0.0.1\", port=8000, ssl_keyfile=\"key.pem\", ssl_certfile=\"cert.pem\"\n    )\nuvicorn.run ensures your application runs on HTTPS using the generated key.pem and cert.pem.\nYou can now run the API using the following code on your terminal\npython main.py\n\n\n\nUntitled\n\n\nIn a production environment, it is recommended to use a reverse proxy server like Nginx to handle SSL termination and forwarded requests to the FastAPI application, to ensure better performance and security.\n\n\n\n\nEncryption is simply the encoding of sensitive information, such that even if the information were to leak, the content is secured and remains unknown, upon reaching its target destination the data is decoded. This is very useful in protecting sensitive data such as passwords, and only authorized users can decrypt the information using a decryption key. Here is a simple example of how encryption works.\n\nImport all necessary libraries and create an instance of the FastAPI class.\nfrom fastapi import FastAPI, HTTPException, Depends\nfrom pydantic import BaseModel\nfrom cryptography.fernet import Fernet\n\napp = FastAPI()\nNext is to generate key for encryption and decryption using the Fernet class.\n# Generate a key for encryption and decryption\nkey = Fernet.generate_key()\ncipher_suite = Fernet(key)\nCreate an Item model for receiving a text, and the EncryptedItem model for receiving the encrypted text.\n# Models\nclass Item(BaseModel):\n    plaintext: str\n\nclass EncryptedItem(BaseModel):\n    ciphertext: str\nCreate the encryption endpoint.\n@app.post(\"/encrypt/\", response_model=EncryptedItem)\nasync def encrypt_item(item: Item):\n    plaintext = item.plaintext.encode(\"utf-8\")\n    ciphertext = cipher_suite.encrypt(plaintext)\n    return {\"ciphertext\": ciphertext.decode(\"utf-8\")}\nThis takes the given item and encodes it to utf-8 , the cipher_suite key encrypts the plaintext to ciphertext which is a string of gibberish characters.\nCreate the decryption endpoint that decrypts the gibberish characters to the plaintext.\n# Decryption endpoint\n@app.post(\"/decrypt/\", response_model=Item)\nasync def decrypt_item(encrypted_item: EncryptedItem):\n    ciphertext = encrypted_item.ciphertext.encode(\"utf-8\")\n    try:\n        plaintext = cipher_suite.decrypt(ciphertext)\n        return {\"plaintext\": plaintext.decode(\"utf-8\")}\n    except Exception as e:\n        raise HTTPException(status_code=400, detail=\"Decryption failed\")\nThis endpoint takes the encrypted_item and encodes it to utf-8 before decrypting it to plaintext using the cipher_suite function. If the wrong ciphertext is provided, a 400 status code is returned with the detail \"Decryption failed\".\n\n\n\nEncrypting sensitive data in FastAPI\n\n\nEncrypting sensitive data in FastAPI\n\n\n\n\nAnother way of securing APIs is by limiting the number of API calls made to the server. This is where rate limiting and throttling comes into play. Rate limiting is a technique of controlling the amount of incoming and outgoing traffic to or from a network, to prevent abuse and overloading of the server. While throttling on the other hand is temporarily slowing down the rate at which the API processes requests. To apply rate limiting and throttling to our previous example.\n\nEnsure you have installed the slowapi library, a library for implementing rate-limiting and throttling to APIs, and add the following new imports.\nfrom slowapi import Limiter, _rate_limit_exceeded_handler\nfrom slowapi.util import get_remote_address\nfrom slowapi.errors import RateLimitExceeded\nNext is to initialize the rate limiter.\nlimiter = Limiter(key_func=get_remote_address)\napp = FastAPI()\napp.state.limiter = limiter\napp.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\nApply the rate limiter to the /token/ endpoint using @limiter.limit(\"5/minute\") decorator, and the request: Request parameter in the login_for_access_token function.\n@app.post(\"/token\")\n@limiter.limit(\"5/minute\")\nasync def login_for_access_token(request: Request, form_data: OAuth2PasswordRequestForm = Depends()):\nAlso, apply a 10-minute rate limiting to the /predict endpoint. Change the parameter name in the predict function from request to prediction_request to avoid confusion with the new request: Request parameter.\n@app.post(\"/predict\")\n@limiter.limit(\"10/minute\")\nasync def predict(\n    request: Request,\n    prediction_request: PredictionRequest,\n    token: str = Depends(oauth2_scheme),\n    current_user: User = Depends(role_checker(\"admin\"))\n\n\n\n\n\nYou can combine all these methods in your ML Model API to ensure maximum security as much as possible. In this article, you have learned how to implement various API security techniques in your FastAPI model such as authentication, authorization, input validation, sanitization, encryption, rate limiting, and throttling. If you want to dive deep into model deployment with FastAPI, here are some extra resources to keep you busy.\n\nML Model Deployment with FastAPI and Streamlit\nHow to Build an Image Classifier Application on Vultr Using FastAPI and HuggingFace\nHow to Build a WhatsApp Image Generator Chatbot with DALL-E, Vonage and FastAPI\nBuild an SMS Spam Classifier Serverless Database with FaunaDB and FastAPI\nImplementing Rate Limits in FastAPI: A Step-by-Step Guide\nImplementing Logging in FastAPI Applications\nML - Deploy Machine Learning Models Using FastAPI\nDeploying and Hosting a Machine Learning Model with FastAPI and Heroku"
  }
]