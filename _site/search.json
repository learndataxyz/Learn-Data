[
  {
    "objectID": "visuals.html",
    "href": "visuals.html",
    "title": "Visuals",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nDaily Nigeria Crude Oil Prices (2009 - 2025)\n\n\n\nnigeria\n\noil price\n\ntime series\n\nforecast\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "reports.html",
    "href": "reports.html",
    "title": "Reports",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nUber Eats Google Playstore Report 2025\n\n\n\nmarket research\n\nbusiness intelligence\n\ndata analysis\n\ndata storytelling\n\n\n\nThis report provides an in-depth analysis of the Uber Eats app on the Google Play Store as of 2025. It covers various aspects such as user ratings, reviews, download‚Ä¶\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Why Traditional Web Development Skills Matter for Modern ML Engineers/index.html",
    "href": "posts/Why Traditional Web Development Skills Matter for Modern ML Engineers/index.html",
    "title": "Why Traditional Web Development Skills Matter for Modern ML Engineers",
    "section": "",
    "text": "In an era where ‚ÄúML‚Äù and ‚ÄúAI‚Äù are the latest industry buzzwords, one essential skill often overlooked by aspiring machine learning engineers is web development.\nEarly in my ML journey, I was obsessed with leaderboard performance; fine-tuning models on Kaggle and Zindi to gain a 0.0001% boost in accuracy. But I soon realized that outside competitions, performance metrics don‚Äôt matter unless your model is accessible and usable by real people. Most users don‚Äôt care about ROC curves or F1 scores; they care about how the model helps them solve a problem.\nThis realization led me to web development. If users are to interact with your ML solution, you must expose it through intuitive interfaces, APIs, or real-time applications. This article explains why foundational web development skills are increasingly crucial for ML and AI practitioners."
  },
  {
    "objectID": "posts/Why Traditional Web Development Skills Matter for Modern ML Engineers/index.html#the-modern-ml-landscape",
    "href": "posts/Why Traditional Web Development Skills Matter for Modern ML Engineers/index.html#the-modern-ml-landscape",
    "title": "Why Traditional Web Development Skills Matter for Modern ML Engineers",
    "section": "The Modern ML Landscape",
    "text": "The Modern ML Landscape\n\nThe Role of an ML Engineer\nA machine learning engineer‚Äôs typical responsibilities include collecting data, training models, evaluating performance, and fine-tuning hyperparameters. However, this is only part of the pipeline.\nAfter training a model, the next challenge is model deployment, also known as inference, which involves bringing the model into production. This includes creating APIs, deploying models to servers, and integrating them into products that users interact with.\nUnfortunately, many ML engineers focus solely on algorithm performance and neglect this final, critical step: serving the model to end users. This is where web development skills come into play.\n\n\nThe Shift Toward Full-Stack ML\nIn the past, ML engineers handed off their models to software engineers for deployment. Today, ML engineers are expected to build end-to-end solutions, from data ingestion to user interaction, especially in startups and lean teams.\nThis shift has driven the growth of MLOps (Machine Learning Operations), which brings automation, scalability, and monitoring to the ML lifecycle. Basic web development knowledge makes implementing MLOps workflows and building more robust systems easier.\nA model only has real-world value if people can use it. Making it accessible, interactive, and reliable is crucial, and these goals overlap heavily with traditional web development."
  },
  {
    "objectID": "posts/Why Traditional Web Development Skills Matter for Modern ML Engineers/index.html#where-web-development-skills-fit-in",
    "href": "posts/Why Traditional Web Development Skills Matter for Modern ML Engineers/index.html#where-web-development-skills-fit-in",
    "title": "Why Traditional Web Development Skills Matter for Modern ML Engineers",
    "section": "Where Web Development Skills Fit In",
    "text": "Where Web Development Skills Fit In\n\nFrontend Development for ML\nA simple user interface allows users to interact with your model, provide feedback, and validate edge cases. Contrary to popular belief, this doesn‚Äôt require mastering HTML, CSS, or JavaScript from scratch.\n\nPython-friendly frameworks like Streamlit, Dash, and Shiny allow you to build dashboards or web apps with minimal code.\nIf you‚Äôre comfortable learning JavaScript, frameworks like React or Vue offer more flexibility and scalability for production-grade frontends.\n\nThe goal isn‚Äôt to become a designer‚Äîit‚Äôs to make your model accessible and usable.\n\n\nBackend Development and APIs\nServing your model to other applications or users usually involves creating an API.\n\nFlask and FastAPI are two lightweight Python frameworks widely used for building REST APIs.\nWith FastAPI, you can efficiently serve your ML model, handle incoming data, and return predictions‚Äîideal for real-time applications.\n\nYou may also need to persist user inputs or logs. In such cases:\n\nUse ORMs like SQLAlchemy or SQLModel for SQL databases.\nConsider NoSQL databases like MongoDB or Firebase for flexible schema needs.\n\n\n\nDeployment and Infrastructure\nOnce your model is wrapped in an API and connected to a frontend, it must be deployed.\n\nDocker is a great way to containerize your app and ensure consistent deployment across different environments.\nHosting options include Heroku, Render, AWS, Google Cloud, and Azure‚Äîmany offer free tiers for small projects.\n\nSetting up CI/CD pipelines ensures automated testing, deployment, and retraining workflows for production-grade projects. GitHub Actions is a great starting point."
  },
  {
    "objectID": "posts/Why Traditional Web Development Skills Matter for Modern ML Engineers/index.html#real-world-use-cases",
    "href": "posts/Why Traditional Web Development Skills Matter for Modern ML Engineers/index.html#real-world-use-cases",
    "title": "Why Traditional Web Development Skills Matter for Modern ML Engineers",
    "section": "Real-World Use Cases",
    "text": "Real-World Use Cases\nWeb development empowers ML engineers to do much more than just modeling:\n\nInteractive Prototypes: Build a Streamlit app to validate your model‚Äôs performance with real user inputs.\nData Exploration Dashboards: Use Dash or Shiny to allow stakeholders to explore predictions and metrics dynamically.\nML-Powered Products: Integrate models into full-stack apps such as fraud detection tools, recommendation systems, and chatbots.\nStartup Flexibility: In small teams, you‚Äôll often wear multiple hats. Web development lets you move quickly from idea to prototype.\n\nRather than emailing static reports or Jupyter Notebooks, you can deliver interactive tools that provide a better user experience and more valuable insights."
  },
  {
    "objectID": "posts/Why Traditional Web Development Skills Matter for Modern ML Engineers/index.html#addressing-concerns-misconceptions",
    "href": "posts/Why Traditional Web Development Skills Matter for Modern ML Engineers/index.html#addressing-concerns-misconceptions",
    "title": "Why Traditional Web Development Skills Matter for Modern ML Engineers",
    "section": "Addressing Concerns & Misconceptions",
    "text": "Addressing Concerns & Misconceptions\n\n‚ÄúI‚Äôm not a front-end person.‚Äù\nYou don‚Äôt need to be. The goal isn‚Äôt to become a UX designer but to build functional interfaces that allow interaction with your models. Tools like Streamlit, Gradio, or simple HTML templates can get you up and running without a design background.\n\n\n‚ÄúIsn‚Äôt this a software engineer‚Äôs job?‚Äù\nTraditionally, yes‚Äîbut roles are evolving. In cross-functional teams and early-stage startups, ML engineers are often expected to own the full lifecycle, from model design to deployment. Even if you collaborate with software engineers, understanding the system integration process makes you more effective and collaborative.\nKnowing how your model fits into the product ensures you build realistic, reliable, scalable ML solutions."
  },
  {
    "objectID": "posts/Why Traditional Web Development Skills Matter for Modern ML Engineers/index.html#conclusion",
    "href": "posts/Why Traditional Web Development Skills Matter for Modern ML Engineers/index.html#conclusion",
    "title": "Why Traditional Web Development Skills Matter for Modern ML Engineers",
    "section": "Conclusion",
    "text": "Conclusion\nThe era of siloed ML engineering is coming to an end. Today, machine learning is about building accurate models and delivering user value. That value is only realized when models are integrated into real-world systems, accessible through intuitive interfaces, and responsive to feedback.\nWeb development is not a distraction for ML engineers‚Äîit‚Äôs an enabler. It bridges the gap between technical brilliance and practical usability.\nBy acquiring foundational web development skills, you:\n\nGain autonomy to deploy and showcase your work.\nYou can make your models practical, testable, and interactive.\nBecome a more valuable team member in cross-functional settings.\nOpen the door to building and owning full-stack AI products.\n\nIf you want your work to be seen, used, and improved by others, then it‚Äôs time to think beyond the notebook. Start small; build a dashboard with Streamlit, expose your model with FastAPI, or deploy a prototype on Render or Heroku. Each step toward the full-stack mindset moves you closer to turning your models into impactful, user-facing products.\nIn short, learning web development doesn‚Äôt make you less of a machine learning engineer; it makes you complete. If you‚Äôre ready to move beyond the notebook and bring your models to life, the journey starts with small steps: build a dashboard, deploy an API, and iterate.\nTo make that journey easier, I‚Äôve created a practical, beginner-friendly course: ‚ÄúML Model Deployment with FastAPI and Streamlit.‚Äù\nThis course walks you through building and deploying real ML applications using the tools you already know: Python, FastAPI, and Streamlit. So, you can stop waiting for engineers and start deploying your solutions.\n\nNeed Help with Data? Let‚Äôs Make It Simple.\nAt LearnData.xyz, we‚Äôre here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we‚Äôve got your back.\nüìß Shoot us an email at admin@learndata.xyz‚Äîlet‚Äôs chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/Why Traditional Web Development Skills Matter for Modern ML Engineers/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/Why Traditional Web Development Skills Matter for Modern ML Engineers/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "Why Traditional Web Development Skills Matter for Modern ML Engineers",
    "section": "Your next breakthrough could be one email away. Let‚Äôs make it happen!",
    "text": "Your next breakthrough could be one email away. Let‚Äôs make it happen!"
  },
  {
    "objectID": "posts/The Hidden Cost of Missing Data in Machine Learning Models/index.html",
    "href": "posts/The Hidden Cost of Missing Data in Machine Learning Models/index.html",
    "title": "The Hidden Cost of Missing Data in Machine Learning Models",
    "section": "",
    "text": "An ML model is only as good as the data it comes from. Imagine training an image recognition model on blurry images, or a forecasting model on data with 50% of the values missing. The model will perform poorly because they were trained on bad data\nMissing data is common but often underestimated. While some ML algorithms can train data with missing values, this is not encouraged. It‚Äôs best to find the cause of the missing data and, if possible, impute the missing values.\nTraining an ML model on missing data can reduce your model accuracy, giving you the impression of selecting the wrong model, or you need more hyperparameter tuning to achieve better results, whereas the problem was with the data itself.\nIn this article, you will learn some of the hidden costs of missing data in ML models, ways to approach such kinds of data."
  },
  {
    "objectID": "posts/The Hidden Cost of Missing Data in Machine Learning Models/index.html#what-is-missing-data",
    "href": "posts/The Hidden Cost of Missing Data in Machine Learning Models/index.html#what-is-missing-data",
    "title": "The Hidden Cost of Missing Data in Machine Learning Models",
    "section": "What Is Missing Data?",
    "text": "What Is Missing Data?\nMissing data are observations in our data that are missing either due to systematic issues, such as data collection errors, equipment malfunction, or survey design flaws, or due to random factors like non-response, recording mistakes, or accidental data loss.\nMissing datasets are of different types:\n\nMissing Completely at Random (MCAR): Missing observations are unrelated to the data. For example, a researcher accidentally loses some survey responses because of a computer glitch. In this case, the missing responses have nothing to do with the participants or their answers.\nMissing at Random (MAR): Missing observations are related to the observed data, but not to the missing data itself. For example, in a health survey, younger people are less likely to report their income. Here, missing income depends on age, which is already recorded, but not on the income value itself.\nMissing Not at Random (MNAR): In this case, missing observations are related to the data itself, even after accounting for the observed data. For example, People with higher incomes are less likely to report their income; the missing data depends on unobserved values.\n\n\n\n\nTypes of Missing Data. Image by Author."
  },
  {
    "objectID": "posts/The Hidden Cost of Missing Data in Machine Learning Models/index.html#the-hidden-costs",
    "href": "posts/The Hidden Cost of Missing Data in Machine Learning Models/index.html#the-hidden-costs",
    "title": "The Hidden Cost of Missing Data in Machine Learning Models",
    "section": "The Hidden Costs",
    "text": "The Hidden Costs\nMissing data is not something that you should take trivial when training your data; there are a lot of hidden consequences that one can encounter if one does not handle them properly.\n\nModel performance degradation\nIf some classes or groups have more missing values than others, your training data becomes skewed toward the classes that are more complete.\nFor example, in a medical dataset, older patients may have more missing lab test results, and if those patients are more likely to have a disease, your model ends up learning from healthier (younger) samples more often. This results in a skewed training, making the model underrepresent older or diseased individuals.\n\n\nBias amplification\nMissing data rarely appear at random; most times they come from an underrepresented group. For example, minority populations may have less access to medical services, so their health outcomes are underrecorded.\nOr, data collected via smartphones or online platforms often underrepresents groups with limited internet access. So, you can see that the data is not missing at random, but it‚Äôs instead tied to inequality in who gets seen or counted.\nThis makes the issue of missing data sensitive and can lead to bias in a machine learning model, reducing model accuracy.\nIf missing data has groups that are underrepresented, it would not have sufficient data to learn from that group, and become less confident and less accurate when making predictions about them.\nA simple example is a facial recognition model trained mostly on images of light-skinned people; this model will perform worse on dark-skinned individuals, not because of bad algorithms, but because of missing or imbalanced data.\n\n\nOperational & Financial Costs\nA lot of time and money is usually spent trying to clean and impute missing data, especially when the proportion of missing data is high. Not doing all these before training your data can incur operational loss.\nA trained model on missing data can offer wrong classification or prediction to users, which can lead to regulatory risks and also breach of trust in the organization‚Äôs services.\nIf the data was not imputed well, this can distort importance scores or SHAP values, making it harder to trust model insights.\n\n\n\nCosts of missing data. Image by Author.\n\n\nCosts of missing data. Image by Author."
  },
  {
    "objectID": "posts/The Hidden Cost of Missing Data in Machine Learning Models/index.html#mitigation-strategies",
    "href": "posts/The Hidden Cost of Missing Data in Machine Learning Models/index.html#mitigation-strategies",
    "title": "The Hidden Cost of Missing Data in Machine Learning Models",
    "section": "Mitigation Strategies",
    "text": "Mitigation Strategies\nThere are various ways to avoid the costs that come with missing data when training an ML model. Here are some:\n\nPrevention: Avoiding errors during data collection ensures that your data is as clean as possible. If the data is coming through pipelines, ensure that the pipeline components are working properly to avoid components breaking and resulting in missing values in the final dataset. Ensure you also have validation checks to ensure that the right data is always passed; for example, numeric fields should not accept characters, and so on.\nImputation: In the presence of missing data, it‚Äôs always encouraged to use advanced imputation techniques where applicable instead of mean, median, or mode imputation. These techniques take into account the relationship of variables in the dataset, ensuring that at least the closest possible values are imputed.\nMonitoring: After deploying your machine learning model, ensure you always have post-deployment checks to check your model‚Äôs performance. Model drifts can happen, which can occur as a result of bad or missing data from new updates."
  },
  {
    "objectID": "posts/The Hidden Cost of Missing Data in Machine Learning Models/index.html#conclusion",
    "href": "posts/The Hidden Cost of Missing Data in Machine Learning Models/index.html#conclusion",
    "title": "The Hidden Cost of Missing Data in Machine Learning Models",
    "section": "Conclusion",
    "text": "Conclusion\nMissing data at times can be easily ignored, but the costs they carry are heavy. It causes a lot of harm than good, especially for the model and the users of the model.\nBad imputation practices, discarding them when there are many, are reasons that can make you have a bad model fit.\nYou must treat missing data imputation as the first step in your ML lifecycle, and spend enough time imputing it.\nBefore imputation, you should find out the relationship between the missing values and the data to know how best to handle them. This ensures you don‚Äôt impute a missing value when it‚Äôs actually supposed to be missing.\nEven though algorithms can now handle data with missing values, too much of disappeared values might prevent the model not to learn some information, which can bias predictions when predicting on the test set.\nIn summary, treat data completeness as a first-class citizen in your ML lifecycle.\n\nNeed Help with Data? Let‚Äôs Make It Simple.\nAt LearnData.xyz, we‚Äôre here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we‚Äôve got your back.\nüìß Shoot us an email at admin@learndata.xyz‚Äîlet‚Äôs chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/The Hidden Cost of Missing Data in Machine Learning Models/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/The Hidden Cost of Missing Data in Machine Learning Models/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "The Hidden Cost of Missing Data in Machine Learning Models",
    "section": "Your next breakthrough could be one email away. Let‚Äôs make it happen!",
    "text": "Your next breakthrough could be one email away. Let‚Äôs make it happen!"
  },
  {
    "objectID": "posts/Test Driven Development with Python Shiny/index.html",
    "href": "posts/Test Driven Development with Python Shiny/index.html",
    "title": "Test Driven Development with Python Shiny",
    "section": "",
    "text": "What if you could write less code and have more confidence that it will work? When building a shiny application, the larger your codebase, the easier it is to break things while adding features to your application. Writing tests makes it easy to see where and what is breaking, and how to fix it.\nIn this article, you will learn how to build a Shiny application using test-driven development, and why it is the best approach.\nTest-driven development (TDD) is a way of testing applications, where you write tests before writing code, and you keep on editing the code until the test passes.\nUnlike the traditional approach of writing tests, after writing all the application code. TDD is an iterative process and ensures that code is refined until the test is passed. TDD ensures that errors are caught on time, which makes debugging easier."
  },
  {
    "objectID": "posts/Test Driven Development with Python Shiny/index.html#what-makes-testing-shiny-apps-uniquely-challenging",
    "href": "posts/Test Driven Development with Python Shiny/index.html#what-makes-testing-shiny-apps-uniquely-challenging",
    "title": "Test Driven Development with Python Shiny",
    "section": "What Makes Testing Shiny Apps Uniquely Challenging?",
    "text": "What Makes Testing Shiny Apps Uniquely Challenging?\nUnlike traditional web applications, which follow a request-response cycle, where a user makes a request, and the server responds. Shiny applications are based on reactive programming, which means that both the input and outputs are connected, and the output automatically updates when the input is changed.\nThe way Shiny operates, outputs can change without a user‚Äôs action, and a small change can affect other outputs randomly.\nDue to Shiny reactive programming, Shiny stores states between interactions in reactive values, reactive expressions, and observers. Side effects, such as database queries, dynamic interaction, and file uploads, can introduce complexity.\nAlthough it‚Äôs commonly believed that UI can‚Äôt be tested due to its interactive nature, you must test your Shiny apps, bearing in mind their reactive nature and how dynamic their state is."
  },
  {
    "objectID": "posts/Test Driven Development with Python Shiny/index.html#reimagining-tdd-for-reactive-systems",
    "href": "posts/Test Driven Development with Python Shiny/index.html#reimagining-tdd-for-reactive-systems",
    "title": "Test Driven Development with Python Shiny",
    "section": "Reimagining TDD for Reactive Systems",
    "text": "Reimagining TDD for Reactive Systems\nTest-Driven Development (TDD) is a software design approach where tests are written before the actual code. TDD follows three major circles:\n\nRed: This is where you write the tests that fail for a feature.\nGreen: You write the minimum code required to pass the test.\nRefactor: You improve the code while passing the test.\n\nYou might think it is challenging to implement TDD due to the reactive flow of Shiny, but TDD can actually enhance how we build them.\nJust like reactivity, TDD also provides immediate feedback during development, and you can test a reactive flow as input changes.\nShiny is a combination of UI and logic, which can make code complex to test. You must make your code modular by separating various components into functions and modules to make them testable.\nYou can also move functions out of reactivity. This makes it easy to test your functions and test reactivity separately."
  },
  {
    "objectID": "posts/Test Driven Development with Python Shiny/index.html#setting-up-the-environment",
    "href": "posts/Test Driven Development with Python Shiny/index.html#setting-up-the-environment",
    "title": "Test Driven Development with Python Shiny",
    "section": "Setting Up the Environment",
    "text": "Setting Up the Environment\nTo employ TDD in Shiny, ensure you conduct unit tests, reactive tests, and UI simulations. One of the major libraries you will use is pytest; a Python testing framework.\nWhile implementing tests, it‚Äôs important to keep your application modular to make it testable and separate pure functions from reactivity or UI.\nHere is an example project directory with tests for a Shiny application.\nmy_shiny_app/\n‚îÇ\n‚îú‚îÄ‚îÄ app.py                # Main Shiny app entry point\n‚îú‚îÄ‚îÄ logic/\n‚îÇ   ‚îî‚îÄ‚îÄ calc.py           # Pure functions (business logic)\n‚îú‚îÄ‚îÄ ui/\n‚îÇ   ‚îî‚îÄ‚îÄ layout.py         # UI definition (optional, for separation)\n‚îú‚îÄ‚îÄ server/\n‚îÇ   ‚îî‚îÄ‚îÄ handlers.py       # Reactive logic, observers\n‚îú‚îÄ‚îÄ tests/\n‚îÇ   ‚îú‚îÄ‚îÄ test_calc.py      # Tests for pure functions\n‚îÇ   ‚îî‚îÄ‚îÄ test_handlers.py  # Tests for reactive server logic\n‚îú‚îÄ‚îÄ conftest.py           # Pytest fixtures (optional)\n‚îî‚îÄ‚îÄ requirements.txt\nWhen writing your tests, start with the simplest ones, which are pure functions, as they don‚Äôt depend on reactivity or UI."
  },
  {
    "objectID": "posts/Test Driven Development with Python Shiny/index.html#testing-the-core-logic-separately",
    "href": "posts/Test Driven Development with Python Shiny/index.html#testing-the-core-logic-separately",
    "title": "Test Driven Development with Python Shiny",
    "section": "Testing the Core Logic Separately",
    "text": "Testing the Core Logic Separately\nYour shiny applications are usually a mix of reactivity and logic, but you can always extract this logic from your reactive code to make it testable. This logic can include calculations, transformations, and other operations.\nFor example, if your Shiny application allows users to summarize a dataset. Instead of embedding the logic in a reactive expression, you can extract it.\n#logic/stats.py\nimport pandas as pd\n\ndef summarize_sales(df, group_by=\"region\"):\n    \"\"\"Summarize total and average sales by group.\"\"\"\n    if group_by not in df.columns:\n        raise ValueError(f\"Column '{group_by}' not found in dataframe.\")\n    summary = df.groupby(group_by)[\"sales\"].agg([\"sum\", \"mean\"]).reset_index()\n    return summary\n#tests/test_stats.py\nimport pandas as pd\nfrom logic.stats import summarize_sales\n\ndef test_summarize_sales_basic():\n    df = pd.DataFrame({\n        \"region\": [\"East\", \"West\", \"East\", \"West\"],\n        \"sales\": [100, 200, 300, 400]\n    })\n    summary = summarize_sales(df)\n    assert summary.shape == (2, 3)\n    assert set(summary[\"region\"]) == {\"East\", \"West\"}\n    assert summary.loc[summary[\"region\"] == \"East\", \"sum\"].iloc[0] == 400\n\ndef test_summarize_sales_invalid_column():\n    df = pd.DataFrame({\n        \"area\": [\"A\", \"B\"],\n        \"sales\": [100, 200]\n    })\n    try:\n        summarize_sales(df, group_by=\"region\")\n    except ValueError as e:\n        assert \"not found\" in str(e)\nBy isolating your logic into pure functions, you use them both in your application and in tests. It also makes it easy to refactor, without breaking any code. And you can call the application without triggering any reaction."
  },
  {
    "objectID": "posts/Test Driven Development with Python Shiny/index.html#testing-shiny-reactive-functions",
    "href": "posts/Test Driven Development with Python Shiny/index.html#testing-shiny-reactive-functions",
    "title": "Test Driven Development with Python Shiny",
    "section": "Testing Shiny Reactive Functions",
    "text": "Testing Shiny Reactive Functions\nYou need to understand Python Shiny reactivity and how to simulate it in tests, without the need for a complete browser UI or manual clicks.\nShiny reactivity is made of the following :\n\nreactive(): This allows you to wrap a function, such that it updates when the value of its dependencies changes.\nrender_*(): These are specialized reactive outputs that display information coming from the UI. Examples are render_plot(), render_table() , and so on.\ninput - is a user-provided reactive value.\n\nHere is an example of a server logic that implements the above:\n\nfrom shiny import reactive, render\nfrom shiny.express import input, output\n\n@reactive.calc\ndef filtered_data():\n    return dataset[dataset[\"category\"] == input.category()]\n\n@output\n@render.table\ndef summary_table():\n    return filtered_data().groupby(\"type\").sum()\nIn the above example, a user inputs a category and expects a filtered summary of data. We want to ensure that the user input is accurately filtered and displayed to the user. You can simulate the above reactivity like this.\nimport pandas as pd\nfrom shiny import reactive\n\ndef test_filtered_data():\n    data = pd.DataFrame({\"category\": [\"A\", \"B\", \"A\"], \"value\": [1, 2, 3]})\n    \n    @reactive.Calc\n    def category_input():\n        return \"A\"\n    \n    @reactive.Calc\n    def filtered():\n        return data[data[\"category\"] == category_input()]\n    \n    assert filtered().shape[0] == 2 \nIf you notice, each function is wrapped with a reactive.Calc() decorator to make it reactive, and it‚Äôs fed into the filtered() function as an input.\nThe test only passes when the shape of the data is equal to the digit specified. This way, you can test for various inputs expected from a user."
  },
  {
    "objectID": "posts/Test Driven Development with Python Shiny/index.html#ui-testing-how-far-should-you-go",
    "href": "posts/Test Driven Development with Python Shiny/index.html#ui-testing-how-far-should-you-go",
    "title": "Test Driven Development with Python Shiny",
    "section": "UI Testing: How Far Should You Go?",
    "text": "UI Testing: How Far Should You Go?\nMost of the time, there are various arguments on whether you should go for browser-based testing or logic-level testing.\nThere are several factors to consider, and the following table provides a tradeoff analysis of both methods.\n\n\n\n\n\n\n\n\nApproach\nPros\nCons\n\n\n\n\nLogic-level testing(reactive functions, outputs)\nFast, deterministic, easy to debug, and encourages modular design\nDoesn‚Äôt catch integration bugs, No UI assurance\n\n\nBrowser-based testing(full end-to-end)\nCaptures real-world behavior, Tests JavaScript/UI bugs\nSlower, flakier, and harder to debug, Higher maintenance\n\n\n\nSome tools you should consider while building browser tests are:\n\nPlaywright: Fast and provides support for multiple browsers.\nSelenium: Heavier, but behaviour is highly customizable\n\nEven though some are of the view that you should not test UI, I advise that you only test key components of your UI and not everything.\nBrowser tests are best suited if you have form submissions, interactive plots, or downloads in your Shiny application. A common rule of thumb is to test your logic thoroughly and UI sparingly and intentionally.\nYou can test critical workflows, such as file upload, file download, and data summary. Also, prioritize dynamic interactions over static levels, layout, or CSS."
  },
  {
    "objectID": "posts/Test Driven Development with Python Shiny/index.html#common-pitfalls-and-misconceptions",
    "href": "posts/Test Driven Development with Python Shiny/index.html#common-pitfalls-and-misconceptions",
    "title": "Test Driven Development with Python Shiny",
    "section": "Common Pitfalls and Misconceptions",
    "text": "Common Pitfalls and Misconceptions\nAs you decide to implement TDD in your workflow, there are a lot of traps you can fall into. Here are some things you need to know.\n\nTDD doesn‚Äôt mean writing all tests first.\nA common misconception when implementing TDD is that you need to write all your tests first, which is not true.\nYou don‚Äôt have to write all your tests and account for all edge cases; you need to take TDD as a cycle, not a phase.\nYou write the minimum amount of code needed to pass a test, then you iterate. This ensures that you grow the system to ensure safety and clarity.\n\n\nIt‚Äôs not about perfection, but feedback loops.\nMost developers abandon TDD when they see their tests not covering everything, which misses the point of TDD.\nYou need to take TDD like a feedback loop, where you silently refactor your code without the fear of breaking things.\n\n\nOver-mocking, when your tests know too much\nWhen building reactive applications, mocking is handy. However, you can fall into the trap of mocking too much, which means your tests no longer reflect real user behavior.\nWhen this happens, your tests always pass because the mocks were rigged, not that the logic works. And when you change the implementation, your application might work while your tests break.\nOne way to determine if you are over-mocking is by mocking the same internal functions across multiple tests, or by having tests pass with broken logic due to the mock‚Äôs lack of realism."
  },
  {
    "objectID": "posts/Test Driven Development with Python Shiny/index.html#the-long-term-payoff",
    "href": "posts/Test Driven Development with Python Shiny/index.html#the-long-term-payoff",
    "title": "Test Driven Development with Python Shiny",
    "section": "The Long-Term Payoff",
    "text": "The Long-Term Payoff\nWhen starting your project, you might see TDD as a drag as you have to write tests and then ensure they pass, although in the long run, this is more beneficial for your workflow.\nTDD enables you to think in terms of small, testable units, and for Python Shiny applications that can evolve into complex interactive dashboards or data tools. TDD ensures that new changes don‚Äôt unexpectedly break existing behaviours.\nAs Shiny applications expand and become tightly coupled, TDD makes it easier to isolate, test, and maintain features.\nTDD also serves as your documentation, which evolves alongside your codebase. For Python Shiny applications, your tests can clarify expected data flow, user interactions, and reactive behaviours more effectively than a README.\n\nConclusion\nTDD is a fundamental way to build robust, interactive applications, and it‚Äôs possible. It enforces discipline on you and brings clarity when collaborating with others.\nBy writing tests first, you force your code to earn its place and stop guessing how your application behaves. Hence, every line matters, and every feature has its purpose. In this light, TDD is less about testing and more about designing software that meets its requirements.\n\n\nNeed Help with Data? Let‚Äôs Make It Simple.\nAt LearnData.xyz, we‚Äôre here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we‚Äôve got your back.\nüìß Shoot us an email at admin@learndata.xyz‚Äîlet‚Äôs chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/Test Driven Development with Python Shiny/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/Test Driven Development with Python Shiny/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "Test Driven Development with Python Shiny",
    "section": "Your next breakthrough could be one email away. Let‚Äôs make it happen!",
    "text": "Your next breakthrough could be one email away. Let‚Äôs make it happen!"
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#introduction",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#introduction",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Introduction",
    "text": "Introduction\nAccording to WHO, the number of people with diabetes rose from 108 million in 1980 to 422 million in 2014. Diabetes is a serious disease that leads to blindness, kidney failure, heart attacks, strokes, and lower limb amputations. It is mostly prevalent in low- and middle-income countries.\nBuilding a healthcare system that uses machine learning to predict patients with diabetes will help in early detection making it easy for healthcare providers to screen patients with diabetes at an early stage, before diagnosis.\n\nMachine learning models tend to degrade with time, highlighting the need for effective and constant monitoring of the model to know when its performance is declining. Often, this arises as a result of the change in the distribution of the data compared to the data the model was trained upon, this phenomenon is known as Data Drift.\nIn this article, you will learn how to monitor a diabetes classifier and detect data drifts in the data received from patients in a health information management system or mobile application using NannyML.\nNannyML is an open-source library for monitoring machine learning model performance in production, even without the predicted values being ready. It allows you to track your machine-learning model over time and see checkpoints where the model degrades."
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#estimating-model-performance-with-nannyml",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#estimating-model-performance-with-nannyml",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Estimating Model Performance with NannyML",
    "text": "Estimating Model Performance with NannyML\nNannyMl offers binary class classification that one could use to estimate the model‚Äôs performance, even without targets. Model estimation performance with NannyML involves:\n\nGetting the reference and analysis sets ready: The reference set is the data where the model behaves as expected, usually the test data. The analysis set is the latest production data, either with target features or not.\nTraining a performance estimator on the reference set: NannyML uses the reference set to train a performance estimator, it‚Äôs advisable to use the test data as reference data to prevent overfitting.\nUsing the estimator to predict performance on the analysis set (simulating real-world data): NannyML estimates the model performance on the analysis data using the trained performance estimator. One can use various classification metrics, such as accuracy or F1-score. Since misclassifying a patient (false negative) is more severe than misclassifying a healthy patient as diabetic (false positive), the AUC-ROC is the most appropriate metric to use in this case."
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#detecting-data-drift-with-nannyml",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#detecting-data-drift-with-nannyml",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Detecting Data Drift with NannyML",
    "text": "Detecting Data Drift with NannyML\nLet‚Äôs say you deployed a machine-learning model. As time goes on, the model tends to degrade, This is due to the nature of the data changing. If you have an application that you initially designed for kids and you train most of your machine learning models using your current user‚Äôs data, then all of a sudden middle-aged people and the elderly start using your application, and they become more of your users than the kids you designed it for. This will change the age distribution of your data, If age is one important feature in your machine learning model, your model will get worse with time. This is where you need to monitor when such changes happen in your data so that you can update the ML model.\n\nNannyML uses various algorithms to detect data drift, either using Univariate drift detection or Multivariate drift detection methods.\n\nUnivariate drift detection: In this approach, NannyML looks at each feature used in classifying if a patient is diabetic, and compares the chunks with those created from the analysis period. The result of the comparison is called a drift metric, and it is the amount of drift between the reference and analysis chunks, which is calculated for each chunk.\nMultivariate Drift Detection Instead of taking every feature one by one, NannyML provides a single summary metric explaining the drift between the reference and the analysis sets. Although this approach can detect slight changes in the data, it is difficult to explain compared to univariate drift.\n\nIn the case of classifying diabetic patients, undetected drift is dangerous and can lead to wrong model classifications. This is worse if the number of false negatives is high, the classifier might not detect some patients with diabetes, this can lead to late diagnosis."
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#estimating-model-performance-in-the-diabetes-classifier",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#estimating-model-performance-in-the-diabetes-classifier",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Estimating Model Performance in the Diabetes Classifier",
    "text": "Estimating Model Performance in the Diabetes Classifier\nNannyML uses two main approaches to estimate model performance, Confidence-based Performance estimation (CBPE) and Direct Loss estimation (DLE). In this case, we are interested in using the CBPE, since we are dealing with a classification task.\nThe CBPE uses the confidence score of the predictions to estimate the model performance, the confidence score is a value that the diabetes classifier gives for each predicted observation, expressing its confidence in predicting if a patient is diabetic., with values ranging from 0 to 1 and the closer it is to 1, the more confident the classifier is with it‚Äôs prediction.\nThe diabetes data contains 253,680 responses and 21 features. In this section, you will learn how to use this data to build an ML model, estimate your model‚Äôs performance, and detect data drift on updated data.\n\nProject Requirements\nTo get started, ensure you have installed NannyML on your JupyterNotebook. Download the analysis and diabetes data. The diabetes data is the data you will train the machine learning model on, and the analysis data is what you would take as the production data from the patients, which you will use to estimate model performance and detect data drift later on.\n\n\nBuilding the ML Model\nLet‚Äôs build a simple random forest classifier to classify respondents as diabetic.\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load your data\ndiabetes = pd.read_csv(\"binary_diabetes.csv\")\n\n# Split the data into features (X) and target (y)\nX = diabetes.drop('Diabetes_binary', axis=1)\ny = diabetes['Diabetes_binary']\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Create and train the model\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Get the predicted probabilities\ny_pred_proba = model.predict_proba(X_test)[:, 1]  \n\n# Print the classification report\nprint(classification_report(y_test, y_pred))\n\n# Create a dataframe with the test data, predicted classes, and predicted probabilities\ntest_results = X_test.copy()\ntest_results['y_true'] = y_test\ntest_results['y_pred'] = y_pred\ntest_results['y_pred_prob'] = y_pred_proba\nHere is the output of the classification model.\n                                precision    recall  f1-score   support\n\n         0.0       0.88      0.97      0.92     65605\n         1.0       0.48      0.18      0.26     10499\n\n    accuracy                           0.86     76104\n   macro avg       0.68      0.57      0.59     76104\nweighted avg       0.83      0.86      0.83     76104\nFrom the report above, the classifier can classify those with diabetes with a precision of 0.88 accurately. This means the chances of the classifier missing a diabetic patient are low. An overall accuracy of 0.86 indicates the model is performing well.\n\n\nEstimating Model Performance\nUsing CBPE, you can estimate the model performance when in production, and the analysis data does not necessarily have to contain the target feature.\nimport nannyml as nml\n\nreference_df = test_results\nanalysis_df = pd.read_csv(\"/content/analysis_df.csv\")\n\nestimator = nml.CBPE(\n    y_pred_proba =\"y_pred_prob\",\n    y_pred = \"y_pred\",\n    y_true = \"y_true\",\n    problem_type = \"classification_binary\",\n    metrics = [\"roc_auc\"],\n    chunk_size = 3000\n)\n\nestimator.fit(reference_df)\nresults = estimator.estimate(analysis_df)\n\nmetric_fig = results.plot()\nmetric_fig.show()\n\nnml.CBPE calculates the CBPE and takes the argument, y_pred_proba the predicted probability, y_pred the predicted classification, y_true the actual classification.\nThe problem_type argument takes the type of classification one is interested in, binary classification in this case.\nResults in NannyML are presented per chunk, by aggregating the data to a single data point on the monitoring results; here I used a chunk of 3000.\n\nYou can use any other metric, but the roc-auc metric is preferred in this situation because this is a health problem and it tells us more about the ability of the classifier to detect true positives, to minimize the chance of missing any diabetic patient.\n\nThe plot above shows that the model ROC-AUC metric started to fail at some point, which led to inaccurate predictions. The blue marker indicates the model metric, if this line exceeds the upper or lower threshold in red, it raises an alert signifying a drift.\nThe next step is to see what causes the poor model performance in these chunks by applying various drift detection methods.\n\n\nDetecting Data Drift\nLet‚Äôs use a multivariate drift detection method to get a summary number that detects any changes in our data using the domain classifier approach. This provides a measure of discriminating the reference data from the examined chunk data. If there is no data drift, the datasets are not different giving a value of 0.5. The higher the drift, the higher the returned measure, with a value as high as 1. This method provides a general overview of data drift in the analysis data.\nnon_feature_columns = ['y_pred_proba', 'y_pred', 'y_true']\nfeature_column_names = [\n    col for col in reference_df.columns\n    if col not in non_feature_columns\n]\ncat_features = [feature for feature in feature_column_names if feature not in ['BMI', 'MentHlth', 'PhysHlth', 'Age']]\n\ncalc = nml.DomainClassifierCalculator(\n    feature_column_names=feature_column_names,\n    chunk_size=3000\n)\n\ncalc.fit(reference_df)\nresults = calc.calculate(analysis_df)\n\nfigure = results.plot()\nfigure.show()\n\nThe plot above shows the domain classifier values in the reference data are either a little below or above 0.5, but that of the analysis data reaches a value of 1, indicating the presence of data drift in the analysis data. Now that we know we have data drift in the analysis data, let‚Äôs see which features are causing this drift.\n\n\nRanking\nWe need to know the features contributing to drift in the analysis data. The ranking method uses the results of univariate drift detection to rank features based on alert counts or correlation ranking.\nLet‚Äôs use the alert count‚Äôs approach to know which features are causing the drift in data.\nuniv_calc = nml.UnivariateDriftCalculator(\n    column_names=feature_column_names,\n    treat_as_categorical=['y_pred', *cat_features],\n    continuous_methods=['kolmogorov_smirnov', 'jensen_shannon'],\n    categorical_methods=['chi2', 'jensen_shannon'],\n    chunk_size=3000\n)\n\nuniv_calc.fit(reference_df)\nunivariate_results = univ_calc.calculate(analysis_df)\n\nalert_count_ranker = nml.AertCountRanker()\nalert_count_ranked_features = alert_count_ranker.rank(\n    univariate_results.filter(methods=['jensen_shannon']),\n    only_drifting = False)\ndisplay(alert_count_ranked_features)\nkolmogorov_smirnov, jensen_shannonand chi2 are various univariate drift detection methods you can always choose from.\n\nThe table above shows the top 10 features likely to cause drifts based on the alert counts. Next, let‚Äôs investigate further the contribution to data drift for each variable using the univariate drift detection method.\n\n\nUnivariate Drift Detection Method\nUnivariate drift detection allows you to see the amount of drift in the suspected features, which was used earlier to rank the features.\nuniv_calc = nml.UnivariateDriftCalculator(\n    column_names=feature_column_names,\n    treat_as_categorical=['y_pred', *cat_features],\n    continuous_methods=['jensen_shannon'],\n    categorical_methods=['jensen_shannon'],\n    chunk_size=3000\n)\n\nuniv_calc.fit(reference_df)\nunivariate_results = univ_calc.calculate(analysis_df)\n\nfigure = univariate_results.filter(column_names=univariate_results.continuous_column_names, methods=['jensen_shannon']).plot(kind='drift')\nfigure.show()\n\nThe plots above show the amount of drift in some of the features using Jensen-Shannon distance, which you can apply to both continuous and categorical features. You can also see that the red dotted points exceed the dotted line in each plot, this signifies drift in the variable.\nYou can also go further into each feature to see the distribution, this lets you know how large this drift is Let‚Äôs take a critical look at PhyHlth, this is the first feature on the ranked list.\nfigure = univariate_results.filter(column_names=[\"PhysHlth\"], methods=['jensen_shannon']).plot(kind='distribution')\nfigure.show() \n\nFrom the plot, you can see in highlight chunks with data drift, the plot is wider and bigger compared to that of the reference data, the plot also tells the presence of negative values in the analysis data. You can follow this procedure for each feature in the model indicating data drift using the univariate drift detection method. From the above results, you can understand the change in model prediction and the cause of those changes."
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#conclusion",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#conclusion",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, you learned about NannyML an open-source tool for monitoring model performance and detecting data drifts. You also learned how to use NannyML on a diabetes classifier and how to apply both univariate and multivariate drift detection methods in detecting data drift using NannyML.\nWhat‚Äôs next after detecting data drift? Check out this article, to know what to do when you detect drifts in your data."
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#recommended-reads",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#recommended-reads",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Recommended Reads",
    "text": "Recommended Reads\n\nMonitoring a Hotel Booking Cancellation Model Part 1: Creating Reference and Analysis Set\nTutorial: Monitoring an ML Model with NannyML and Google Colab\nHow to Estimate Performance and Detect Drifting Images for a Computer Vision Model?\n\n\nNeed Help with Data? Let‚Äôs Make It Simple.\nAt LearnData.xyz, we‚Äôre here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we‚Äôve got your back.\nüìß Shoot us an email at admin@learndata.xyz‚Äîlet‚Äôs chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Your next breakthrough could be one email away. Let‚Äôs make it happen!",
    "text": "Your next breakthrough could be one email away. Let‚Äôs make it happen!"
  },
  {
    "objectID": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html",
    "href": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html",
    "title": "Linear Regression with Python Statsmodels: Assumptions and Interpretation",
    "section": "",
    "text": "Let‚Äôs say you are a real estate agent and want to know the price of houses based on their characteristics. You will need records of available homes, their features and prices, and you will use this data to estimate the price of a house based on those features.\nThis technique is known as regression analysis, and this article will focus specifically on linear regression. You will also learn about the requirements your data should meet, before you can perform a linear regression analysis using the Python library statsmodels , how to conduct the linear regression analysis, and interpret the results."
  },
  {
    "objectID": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html#what-is-linear-regression",
    "href": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html#what-is-linear-regression",
    "title": "Linear Regression with Python Statsmodels: Assumptions and Interpretation",
    "section": "What is Linear Regression?",
    "text": "What is Linear Regression?\nLinear regression is a statistical technique used to model the relationship between a continuous dependent variable(outcome) and one or more independent variables (predictors) by fitting a linear equation to the observed data. This allows us to understand how the outcome variable changes to the predictor variables.\n\nTypes of linear regression\nWe have various types of linear regression.\n\nSimple Linear Regression: This examines the relationship between a single outcome variable and a single predictor variable.\nMultiple Linear Regression: This examines the relationship between a single outcome variable and multiple predictor variables.\n\n\n\nAssumptions of linear regression\nBefore conducting a linear regression, our data should meet some assumptions:\n\nLinearity: The relationship between the outcome and predictor variables is linear. You can check this by plotting a graph of the predictor variable against the outcome variable and ensuring that the points on the graph form a straight line.\nIndependence: Observations are independent of each other. That is, the occurrence of subsequent observations does not depend on the occurrence of previous observations. This is checked by observing the structure of the data or how the data was collected.\nHomoscedasticity: The variance of the errors is constant across all levels of the independent variables. The difference between predicted and actual values is almost the same in all observations. The Breusch-Pagan test is one way to check for this assumption.\nNormality: The outcome variable is normally distributed. That is, its graph should have a curve similar to a bell-shaped curve. You can check for this assumption by plotting the graph of the outcome variable.\nNo Multicollinearity: In the case of multiple linear regression, independent variables are not highly correlated. If you have two predictors having a strong relationship, you need to drop one and select the other for your model. There are various ways of checking for this, such as using the correlation matrix or checking the variance inflation factor (VIF)."
  },
  {
    "objectID": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html#linear-regression-with-statsmodels",
    "href": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html#linear-regression-with-statsmodels",
    "title": "Linear Regression with Python Statsmodels: Assumptions and Interpretation",
    "section": "Linear Regression with Statsmodels",
    "text": "Linear Regression with Statsmodels\nstatsmodels is a Python library for statistical modeling, hypothesis testing, and data analysis. Take it as a Python library that makes Python feel like a statistical software. We will use it to demonstrate how to develop a linear regression model.\nBefore starting, ensure you have installed the following libraries:\n\npandas for data wrangling\nmatplotlib for data visualization\nseaborn to use the mpg dataset\nstatsmodels for regression analysis\n\nIn this tutorial, we will analyze the mpg dataset and perform a regression analysis to predict mpg (miles per gallon), measuring a car‚Äôs fuel efficiency. We will use the following variables as our predictor variables.\n\ndisplacement - is the total volume of all cylinders in the engine, measured in cubic inches.\nweight - is the weight of the car measured in pounds.\nacceleration is the time a car takes from 0 to 60 mph.\n\nLet‚Äôs load the needed libraries.\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport statsmodels.api as sm\nimport seaborn as sns\n\n%matplotlib inline\nLet‚Äôs load the mpg dataset and drop the row with missing values.\n# Load the 'mpg' dataset from seaborn\ndf = sns.load_dataset('mpg')\n\n# Drop rows with missing values\ndf = df.dropna()\n\ndf.head()\n\n\n\nLoad the mpg dataset. Image by Author\n\n\nLet‚Äôs define the outcome and predictor variables,\n# Define the dependent variable (y) and independent variables (X)\ny = df['mpg']\nX = df[['displacement', 'weight', 'acceleration']]"
  },
  {
    "objectID": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html#checking-for-assumptions",
    "href": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html#checking-for-assumptions",
    "title": "Linear Regression with Python Statsmodels: Assumptions and Interpretation",
    "section": "Checking for assumptions",
    "text": "Checking for assumptions\nBefore we perform the regression analysis, we need to check for some of the assumptions stated earlier.\n\nLinearity\nWe can use a scatterplot to check the linearity of the outcome and each independent variable.\n# Plot the X and y axis\nfig, axes = plt.subplots(nrows=1, ncols=len(X.columns), figsize=(20, 5))\nfig.suptitle(\"Scatter Plots with Linear Regression Lines\", fontsize=16)\n\n# Loop through each column in X and create a scatter plot with regression line\nfor i, col in enumerate(X.columns):\n    sns.regplot(ax=axes[i], x=X[col], y=y, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n    axes[i].set_title(f'{col} vs mpg')\n    axes[i].set_xlabel(col)\n    axes[i].set_ylabel('mpg')\n\nplt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit the subtitle\nplt.show()\n\n\n\nCheck for linearity of relationship between outcome and predictor variables. Image by Author.\n\n\nThe image above shows that mpg is linearly related to each predictor. If one of the plots were to show a curve shape, we would have to drop that predictor since it violates the assumption of linearity.\n\n\nIndependence\nWe can find this out from the nature of the data. For example, you have observations recorded based on time, such as hourly, daily, and so on. We can say these observations depend on each other, as we can‚Äôt have the next observation unless the previous one has already been recorded. However, in the case of the mpg dataset, the observations are drawn from various vehicles and are independent of each other. We can say that this particular assumption of independence is satisfied.\n\n\nNormality\nYou can check this assumption by plotting a density plot of the outcome variable.\nsns.kdeplot(y)\nplt.xlabel('mpg')\nplt.ylabel('Density')\nplt.title('Density Plot of mpg')\nplt.show()\n\n\n\nCheck for normality of the outcome variable. Image by Author.\n\n\nThe above plot shows that the variable mpg has a shape similar to a bell-shaped curve, which implies that this assumption is satisfied.\n\n\nNo Multicollinearity\nYou can investigate multicollinearity by calculating each predictor variable‚Äôs variance inflation factor (VIF). If you have predictors with VIF values greater or equal to five, you can plot a correlation matrix further to check the relationship between these variables.\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['features'] = X.columns\nprint(vif)\n\n\n\nCheck the VIF of each predictor. Image by Author.\n\n\nThe results show that all predictors have VIF values above five. Let‚Äôs plot a correlation matrix to see the relationship between these predictors.\n# Calculate the correlation matrix\ncorrelation_matrix = df[['displacement', 'weight', 'acceleration']].corr()\n\n# Plot the correlation matrix as a heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", cbar=True)\nplt.title(\"Correlation Matrix of Independent Variables\")\nplt.show()\n\n\n\nCorrelation matrix. Image by Author.\n\n\nYou can see that displacement and weight are highly correlated. Let‚Äôs drop the weight variable from our predictors and check the VIF values again.\n# Drop the weight variable\nX = df[['displacement', 'acceleration']]\n\n# Calculate VIF for each independent variable\n\nvif = pd.DataFrame()\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['features'] = X.columns\nprint(vif)\n\n\n\nRemove weight variable, and check VIF values again. Image by Author.\n\n\nThe VIF values from above show that there is no multicollinearity present in the predictor variables. We can now proceed to build the regression model."
  },
  {
    "objectID": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html#building-the-regression-model",
    "href": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html#building-the-regression-model",
    "title": "Linear Regression with Python Statsmodels: Assumptions and Interpretation",
    "section": "Building the regression model",
    "text": "Building the regression model\nBefore proceeding to the regression analysis, let‚Äôs add a constant to the independent variables. This is done to account for the regression model‚Äôs intercept, which is the value of the dependent variable when all independent variables are zero.\n# Add a constant to the independent variables\nX = sm.add_constant(X)\nLet‚Äôs fit the linear regression model.\n# Create and fit the OLS model\nmodel = sm.OLS(y, X)\nresults = model.fit()\n\n# Print the model summary\nprint(results.summary())\n\n\n\nSummary statistics of the regression model. Image by Author.\n\n\n\nHomoscedasticity\nTo check this assumption, we need the results, which we can only get from the summary results. According to the Breusch-Pagan test, if the p-value we get is greater than 0.05, we reject the null hypothesis and conclude that the variance of the errors is constant in all observations.\nfrom statsmodels.stats.diagnostic import het_breuschpagan\n\nbp_test = het_breuschpagan(results.resid, results.model.exog)\nbp_pvalue = bp_test[-1]\nbp_pvalue\n\n&gt;&gt; 1.0127670189356358e-05\nAlthough our data failed the test of homoscedasticity, we can still proceed with the regression model since most of the assumptions of linear regression are already satisfied."
  },
  {
    "objectID": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html#interpretation",
    "href": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html#interpretation",
    "title": "Linear Regression with Python Statsmodels: Assumptions and Interpretation",
    "section": "Interpretation",
    "text": "Interpretation\nThere are a lot of metric in the regression summary, but the most important are the regression coefficients and the R-squared values.\n\nIntercept and coefficients\nThe intercept coefficient 36.1882, is when all predictor variables are zero, that is, the mpg of a car with zero displacement and acceleration.\nThe displacement coefficient -0.0609 means that if a car‚Äôs displacement increases by a unit, its mpg will decrease by -0.0609 units. The p-value of 0.000, less than 0.05, means the relationship is statistically significant. This suggests that displacement and mpg are negatively related.\nIf the acceleration increases by one unit, the mpg is expected to decrease by 0.0582 units. However, the p-value is greater than 0.05, signifying that the relationship is not statistically significant.\n\n\n\nCoefficients of regression. Image of Author.\n\n\n\n\nCoefficient of determination\nThe R-squared indicates the amount of variability explained by the model, while the adjusted R-squared adjusts for the number of predictors in the model. It measures how well the model fits, and choosing the adjusted R-squared over the R-squared is advisable. The adjusted R-squared is 0.647, meaning the model explains 64.7% of the variability in the outcome variable mpg. This indicates a strong fit.\n\n\n\nR-squared and Adjusted R-squared. Image by Author."
  },
  {
    "objectID": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html#conclusion",
    "href": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html#conclusion",
    "title": "Linear Regression with Python Statsmodels: Assumptions and Interpretation",
    "section": "Conclusion",
    "text": "Conclusion\nLinear regression is just one of the many regression analyses, but it‚Äôs easy to conduct and interpret as long as all the model assumptions are met. With what you have learned in this article, I am sure you can apply linear regression to any data you choose and accurately interpret it. Here are some extra resources that explain more of what we have touched in this article.\nAdjusted R-Squared: A Clear Explanation with Examples\nR-squared vs Adjusted R-squared for Regression Analysis\nHomoscedasticity and heteroscedasticity\nHeteroskedasticity vs.¬†Homoskedasticity‚Üí Assumption of Linear Regression\nThe normality assumption in linear regression analysis\n\nNeed Help with Data? Let‚Äôs Make It Simple.\nAt LearnData.xyz, we‚Äôre here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we‚Äôve got your back.\nüìß Shoot us an email at admin@learndata.xyz‚Äîlet‚Äôs chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/Linear Regression with Python Statsmodels Assumptions and Interpretation/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "Linear Regression with Python Statsmodels: Assumptions and Interpretation",
    "section": "Your next breakthrough could be one email away. Let‚Äôs make it happen!",
    "text": "Your next breakthrough could be one email away. Let‚Äôs make it happen!"
  },
  {
    "objectID": "posts/Introduction to Kaplan-Meier Survival Analysis Estimation with Python/index.html",
    "href": "posts/Introduction to Kaplan-Meier Survival Analysis Estimation with Python/index.html",
    "title": "Introduction to Kaplan-Meier Survival Analysis Estimation with Python",
    "section": "",
    "text": "Traditional linear and logistic regression methods have been shown to predict outcomes with minimal bias over time. But there is a caveat: These methods don‚Äôt account for time-dependent outcomes. In this article, you will learn more about Kaplan-Meier survival analysis estimation, its applications, and how to use it to analyze data using the survival analysis Python library lifelines."
  },
  {
    "objectID": "posts/Introduction to Kaplan-Meier Survival Analysis Estimation with Python/index.html#what-is-survival-analysis",
    "href": "posts/Introduction to Kaplan-Meier Survival Analysis Estimation with Python/index.html#what-is-survival-analysis",
    "title": "Introduction to Kaplan-Meier Survival Analysis Estimation with Python",
    "section": "What is Survival Analysis?",
    "text": "What is Survival Analysis?\nImagine a clinical study where patients are given a new cancer treatment, and you want to analyze how long it takes for them to relapse.\nLet‚Äôs say the key variables are the type of treatment, age of the patient, time to relapse, and censoring; that is, patients might relapse at the end of the study with no idea of their relapse time, or some might die or drop out before the end of the study.\nUnlike traditional regression models, we might want to model the time to relapse using the treatment type and age. However, this approach has a lot of setbacks:\n\nIf a patient has not relapsed by the end of the study, their time is censored. We have to either exclude these patients from the study, which can lead to biased estimates, or treat these censored patients as if they had relapsed before the study‚Äôs end, which is incorrect.\nThe risk of relapse is not constant over time, which means the model does not account for the fact that patients might be at different risks at different times.\nThe data may or may not be normally distributed, and linear regression assumes the normality of errors.\nFor logistic regression, it only tells us if a patient relapsed or not but does not tell us when the relapse happened.\n\nSurvival analysis is designed explicitly for time-dependent data and addresses all the above setbacks of traditional regression models. It is a branch of statistics that focuses on analyzing the time until an event of interest occurs. This event can be death, disease relapse, equipment failure, customer churn, or any other time-dependent event."
  },
  {
    "objectID": "posts/Introduction to Kaplan-Meier Survival Analysis Estimation with Python/index.html#terminologies-in-survival-analysis",
    "href": "posts/Introduction to Kaplan-Meier Survival Analysis Estimation with Python/index.html#terminologies-in-survival-analysis",
    "title": "Introduction to Kaplan-Meier Survival Analysis Estimation with Python",
    "section": "Terminologies in Survival Analysis",
    "text": "Terminologies in Survival Analysis\nBefore conducting a survival analysis, there are some terminologies you should be familiar with:\n\nSurvival Time: This is the duration until an event of interest occurs.\nEvent: This is the occurrence of the outcome of interest.\nCensoring: This is incomplete information about a subject, either because the subject experienced the event of interest after the end of the study or the subject dropped out of the study. For example, a patient changing hospital during a cancer study on treatment relapse or the patient relapsing after the end of the study. There are three types of censoring:\n\nRight Censoring: The event of interest did not occur before the end of the study.\nLeft Censoring: The event of interest occurred before the start of the observation period or the subject dropped out of the study.\nInterval Censoring: Events of interest occur within a specific time interval, but the exact time is unknown.\n\nSurvival Function: The probability that a subject will survive beyond a specific time t.\nSurvival Curve: This graphical representation of the survival function over time."
  },
  {
    "objectID": "posts/Introduction to Kaplan-Meier Survival Analysis Estimation with Python/index.html#what-is-kaplan-meier-estimation",
    "href": "posts/Introduction to Kaplan-Meier Survival Analysis Estimation with Python/index.html#what-is-kaplan-meier-estimation",
    "title": "Introduction to Kaplan-Meier Survival Analysis Estimation with Python",
    "section": "What is Kaplan-Meier Estimation?",
    "text": "What is Kaplan-Meier Estimation?\nKaplan-Meier is a statistical methodology used to estimate the survival function from time-to-event data. The survival probability at the time \\(t_i\\) is given as:\n\\[\nS(t_i) = \\prod_{j=1}^{i}(1-\\frac{d_j}{n_j})\n\\]\nWhere:\n\n\\(t_i\\) - Time of the \\(i_{th}\\) event\n\\(d_j\\) - Number of events (e.g., deaths) at \\(t_i\\)\n\\(n_j\\) - Number of individuals at risk just before \\(t_i\\).\n\nDon‚Äôt worry; you won‚Äôt have to calculate this by hand. The lifelines Python library has classes and methods to help you perform a Kaplan-Meier Estimation."
  },
  {
    "objectID": "posts/Introduction to Kaplan-Meier Survival Analysis Estimation with Python/index.html#case-study-employee-churn",
    "href": "posts/Introduction to Kaplan-Meier Survival Analysis Estimation with Python/index.html#case-study-employee-churn",
    "title": "Introduction to Kaplan-Meier Survival Analysis Estimation with Python",
    "section": "Case Study: Employee Churn",
    "text": "Case Study: Employee Churn\nIn this article, we will use the Telco customer churn dataset to demonstrate how to perform a Kaplan-Meier estimation by estimating customer retention and also find out which categories of customer type are likely to churn. Before loading the data, ensure the following libraries are installed.\n\npandas\nmatplotlib\nlifelines\n\nLet‚Äôs add the following imports, load the data, and preview the main variables of interest.\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\ntelco_df = pd.read_csv(\"/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\ntelco_df.loc[:,[\"tenure\",\"Churn\"]].head()\n\n\n\nPreview of the tenure and Churn variables. Image by Author.\n\n\nFor a survival analysis, we are interested in two key variables, event and time, which are Churn and tenure in the dataset, where tenure is measured in months.\nRecode the Churn variable, so 1 represents churn, while 0 represents censoring. Censoring, in this case, means that a customer might churn in the future.\ntelco_df[\"Churn\"] = telco_df[\"Churn\"].map({\"No\":0,\"Yes\":1})\ntelco_df.loc[:,[\"tenure\",\"Churn\"]].head() \n\n\n\nRecode Churn variable into numeric format. Image by Author.\n\n\nRename the tenure and Churn variables to time and event. This is unnecessary, but it is just to make it easy to relate to the concepts discussed earlier.\ntime = telco_df[\"tenure\"]\nevent = telco_df[\"Churn\"]\n\nKaplan-Meier Estimation\nImport the KaplanMeierFitter() class from the lifelines library to perform a Kaplan-Meier estimation. Then, we create an instance of the class and fit it into our data.\nfrom lifelines import KaplanMeierFitter\n\nkmf = KaplanMeierFitter()\nkmf.fit(time, event_observed=event)\nYou can get the Kaplan-Meier estimate for each timeline by calling the survival_function_ method on the kmf object.\nkmf.survival_function_\n\n\n\nKaplan-Meier estimate for each timeline. Image by Author.\n\n\nYou can also get a plot of these values by calling the plot_survival_function method.\nkmf.plot_survival_function()\nplt.title(\"Survival function of customer churn\")\n\n\n\nSurvival function of customer churn. Image by Author.\n\n\n\n\nSurvival Estimate by Group\nWhat if we are interested in seeing the survival plots of various groups in the dataset? You can achieve this by filtering and fitting a survival function for each group and combining it into a single plot. For example, let‚Äôs see the survival plot of customers based on whether they have partners or not.\npartner = telco_df[\"Partner\"]\nix = (partner == \"Yes\")\n\nkmf.fit(time[~ix], event[~ix], label = \"No\")\nax = kmf.plot_survival_function()\n\nkmf.fit(time[ix], event[ix], label = \"Yes\")\nax = kmf.plot_survival_function()\n\n\n\nSurvival plot based on if customers have partners or not. Image by Author.\n\n\nFor variables with more than two groups, you can follow the following approach. Let‚Äôs examine each group based on PaymentMethod and see which payment method group will have the highest retention.\npayment_methods = telco_df[\"PaymentMethod\"].unique()\n\nfor i, payment_method in enumerate(payment_methods):\n    ax = plt.subplot(2, 3, i + 1)\n\n    ix = telco_df['PaymentMethod'] == payment_method\n    kmf.fit(time[ix], event[ix], label=payment_method)\n    kmf.plot_survival_function(ax=ax, legend=False)\n\n    plt.title(payment_method)\n    plt.xlim(0, 50)\n\nplt.tight_layout()\n\n\n\nSurvival estimates by payment method. Image by Author.\n\n\n\n\nAdd a Descriptive Table Below the Plot\nWhen publishing results, following the plots with descriptive tables is encouraged. This table shows the number of customers who will churn at various time intervals, censored customers, and customers at risk of churning.\npartner = telco_df[\"Partner\"]\nix = (partner == \"Yes\")\n\nax = plt.subplot(111)\n\nkmf_yes = KaplanMeierFitter()\nax = kmf_yes.fit(telco_df.loc[ix][\"tenure\"], telco_df.loc[ix][\"Churn\"],label = \"Yes\").plot_survival_function(ax = ax)\n\nkmf_no = KaplanMeierFitter()\nax = kmf_no.fit(telco_df.loc[~ix][\"tenure\"], telco_df.loc[~ix][\"Churn\"],label = \"No\").plot_survival_function(ax = ax)\n\nfrom lifelines.plotting import add_at_risk_counts\nadd_at_risk_counts(kmf_yes, kmf_no, ax=ax)\nplt.tight_layout()\n\n\n\nSurvival estimate by partners with descriptive table. Image by Author.\n\n\n\n\nInterpreting Kaplan-Meier Plots\nInterpreting the Kaplan-Meier plot is easy since it‚Äôs a descriptive statistical methodology. Let‚Äôs take a look at the first plot we had.\n\n\n\nSurvival estimate at mid-duration and end of timeline. Image by Author.\n\n\nIn the plot above, the y-axis represents the number of customers remaining at a given time. The survival probability starts at 1 (100%) at time 0, meaning all customers are present initially. As time progresses, the likelihood of customer retention decreases. The downward trend shows that customer retention is continuous over time.\nThe shaded area indicates the confidence interval for the Kaplan-Meier survival analysis estimate, where a narrower band implies higher confidence and a wider band suggests more uncertainty. Around 75% of customers remain by mid-duration (1). At the end of the timeline, 60% of customers are still active (2), showing that the business retains a significant portion of customers over the period.\n\n\n\nSurvival estimate by partners. Image by Author.\n\n\nThe plot above groups the previous plot into two categories: those with and without partners. The orange curve represents those with partners, while the blue represents those without partners.\nThose with partners have a higher retention probability than those without partners. Their survival probability is around 70% by the end of the timeline.\nThose without partners exhibit a steeper decline in survival probability, indicating they are more likely to churn faster. By the end of the timeline, their survival probability is approximately 45%.\nThis gap between the two curves highlights that the presence of a partner is a significant factor influencing customer retention.\nThe table below the plot further explains the plot. At time zero, the starting population was 3,393 customers with partners and 3,639 without partners. Nine were censored with zero events for customers with partners, while two were censored with zero events for those without partners.\nAt the end of the timeline, 452 customers with partners are at risk of churning, 2,292 have been censored, and 658 churned. For those without partners, 80 are at risk of churning, 2,362 have been censored, and 1,199 have churned.\nThese results show that customers with partners have a higher retention rate than those without partners. This could be due to shared decision-making or factors like excellent financial stability or dual service usage.\n\n\n\nSurvival estimate by payment methods. Image by Author.\n\n\nThe plot above also compares customer retention across four different payment methods. Electronic checks show the poorest retention rate, with a sharp initial and continuous decline. By timeline 50, with a customer retention rate of about 45%.\nMailed check customers show better retention than electronic check users but still demonstrate concerning churn patterns. Starting with a sharp initial drop, the curve continues to decline more gradually, reaching about 75% retention by the end of the period. This suggests that while mailed check users are more stable than electronic check users, they still represent a higher risk group than automatic payment methods.\nBank transfer (automatic) shows strong retention patterns. The curve declines gradually, maintaining approximately 85% retention by the end of the observation period. This suggests that customers who set up automatic bank transfers are significantly more likely to remain loyal customers.\nCredit card (automatic) payments show similarly strong retention patterns to automatic bank transfers. The survival curve remains high, ending at roughly 85% retention. The gradual decline and narrow confidence intervals suggest that automatic credit card payments, like automatic bank transfers, are associated with more stable, long-term customer relationships.\nOverall, a clear pattern shows that automatic payment methods (bank transfer and credit card) are associated with significantly better customer retention than manual payment methods (electronic and mailed checks)."
  },
  {
    "objectID": "posts/Introduction to Kaplan-Meier Survival Analysis Estimation with Python/index.html#conclusion",
    "href": "posts/Introduction to Kaplan-Meier Survival Analysis Estimation with Python/index.html#conclusion",
    "title": "Introduction to Kaplan-Meier Survival Analysis Estimation with Python",
    "section": "Conclusion",
    "text": "Conclusion\nWell, that was a quick introduction to Kapler-Meier survival analysis estimation. I hope by now you can pick any dataset of your choice and implement what you have learned in this article, like estimating the survival function at different timelines and plotting the survival curve. If you want to know more, here are some resources I hope you find helpful.\n12. Survival analysis\nWhat is survival analysis, and when should I use it? - PMC\nThe Ultimate Guide to Survival Analysis\nIntroduction to Survival Analysis with scikit-survival\nThe Kaplan Meier estimate in survival analysis\n\nNeed Help with Data? Let‚Äôs Make It Simple.\nAt LearnData.xyz, we‚Äôre here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we‚Äôve got your back.\nüìß Shoot us an email at admin@learndata.xyz‚Äîlet‚Äôs chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/Introduction to Kaplan-Meier Survival Analysis Estimation with Python/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/Introduction to Kaplan-Meier Survival Analysis Estimation with Python/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "Introduction to Kaplan-Meier Survival Analysis Estimation with Python",
    "section": "Your next breakthrough could be one email away. Let‚Äôs make it happen!",
    "text": "Your next breakthrough could be one email away. Let‚Äôs make it happen!"
  },
  {
    "objectID": "posts/How to Plot a Time Series Plot with Python Plotnine/index.html",
    "href": "posts/How to Plot a Time Series Plot with Python Plotnine/index.html",
    "title": "How to Plot a Time Series Plot with Python Plotnine",
    "section": "",
    "text": "When you have data with observations in subsequent time intervals, such as hourly, daily, weekly, or yearly.\nVisualizing such data enables you to identify trends and seasonality over time and forecast future values.\nIn this article, you will learn how to use Python Plotnine, a Python library based on ggplot2 that allows you to build plots based on the grammar of graphics."
  },
  {
    "objectID": "posts/How to Plot a Time Series Plot with Python Plotnine/index.html#prerequisites",
    "href": "posts/How to Plot a Time Series Plot with Python Plotnine/index.html#prerequisites",
    "title": "How to Plot a Time Series Plot with Python Plotnine",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nGoogle Colab, Jupyter Notebook, or Positron installed\nThe following libraries were installed:\n\nscikit-misc - Miscellaneous scientific algorithms\nnumpy - Numerical computing\npandas - Data Analysis\nplotnine - Data visualization based on the grammar of graphics"
  },
  {
    "objectID": "posts/How to Plot a Time Series Plot with Python Plotnine/index.html#preparing-the-data",
    "href": "posts/How to Plot a Time Series Plot with Python Plotnine/index.html#preparing-the-data",
    "title": "How to Plot a Time Series Plot with Python Plotnine",
    "section": "Preparing the Data",
    "text": "Preparing the Data\nBefore you proceed, you need to download and prepare the data for the time series visualization. In this tutorial, you will make use of the daily Netflix stock price data from the year 2000 to date.\nFirst of all, import the following libraries\nimport scikit_misc\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom plotnine import (\n    ggplot, aes, geom_line, geom_point, geom_smooth,\n    labs, theme_minimal, theme, scale_x_datetime,\n    element_text, element_blank\n)\nImport the data.\ndf = pd.read_csv('/content/Netflix_stock_history.csv')\ndf.head()\n\n\n\nNetflix stock history. Image by author.\n\n\nEnsure the Date column is in datetime format.\ndf['Date'] = pd.to_datetime(df['Date'], utc=True)\nYou can resample the data to any preferred frequency; either weekly, monthly, or yearly.\nFor weekly frequency,\n# Weekly aggregation \ndf_weekly = df.set_index('Date').groupby([pd.Grouper(freq='W')]).agg({\n    'Close': 'mean'\n}).reset_index()\n\ndf_weekly.head()\n\n\n\nWeekly frequency. Image by Author.\n\n\nFor monthly,\n# Monthly aggregation\ndf_monthly = df.set_index('Date').groupby([pd.Grouper(freq='M')]).agg({\n    'Close': 'mean'\n}).reset_index()\n\ndf_monthly.head()\n\n\n\nMonthly frequency. Image by Author.\n\n\nFor yearly,\n# Yearly aggregation\ndf_yearly = df.set_index('Date').groupby([pd.Grouper(freq='Y')]).agg({\n    'Close': 'mean'\n}).reset_index()\n\ndf_yearly.head()\n\n\n\nYearly frequency. Image by Author."
  },
  {
    "objectID": "posts/How to Plot a Time Series Plot with Python Plotnine/index.html#creating-a-basic-time-series-plot",
    "href": "posts/How to Plot a Time Series Plot with Python Plotnine/index.html#creating-a-basic-time-series-plot",
    "title": "How to Plot a Time Series Plot with Python Plotnine",
    "section": "Creating a Basic Time Series Plot",
    "text": "Creating a Basic Time Series Plot\nIf you have used ggplot2 , you should not find it difficult to grasp plotnine. They both work using the same principle: The Grammar of Graphics.\nThe grammar of graphics works by adding layers to elements to build a plot.\nHere is a basic example using our data to plot a line plot.\nbasic_plot = (\n    ggplot(df) +\n    aes(x='Date', y='Close') +\n    geom_line()\n)\n\nbasic_plot\n\n\n\nLine plot showing Netflix closing stock price history. Image by Author.\n\n\nFirst of all, you specify your dataset inside the ggplot() function, then add the axis in the aes() function to it, then finally the geom_line() function to create a line plot.\nAll these are layers, and you can add more to your plot using the + sign.\nstyled_plot = (\n    ggplot(df) +\n    aes(x='Date', y='Close') +\n    geom_line(color='steelblue', size=1.2, linetype='solid')\n)\nstyled_plot\nThe geom_line() has arguments where you can specify the color, size, and linetype of your plot.\n\n\n\nColor the line. Image by Author."
  },
  {
    "objectID": "posts/How to Plot a Time Series Plot with Python Plotnine/index.html#enhancing-the-plot",
    "href": "posts/How to Plot a Time Series Plot with Python Plotnine/index.html#enhancing-the-plot",
    "title": "How to Plot a Time Series Plot with Python Plotnine",
    "section": "Enhancing the Plot",
    "text": "Enhancing the Plot\nIf your dataset has multiple categories, you can add it to the color argument in the aes() function.\nThough, we don‚Äôt have a category in our dataset, we will make do of the closing prices as a category for demonstration purposes.\nmulti_category_plot = (\n    ggplot(df) +\n    aes(x='Date', y='Close', color='Close') +\n    geom_line(size=1)\n)\n\nmulti_category_plot\n\n\n\nAdd colors to your line plot. Image by Author.\n\n\nYou can use the labs() function to add titles and axes labels to your plots.\nenhanced_plot = (\n    ggplot(df) +\n    aes(x='Date', y='Close', color='Close') +\n    geom_line(size=1.2) +\n    labs(\n        title='Time Series Analysis with Plotnine',\n        subtitle='Netflix closing values from 2020-Date with trend and seasonality',\n        x='Date',\n        y='Close',\n        color='Close'\n    ) \n)\nenhanced_plot\n\n\n\nAdd title and axis labels. Image by author.\n\n\nYou can also add a smoothing line to show the trend on the plot.\nadvanced_plot = (\n    ggplot(df) +\n    aes(x='Date', y='Close', color='Close') +\n    geom_line(alpha=0.6, size=0.8) +  # Semi-transparent lines\n    geom_smooth(method='loess', se=True, alpha=0.2) +  # Smoothing with confidence interval\n    labs(\n        title='Advanced Time Series with Trend Lines',\n        subtitle='Original data with LOESS smoothing and confidence intervals',\n        x='Date',\n        y='Close',\n        color='Close'\n    ) +\n    theme_minimal() +\n    theme(\n        panel_grid_minor=element_blank(),  # Remove minor grid lines\n        legend_position='bottom',\n        plot_title=element_text(size=14, face='bold')\n    )\n)\n\nadvanced_plot\n\n\n\nAdd a smoothing line to the plot. Image by Author."
  },
  {
    "objectID": "posts/How to Plot a Time Series Plot with Python Plotnine/index.html#conclusion",
    "href": "posts/How to Plot a Time Series Plot with Python Plotnine/index.html#conclusion",
    "title": "How to Plot a Time Series Plot with Python Plotnine",
    "section": "Conclusion",
    "text": "Conclusion\nUnlike other Python visualization libraries, plotnine brings an elegant way of building visualizations using the grammar of graphics, where you build visualizations layer upon layers. You first of all prepare your data, then your plot, then you customize, and finally, enhance your plot.\nYou can go beyond this and explore various other types of visualizations in plotnine, and also further way to customize your plot.\n\nNeed Help with Data? Let‚Äôs Make It Simple.\nAt LearnData.xyz, we‚Äôre here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we‚Äôve got your back.\nüìß Shoot us an email at admin@learndata.xyz‚Äîlet‚Äôs chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/How to Plot a Time Series Plot with Python Plotnine/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/How to Plot a Time Series Plot with Python Plotnine/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "How to Plot a Time Series Plot with Python Plotnine",
    "section": "Your next breakthrough could be one email away. Let‚Äôs make it happen!",
    "text": "Your next breakthrough could be one email away. Let‚Äôs make it happen!"
  },
  {
    "objectID": "posts/How to Dockerize a Python Shiny Application/index.html",
    "href": "posts/How to Dockerize a Python Shiny Application/index.html",
    "title": "How to Dockerize a Python Shiny Application",
    "section": "",
    "text": "Have you ever built a Shiny application, only to watch it fail when you try running it in a different environment?\nIt‚Äôs frustrating, time-consuming, and, unfortunately, quite common. This is precisely the type of problem Docker was created to solve.\nAlthough the term Docker is not new to you, understanding its potential to streamline Shiny application development and deployment is a game-changer.\nYou might wonder, ‚ÄúWhy bother with Docker? Shiny applications aren‚Äôt that complex.‚Äù\nTrue, they are not the most complicated pieces of software, but that‚Äôs not the point. The real question is: how can you make deploying your application smoother, faster, and more reliable, no matter where it runs?\nWhen you view it from the angle of convenience and consistency, the benefits of Docker become much clearer.\nDocker brings three significant advantages to the table: portability, reproducibility, and simplified deployment. With Docker, you can package your Shiny application along with all its dependencies into a single container.\nThis means you can run it on any system without worrying about missing packages, version mismatches, or other environmental issues. You can even switch hosting providers with minimal effort, sidestepping the pain of vendor lock-in.\nTo help you get started, I‚Äôve put together this practical and straightforward guide, designed to show you step-by-step how you can package, test, and launch your Shiny application using Docker."
  },
  {
    "objectID": "posts/How to Dockerize a Python Shiny Application/index.html#prerequisites",
    "href": "posts/How to Dockerize a Python Shiny Application/index.html#prerequisites",
    "title": "How to Dockerize a Python Shiny Application",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore proceeding, ensure you have the following ready.\n\nPython 3.9+ installed\nDocker Desktop\nBasic command line knowledge\nA sample Shiny project with the following file directory. Where app folder and requirements.txt contain your Shiny application and project dependencies, respectively.\nmy_shiny_app/\n‚îú‚îÄ‚îÄ app/\n‚îÇ   ‚îî‚îÄ‚îÄ app.py\n‚îî‚îÄ‚îÄ requirements.txt\nA sample Shiny application in your app.py file, use the one below if you don‚Äôt have one.\nfrom shiny import App, render, ui\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Hello Shiny!\"),\n    ui.input_text(\"name\", \"Enter your name:\", value=\"World\"),\n    ui.input_slider(\"n\", \"Number of greetings:\", min=1, max=10, value=1),\n    ui.output_text(\"greeting\"),\n)\n\ndef server(input, output, session):\n    @render.text\n    def greeting():\n        return f\"Hello {input.name()}! \" * input.n()\n\napp = App(app_ui, server)"
  },
  {
    "objectID": "posts/How to Dockerize a Python Shiny Application/index.html#step-by-step-process",
    "href": "posts/How to Dockerize a Python Shiny Application/index.html#step-by-step-process",
    "title": "How to Dockerize a Python Shiny Application",
    "section": "Step-by-Step Process",
    "text": "Step-by-Step Process\n\nStep 1 ‚Äì Create a Dockerfile\nTo containerize your application, you need to create a Dockerfile. This Dockerfile is what you will use to create an image.\nTake your Dockerfile as the blueprint for a building, while the final product, which is the Docker Image, is the completed building.\nDocker uses your Dockerfile as a step-by-step process in building your Shiny application Image.\nIn your project directory, create a new file Dockerfile .Then, copy and paste the code below.\nFROM python:3.11-slim\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\nCOPY . .\nCMD [\"shiny\", \"run\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"app.py\"]\nLet‚Äôs break down the code above.\n\nFROM python:3.11-slim instructs Docker to use a Python base image with version 3.11. The-slim tag tells Docker to use a lightweight Python version.\nWORKDIR /app sets the working directory inside the container to /app.\nCOPY requirements.txt . copies the requirements.txt into the current directory within the container.\nRUN pip install --no-cache-dir -r requirements.txt installs all Python dependencies into the container. The -- no-cache-dir avoids storing pip‚Äôs cache inside the image to reduce the image size.\nCOPY . . Copies the entire local project directory into the /app directory in the container.\nCMD [\"shiny\", \"run\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"[app.py](http://app.py/)\"] sets the default command to run when the container starts on the host 0.0.0.0 and port 8000.\n\n\n\nStep 2 ‚Äì Build the Docker Image\nUse the command below to build the Docker Image.\ndocker build -t shiny-python-app .\n\n\n\nBuild your Dockerfile. Image by Author.\n\n\nIf the build was successful, you should see a message similar to the following.\n\n\n\nRun the Docker Image. Image by Author.\n\n\n\n\nStep 3 ‚Äì Run the Container\nA Container is an instance of an Image; this implies that an Image can spawn many containers.\nRun the command below to run the container.\ndocker run -p 8000:8000 shiny-python-app\nYou will see your application available at http://localhost:8000\n\n\n\nStep 4 ‚Äì Push to Docker Hub\nFirst of all, you have to log in to Docker Hub if you are not logged in, run the command below.\ndocker login\n\n\n\nLogin to Docker. Image by Author.\n\n\nIf you want to push an Image to Docker Hub, you need to specify your username and the Image tag using the following format during builds; &lt;your-user-name&gt;/&lt;your-app-name&gt;:&lt;tag&gt;.\nThe tag is a label you give to various versions of your Image.\ndocker build -t adejumoridwan/shiny-python-app:latest .\nFinally, push the Image to Docker Hub.\ndocker push adejumoridwan/shiny-python-app:latest\nGo to the My Hub tab on your Docker Hub dashboard, and you should see your published Image.\n\n\n\nGo to Docker Hub. Image by Author."
  },
  {
    "objectID": "posts/How to Dockerize a Python Shiny Application/index.html#notes-when-building-docker-applications",
    "href": "posts/How to Dockerize a Python Shiny Application/index.html#notes-when-building-docker-applications",
    "title": "How to Dockerize a Python Shiny Application",
    "section": "Notes When Building Docker Applications",
    "text": "Notes When Building Docker Applications\nThere are some points you need to take note of when building your application:\n\nUse .dockerignore to keep your build clean\nA .dockerignore is just like .gitignore. It instructs Docker not to copy the specified files into the image.\nThis is important because when you run docker build ., it will copy all files in your entire directory unless you explicitly exclude them.\nFor your Shiny application, you would want to exclude things like; __pycache__/, .pytest_cache/, .git , venv/ and so on.\nMost of the above files are usually huge and can increase the size of your image.\nHere is an example of a .dockerignore file.\n__pycache__/\n*.pyc\n*.pyo\n*.pyd\n*.log\n*.db\n.venv/\n.env\n.git\n.gitignore\nDockerfile\ndocker-compose.yml\n\n\nUse multi-stage builds for smaller images\nIn multi-stage builds, you separate your build environment from the runtime environment.\nIn your Shiny applications, you can:\n\nInstall dependencies, compile wheels, or build your frontend assets if you have them.\nCopy only the necessary runtime files (app code + installed dependencies) into a slimmer base image like python:3.11-slim.\n\nHere is an example of a multi-stage build, where the runtime Python environment is a lightweight version of the build Python environment.\n# Stage 1: Build environment\nFROM python:3.11 AS builder\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install --user -r requirements.txt\n\n# Stage 2: Final runtime\nFROM python:3.11-slim\nWORKDIR /app\n\n# Copy dependencies from builder\nCOPY --from=builder /root/.local /root/.local\n\n# Update PATH for installed packages\nENV PATH=/root/.local/bin:$PATH\n\n# Copy only necessary app files\nCOPY app.py .\n\n# Run shiny app\nCMD [\"shiny\", \"run\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"app.py\"]\n\n\nEnsure your app binds to 0.0.0.0 inside Docker.\nBy default, your Shiny application binds to 127.0.0.1 which is your localhost, but inside Docker, 127.0.0.1 means the content is only accessible inside the container, which implies your host machine won‚Äôt be able to reach it.\nTo make your application accessible outside your container, bind it to all interfaces, using the command below.\nshiny run --host 0.0.0.0 --port 8000 app.py"
  },
  {
    "objectID": "posts/How to Dockerize a Python Shiny Application/index.html#conclusion",
    "href": "posts/How to Dockerize a Python Shiny Application/index.html#conclusion",
    "title": "How to Dockerize a Python Shiny Application",
    "section": "Conclusion",
    "text": "Conclusion\nAlthough setting up Docker to deploy your applications may seem daunting, the effort is worth it in the long run.\nEspecially if your application grows and you are collaborating with others, Docker makes it easy to share the same environment without the fear of the application breaking from a collaborator‚Äôs end.\nYou can extend your learning by reading more on Docker Compose, a tool designed to run multiple Docker containers on a single host. This is particularly useful if your application interacts with various services, such as backends, databases, and others.\nYou can also read more on CI/CD integration with Shiny to avoid manually deploying your application every time you make changes.\n\nNeed Help with Data? Let‚Äôs Make It Simple.\nAt LearnData.xyz, we‚Äôre here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we‚Äôve got your back.\nüìß Shoot us an email at admin@learndata.xyz‚Äîlet‚Äôs chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/How to Dockerize a Python Shiny Application/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/How to Dockerize a Python Shiny Application/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "How to Dockerize a Python Shiny Application",
    "section": "Your next breakthrough could be one email away. Let‚Äôs make it happen!",
    "text": "Your next breakthrough could be one email away. Let‚Äôs make it happen!"
  },
  {
    "objectID": "posts/How R Helps You Catch Errors Before They Become Headlines/index.html",
    "href": "posts/How R Helps You Catch Errors Before They Become Headlines/index.html",
    "title": "How R Helps You Catch Errors Before They Become Headlines",
    "section": "",
    "text": "In a world driven by data, accuracy is crucial for informing business decisions, shaping public policy, or guiding scientific research; even a minor error in a dataset can lead to significant consequences. A misplaced decimal, a missing value, or a wrong assumption can quickly spiral into misinformation, financial loss, or damaged reputations.\nWe‚Äôve seen it happen: news headlines expose flawed reports, insufficient data erodes public trust, and organizations scramble to correct errors that could have been caught early. These real-world data scandals highlight a critical truth: ensuring accuracy isn‚Äôt just a technical task; it‚Äôs a responsibility.\nThat‚Äôs where tools like R come in. With the proper practices and built-in features, R empowers analysts and data scientists to spot issues before they escalate, helping prevent the next headline-making blunder.\nIn 2012, JP Morgan Chase suffered a $6 billion trading loss, partly attributed to errors in an Excel risk model. The model contained manual copy-paste errors and formula mistakes, resulting in an underestimation of potential losses. This incident highlights the risks associated with relying on complex spreadsheets without rigorous checks and balances."
  },
  {
    "objectID": "posts/How R Helps You Catch Errors Before They Become Headlines/index.html#why-r",
    "href": "posts/How R Helps You Catch Errors Before They Become Headlines/index.html#why-r",
    "title": "How R Helps You Catch Errors Before They Become Headlines",
    "section": "Why R?",
    "text": "Why R?\n\nR as a tool for data analysis and validation\nR is a powerful environment specifically built for data analysis, statistical computing, and visualization. What sets R apart is its strong focus on data integrity and reproducibility, making it an excellent choice for catching and correcting errors before they become costly.\nRegarding data validation, R offers many tools to clean, inspect, and audit datasets. Analysts can write scripts to automate checks, identify anomalies, and enforce consistency across large data sets. Unlike manual spreadsheet work, R encourages repeatable workflows, reducing the risk of human error.\nR‚Äôs active ecosystem includes packages like dplyr, readr, janitor, and assertr, each designed to streamline the process of identifying and handling dirty or flawed data, such as checking for missing values, outliers, or logic inconsistencies.\n\n\nBuilt-in features for error detection\nR offers a robust set of built-in features and packages designed to detect and prevent data errors before they escalate into significant issues.\nFunctions such as summary(), str(), and head() enable you to quickly inspect the structure and contents of your data. These commands can reveal missing values, unexpected data types, or unusual ranges; all red flags that something may be amiss.\nR also provides clear warnings and error messages when something goes wrong during code execution. For example, if you attempt a calculation on incompatible data types, R will alert you with informative feedback, making it easier to diagnose and fix the issue.\nAdditionally, tools like is.na() help identify missing values, while functions such as duplicated() and anyDuplicated() catch repeated entries that could skew results. Logical checks such as all(), any(), and stopifnot() enforce rules and conditions in your workflow, effectively acting as built-in safeguards.\nFor more advanced needs, R offers specialized packages such as assert, validate, and testthat, which formalize and automate error detection and testing routines."
  },
  {
    "objectID": "posts/How R Helps You Catch Errors Before They Become Headlines/index.html#common-error-types",
    "href": "posts/How R Helps You Catch Errors Before They Become Headlines/index.html#common-error-types",
    "title": "How R Helps You Catch Errors Before They Become Headlines",
    "section": "Common Error Types",
    "text": "Common Error Types\nWhen working with data, mistakes can come from many sources. Recognizing standard error types is the first step toward preventing them from affecting your analysis:\n\nData Entry Errors: Manual data entry is prone to typos, incorrect formats, and misclassifications. For example, entering ‚Äú220‚Äù instead of ‚Äú22‚Äù for age, or misspelling a category as ‚ÄúFemle‚Äù instead of ‚ÄúFemale,‚Äù can create false patterns in analysis.\nMissing or Inconsistent Values: Missing data is frequent, especially when combining datasets from multiple sources. Gaps can appear as NA, empty strings, or zero values, requiring different handling. Inconsistencies, like mixing ‚ÄúYes/No‚Äù with ‚ÄúY/N‚Äù, can make grouping or summarizing data difficult.\nLogical Inconsistencies: These errors occur when data contradicts itself. For instance, a record showing a person‚Äôs birthdate after their date of death, or a student listed as ‚Äúgraduated‚Äù but with no courses completed. Such issues often go unnoticed until they cause problems in the analysis or report."
  },
  {
    "objectID": "posts/How R Helps You Catch Errors Before They Become Headlines/index.html#base-r-functions-for-preliminary-checks",
    "href": "posts/How R Helps You Catch Errors Before They Become Headlines/index.html#base-r-functions-for-preliminary-checks",
    "title": "How R Helps You Catch Errors Before They Become Headlines",
    "section": "Base R Functions for Preliminary Checks",
    "text": "Base R Functions for Preliminary Checks\n\nsummary()\nThe summary() function gives a quick statistical summary of each column in a data frame, For factors, it shows the frequency of each level; for logical it shows the counts of TRUEs and FALSEs, while for numeric data it shows the:\n\nminimum\nfirst quartile\nmedian\nmean\nthird quartile\nmaximum\n\nIt is used to quickly identify outliers or unexpected values. Here is an example using the iris data.\nsummary(iris)\n\n\n\nsummary() of the iris data. Image by Author.\n\n\n\n\nstr()\nThe str() function reveals the internal structure of an R object, displaying information such as data types and sample values for each variable, as well as the number of observations and variables.\nBefore proceeding with the analysis, you should use str() to understand the kind of data you are handling.\nstr(iris)\n\n\n\nstr() revealing the structure of the iris dataset. Image by Author.\n\n\n\n\nis.na()\nis.na() function detects missing values (NA) in your data, returning a logical vector of TRUE where values are NA, and FALSE otherwise. It checks for incomplete data, which can affect analysis or model performance.\nsum(is.na(iris))  # Count total missing values\n\n\n\nTotal missing values in the iris dataset. Image by Author.\n\n\n\n\nduplicated()\nThe duplicated() function identifies duplicate rows or elements and returns a logical vector indicating which entries are duplicated. This is useful for cleaning datasets and flagging potential data entry issues.\nduplicated(iris)        # Shows which rows are duplicates\niris[duplicated(iris), ]  # Displays duplicate rows\n\n\n\nNumber of duplicated values in each column of the iris dataset. Image by Author.\n\n\n\n\nExternal Packages\nOther external packages extend what R does in terms of detecting errors.\n\nvalidate package\nThe validate package lets users define and apply custom validation rules to datasets. For example:\nlibrary(validate)\n\n# Define validation rules for iris\n# For example:\n# - Sepal.Length and Sepal.Width should be positive\n# - Species should be one of the known species in iris\nrules &lt;- validator(\n  Sepal.Length &gt; 0,\n  Sepal.Width &gt; 0,\n  Species %in% c(\"setosa\", \"versicolor\", \"virginica\")\n)\n\n# Apply the rules to the iris dataset\nconfronted &lt;- confront(iris, rules)\n\n# Summarize the results of the validation\nsummary(confronted)\n\n\n\nSummary of the results validation using the validator package. Image by Author.\n\n\nThis approach helps systematically identify records that violate predefined conditions, ensuring data consistency and reliability.\n\n\nassertr package\nassertr provides a pipeline-friendly syntax for asserting conditions on data frames. It integrates seamlessly with dplyr and checks assumptions within data processing workflows. For example:\n# Load necessary packages\nlibrary(dplyr)\nlibrary(assertr)\n\n# Use dplyr pipeline and assert conditions on iris data\niris |&gt;\n  # Assert Sepal.Length is positive\n  assert(within_bounds(0, Inf), Sepal.Length) |&gt;\n  \n  # Assert Sepal.Width is positive\n  assert(within_bounds(0, Inf), Sepal.Width) |&gt;\n  \n  # Assert Species is one of the valid species\n  assert(in_set(c(\"setosa\", \"versicolor\", \"virginica\")), Species) |&gt;\n  \n  # Select only Sepal measurements\n  select(Sepal.Length, Sepal.Width, Species)\n\n\n\nImplementation of the assert package. Image by Author.\n\n\n\n\ncheckmate package\nThe checkmate package is designed for defensive programming, offering a suite of functions to validate function arguments and data structures. It ensures that inputs meet expected criteria before further processing, reducing the risk of runtime errors. Here is an example using the iris dataset:\n# Load the checkmate package\nlibrary(checkmate)\n\n# Define a function that processes iris data\nprocess_iris_data &lt;- function(data) {\n  \n  # Defensive checks using checkmate\n  assert_data_frame(data, any.missing = FALSE, min.rows = 1)\n  assert_names(names(data), must.include = c(\"Sepal.Length\", \"Sepal.Width\", \"Species\"))\n  assert_numeric(data$Sepal.Length, lower = 0)\n  assert_numeric(data$Sepal.Width, lower = 0)\n  assert_factor(data$Species, levels = c(\"setosa\", \"versicolor\", \"virginica\"))\n  \n  # Continue processing safely after checks\n  summary_stats &lt;- summary(data)\n  return(summary_stats)\n}\n\n# Apply the function to the iris dataset\nprocess_iris_data(iris)\n\n\n\nImplementation of the checkmate package. Image by Author.\n\n\n\n\ndata.validator package\nDeveloped by Appsilon, the data.validator enables the creation of automated data quality reports. It facilitates the communication of data validation results to stakeholders, generating reports which you can export into HTML, CSV, and TXT formats. For example:\n# Load libraries\nlibrary(data.validator)\nlibrary(dplyr)\n# Initialize the report\nreport &lt;- data_validation_report()\n\n# Add validation steps\nvalidate(iris) %&gt;%\n  validate_cols(description = \"Sepal.Length is numeric\", predicate = is.numeric, cols = Sepal.Length) %&gt;%\n  validate_cols(description = \"Sepal.Width is numeric\", predicate = is.numeric, cols = Sepal.Width) %&gt;%\n  validate_cols(description = \"Petal.Length is numeric\", predicate = is.numeric, cols = Petal.Length) %&gt;%\n  validate_cols(description = \"Petal.Width is numeric\", predicate = is.numeric, cols = Petal.Width) %&gt;%\n  validate_cols(description = \"Species is a factor\", predicate = is.factor, cols = Species) %&gt;%\n  validate_if(description = \"No missing values in Sepal.Length\", Sepal.Length %&gt;% is.na() %&gt;% not()) %&gt;%\n  validate_if(description = \"Sepal.Length &gt; 0\", Sepal.Length &gt; 0) %&gt;%\n  validate_if(description = \"Sepal.Width &gt; 0\", Sepal.Width &gt; 0) %&gt;%\n  add_results(report)\n \n report\n\n\n\nImplementation of the data.validator package. Image by Author."
  },
  {
    "objectID": "posts/How R Helps You Catch Errors Before They Become Headlines/index.html#best-practices",
    "href": "posts/How R Helps You Catch Errors Before They Become Headlines/index.html#best-practices",
    "title": "How R Helps You Catch Errors Before They Become Headlines",
    "section": "Best Practices",
    "text": "Best Practices\nTo get the most out of R for data validation and error detection, follow these proven best practices:\n\nWrite Reproducible Code: Always aim for scripts that can be run from start to finish without manual intervention. Combine analysis and reporting in one reproducible document using R Markdown or Quarto.\nDocument Your Workflow: Use comments and meaningful variable names to make your code understandable to others and yourself in the future. Tools like roxygen2 can help document functions.\nAutomate Checks Early: Build validation rules into your workflow from the beginning, not as an afterthought. Use packages like validate, assertr, and checkmate to set up automated gates for bad data.\nTest Regularly: Use testthat to create unit tests for your functions and workflows. Testing small components ensures that failures are caught early.\nUse Version Control: Track your scripts and data validation logic using Git. Integrating Git with RStudio enables you to track changes, collaborate with teammates, and roll back changes if something goes wrong.\nLog and Monitor Data Quality: Set up periodic reports or dashboards that highlight rule violations using tools like data.validator.\nCreate Reusable Validation Templates: Develop standardized validation templates that can be easily adapted to new datasets, particularly for recurring tasks."
  },
  {
    "objectID": "posts/How R Helps You Catch Errors Before They Become Headlines/index.html#real-world-examples",
    "href": "posts/How R Helps You Catch Errors Before They Become Headlines/index.html#real-world-examples",
    "title": "How R Helps You Catch Errors Before They Become Headlines",
    "section": "Real-World Examples",
    "text": "Real-World Examples\nWhile high-profile failures, such as the JP Morgan Chase incident, remind us of what can go wrong without validation, many organizations quietly use R to prevent such problems. Here are a few real-world cases:\n\nPublic Health Surveillance: During the COVID-19 pandemic, epidemiologists widely used R to clean and validate daily case data. By integrating validation checks into their R scripts, public health teams quickly caught inconsistent age brackets, duplicate case IDs, and conflicting test result entries, avoiding flawed metrics in official reports.\nSilent Data Corruption in RNA-Seq Analysis: A computational biologist encountered silent data corruption during differential gene expression analysis. The issue stemmed from a buggy error message in a package, a malformed input file, and unexpected behavior in R‚Äôs conversion from factor to integer. Quality control functions within R helped identify and rectify these subtle errors.\nIndependent Quality Control in Clinical Studies: In a medical device study, R was used to perform independent quality control by generating reporting datasets and statistical outputs. Utilizing packages like tidyverse, admiral, and Tplyr, this approach provided an additional layer of assurance by cross-validating results obtained through different programming languages."
  },
  {
    "objectID": "posts/How R Helps You Catch Errors Before They Become Headlines/index.html#conclusion",
    "href": "posts/How R Helps You Catch Errors Before They Become Headlines/index.html#conclusion",
    "title": "How R Helps You Catch Errors Before They Become Headlines",
    "section": "Conclusion",
    "text": "Conclusion\nIn a data-driven world, accuracy isn‚Äôt just a technical requirement; it‚Äôs a strategic necessity. Errors in datasets can lead to flawed decisions, eroded trust, and costly mistakes. However, with the right tools and practices, many of these issues can be prevented.\nR offers a rich ecosystem for identifying, correcting, and preventing data errors through automation, reproducibility, and rigorous checks. From base functions like summary() and is.na() to powerful packages like assertr, checkmate, and data.validator, R gives analysts the tools they need to maintain data integrity at scale.\nIntegrating these practices into your workflow allows you to write better code, and helps your organization make smarter, safer decisions. In the end, good data hygiene isn‚Äôt just about avoiding errors; it‚Äôs about confidently enabling insight.\n\nNeed Help with Data? Let‚Äôs Make It Simple.\nAt LearnData.xyz, we‚Äôre here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we‚Äôve got your back.\nüìß Shoot us an email at admin@learndata.xyz‚Äîlet‚Äôs chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/How R Helps You Catch Errors Before They Become Headlines/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/How R Helps You Catch Errors Before They Become Headlines/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "How R Helps You Catch Errors Before They Become Headlines",
    "section": "Your next breakthrough could be one email away. Let‚Äôs make it happen!",
    "text": "Your next breakthrough could be one email away. Let‚Äôs make it happen!"
  },
  {
    "objectID": "posts/E2E/index.html",
    "href": "posts/E2E/index.html",
    "title": "End-to-end Testing with Playwright and Python Shiny",
    "section": "",
    "text": "When an application‚Äôs code base grows large, it becomes difficult to maintain. Anytime a change is made in the code, some app functionality can break and go unnoticed. This is why writing tests in your Python Shiny applications is essential.\nOne such test is end-to-end tests (E2E). End-to-end tests help mimic user interactions on an application, such as clicking buttons, file uploads, and browser variations, to ensure that the application‚Äôs user interface is working as expected.\nThis article will teach you how to conduct end-to-end tests on your Python Shiny applications using Playwright, an open-source automation framework for testing web applications."
  },
  {
    "objectID": "posts/E2E/index.html#what-is-playwright",
    "href": "posts/E2E/index.html#what-is-playwright",
    "title": "End-to-end Testing with Playwright and Python Shiny",
    "section": "What is Playwright",
    "text": "What is Playwright\nPlaywright is an automation framework used to test web applications across different browsers. It automates browser interactions with an application, just as a user would if they were to use the application. Playwright was built on Node.js but is also available as a Python library.\n\nBenefits of End-to-end testing with Playwright.\n\nWide Support and Compatibility: Playwright works with almost all operating systems and major browsers and has libraries in major programming languages.\nResilience and Dynamic Wait Times: Some elements or interactions can take time to load. Playwright features auto-wait for these elements or interactions, hence preventing unnecessary timeouts.\nTest Isolation: Playwright simulates a browser environment in a new tab. This allows you to run multiple tests on different sessions of the applications."
  },
  {
    "objectID": "posts/E2E/index.html#a-basic-python-shiny-example",
    "href": "posts/E2E/index.html#a-basic-python-shiny-example",
    "title": "End-to-end Testing with Playwright and Python Shiny",
    "section": "A Basic Python Shiny Example",
    "text": "A Basic Python Shiny Example\nLet‚Äôs build a simple web application using the affairs dataset from the statsmodels library. Ensure you have the following libraries installed.\n\nstatsmodels\npandas\nshiny\n\npip install statsmodels pandas shiny\nIn your project directory, create a new file called utilis.py. This file will contain the function that filters the affairs dataset based on an individual‚Äôs age and number of children.\nimport statsmodels.api as sm\n\n# Load the dataset from statsmodels (Affair dataset on extramarital affairs)\ndata = sm.datasets.fair.load_pandas().data\n\n# Function to filter data based on inputs\ndef filter_data(age_range, kids_range):\n    return data[(data[\"age\"] &gt;= age_range) & (data[\"children\"] &lt;= kids_range)]\nCreate another file, main.py, that will contain the code for the Python Shiny application.\nfrom shiny.express import input, render, ui\nfrom utilis import filter_data\n\n# Add UI elements\nwith ui.card():\n    ui.h2(\"Extramarital Affairs Dataset (Fair)\"),\n    ui.input_slider(\"age\", \"Age Filter\", 15, 60, 20)\n    ui.input_slider(\"kids\", \"Number of Kids\", 0, 5, 2)\n\n# Table output\nwith ui.card():\n\n    @render.table\n    def filtered_table():\n        filtered = filter_data(input.age(), input.kids())\n        return filtered.head(10)  # Display first 10 rows\nThe code above creates a shiny application that takes in two user inputs: age and kids. This now returns the first ten rows of the filtered dataset. Run the following code to view the application\nshiny run --reload main.py\n\n\n\nLive Python Shiny application. Image by Author"
  },
  {
    "objectID": "posts/E2E/index.html#integrating-playwright-with-python-shiny",
    "href": "posts/E2E/index.html#integrating-playwright-with-python-shiny",
    "title": "End-to-end Testing with Playwright and Python Shiny",
    "section": "Integrating Playwright with Python Shiny",
    "text": "Integrating Playwright with Python Shiny\nTo use Playwright, you must install the Playwright Python library and Pytest.\npip install pytest pytest-playwright\nJust like unit tests, all end-to-end test files must have a test prefix or suffix. Create a test file test_app.py, and paste the following code to see if the table output in the application appears as expected.\nfrom shiny.playwright import controller\nfrom shiny.run import ShinyAppProc\nfrom playwright.sync_api import Page\nfrom shiny.pytest import create_app_fixture\nfrom utilis import filter_data\n\napp = create_app_fixture(\"./main.py\")\n\ntest_data = filter_data(28, 4).head(10)\n\nn_row = test_data.shape[0]\nn_col = test_data.shape[1]\ncolumns_names = [*test_data.columns]\n\ndef test_table(page: Page, app: ShinyAppProc):\n    page.goto(app.url)\n    table = controller.OutputTable(page, \"filtered_table\")\n    slider_1 = controller.InputSlider(page, \"age\")\n    slider_2 = controller.InputSlider(page, \"kids\")\n    slider_1.set(\"28\")\n    slider_2.set(\"4\")\n    table.expect_column_labels(columns_names)\n    table.expect_ncol(n_col)\n    table.expect_nrow(n_row)\nHere is a breakdown of the above code:\n\nFirst of all, the controller module is imported. This controls the shiny components and mimics browser interactions on the Python shiny application.\npage is an instance of the Page class, representing a single tab on the browser.\napp is an instance of the ShinyAppProc class, representing the Python Shiny application.\nThe function test_table is designed to mimic user interaction with the inputs to generate an output table. It is always good practice to ensure that your tests cover a single functionality.\nThe controller.OutputTable and controller.InputSlider are methods from the controller module that mimic human interaction. Here is a list of all the available shiny controller classes and methods.\nThe .set method sets a value for each controller, just like a user will set it on the browser.\nThe .expect_column_labels, expect_ncol, and .expect_nrow methods are all specific to the controller, InputSlider. They check whether a table has the proper column labels and dimensions.\n\nType pytest on the command line, and click enter to run.\n\n\n\nTest result passed. Image by Author.\n\n\nYou can see that the test was successful. If the number of expected columns, rows, or label names provided does not match what the application is displaying, the test will fail and also give a reason for the failure.\n\n\n\nTest result fails. Image by Author."
  },
  {
    "objectID": "posts/E2E/index.html#end-to-end-testing-best-practices",
    "href": "posts/E2E/index.html#end-to-end-testing-best-practices",
    "title": "End-to-end Testing with Playwright and Python Shiny",
    "section": "End-to-end Testing Best Practices",
    "text": "End-to-end Testing Best Practices\n\nDefine Workflows to Test: When writing E2E tests, you need to define the parts of the application that you want to cover. It is unrealistic to say you want to write a test covering 100% of the application.\nSimulate Real-World User Experience: Ensure that any test you write simulates what users will realistically do, such as button clicks, file uploads, and so on.\nUse Descriptive Names: When creating test files and test functions, ensure you use descriptive names. This ensures anyone viewing the code understands what‚Äôs going on. Make sure the names of the test functions relate to the function of the application being tested.\nConduct Cross-browser Testing: Ensure you test all relevant browsers to ensure users can access your applications from various browsers.\nAutomate Tests: Ensure you integrate CI/CD workflows into your code, using tools like GitHub Actions, Jenkins, Circle CI, and others to run automated tests anytime a commit is made."
  },
  {
    "objectID": "posts/E2E/index.html#conclusion",
    "href": "posts/E2E/index.html#conclusion",
    "title": "End-to-end Testing with Playwright and Python Shiny",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, you have learned about E2E tests and how to write E2E tests with Playwright in your Python Shiny applications. E2E tests only covers user interactions, what if you want to test your business logic, such as calculations in your applications. This is where unit tests come into place, Check out How to Conduct Unit Tests in Python Shiny with Pytest to learn more.\nIf you want to learn more about Playwright, here are some resources that are of help.\n\nPlaywright Best Practices\nHow to perform End to End Testing using Playwright\nPlaywright End to End Testing: Complete Guide\nPlaywright: Fast and reliable end-to-end testing for modern web apps\n\n\nNeed Help with Data? Let‚Äôs Make It Simple.\nAt LearnData.xyz, we‚Äôre here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we‚Äôve got your back.\nüìß Shoot us an email at admin@learndata.xyz‚Äîlet‚Äôs chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/E2E/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/E2E/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "End-to-end Testing with Playwright and Python Shiny",
    "section": "Your next breakthrough could be one email away. Let‚Äôs make it happen!",
    "text": "Your next breakthrough could be one email away. Let‚Äôs make it happen!"
  },
  {
    "objectID": "posts/Building Your First ML Model API with FastAPI A Step-by-Step Guide/index.html",
    "href": "posts/Building Your First ML Model API with FastAPI A Step-by-Step Guide/index.html",
    "title": "Building Your First ML Model API with FastAPI: A Step-by-Step Guide",
    "section": "",
    "text": "If you have built AI applications, you might have used an API from OpenAI or Anthropic. These are models that are made easily accessible to developers through APIs. Have you ever wondered how these APIs were built? This article will teach you how to build your first machine learning model API using FastAPI.\nFastAPI is a Python library for building APIs, especially REST APIs. As a data scientist or machine learning engineer, you can make your machine learning model available to clients through an API. This API will fetch client input and return predictions, making it easy for anyone to use your model in their application."
  },
  {
    "objectID": "posts/Building Your First ML Model API with FastAPI A Step-by-Step Guide/index.html#prerequisites",
    "href": "posts/Building Your First ML Model API with FastAPI A Step-by-Step Guide/index.html#prerequisites",
    "title": "Building Your First ML Model API with FastAPI: A Step-by-Step Guide",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nPython 3.9+ installed\nIDE or code editor such as VS Code\nThe following libraries are are installed:\n\nfastapi to build the ML model API\npandas for data wrangling\nseaborn for the sample dataset\nscikit-learn for building the predictive model\njoblib for exporting the model as a joblib file\nuvicorn a lightweight server for running the ML API\npydantic for type annotations"
  },
  {
    "objectID": "posts/Building Your First ML Model API with FastAPI A Step-by-Step Guide/index.html#step-1---setup-working-directory",
    "href": "posts/Building Your First ML Model API with FastAPI A Step-by-Step Guide/index.html#step-1---setup-working-directory",
    "title": "Building Your First ML Model API with FastAPI: A Step-by-Step Guide",
    "section": "Step 1 - Setup Working Directory",
    "text": "Step 1 - Setup Working Directory\nCreate a project and give it an appropriate name. Inside the project directory, create the following files: app.py for the API code and utilis.py for the code to build the predictive model. Next, create and activate a virtual environment.\npython -m venv venv\nsource venv/bin/activate\nNext, install the following libraries.\npip install fastapi pandas seaborn scikit-learn joblib uvicorn pydantic"
  },
  {
    "objectID": "posts/Building Your First ML Model API with FastAPI A Step-by-Step Guide/index.html#step-2---preprare-the-dataset",
    "href": "posts/Building Your First ML Model API with FastAPI A Step-by-Step Guide/index.html#step-2---preprare-the-dataset",
    "title": "Building Your First ML Model API with FastAPI: A Step-by-Step Guide",
    "section": "Step 2 - Preprare the Dataset",
    "text": "Step 2 - Preprare the Dataset\nFor this article, you will build a machine learning model using the tips dataset in the seaborn library. Here are the variables in the tips dataset.\n\ntotal_bill: The total amount of the bill.\ntip: The tip amount given.\nsex: The gender of the payer.\nsmoker: Whether or not the payer smoked.\nday: The day of the week.\ntime: The time of day.\nsize: The size of the dining party.\n\nThe tips dataset has six features, and you will build a model predicting the tip amount given. Import the following libraries into the utilis.py file.\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport joblib\nCreate a function prepare_dataset inside utilis.py that splits the tips dataset into training and testing datasets and returns them.\ndef prepare_dataset():\n    \"\"\"Load and prepare the dataset for modeling\"\"\"\n    # Load the tips dataset from seaborn\n    tips = sns.load_dataset('tips')\n    \n    print(\"Dataset overview:\")\n    print(tips.head())\n    print(\"\\nDataset info:\")\n    print(tips.info())\n    \n    # Convert categorical features to dummy variables\n    tips_encoded = pd.get_dummies(tips, columns=['sex', 'smoker', 'day', 'time'])\n    \n    # Define features and target\n    X = tips_encoded.drop('tip', axis=1)\n    y = tips_encoded['tip']\n    \n    # Split the data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    return X_train, X_test, y_train, y_test, X.columns\nAs you may have noticed, we have one-hot encoded some of our features: sex, smoker, day, and time. This means that each value in these features is a variable, and the clients will send Boolean values to these new variables."
  },
  {
    "objectID": "posts/Building Your First ML Model API with FastAPI A Step-by-Step Guide/index.html#step-3---build-the-machine-learning-model",
    "href": "posts/Building Your First ML Model API with FastAPI A Step-by-Step Guide/index.html#step-3---build-the-machine-learning-model",
    "title": "Building Your First ML Model API with FastAPI: A Step-by-Step Guide",
    "section": "Step 3 - Build the Machine Learning Model",
    "text": "Step 3 - Build the Machine Learning Model\nCreate the function train_model inside utilis.py that builds a random forest regressor by taking the training features and outcome variable.\ndef train_model(X_train, y_train):\n    \"\"\"Train a RandomForestRegressor model\"\"\"\n    # Create a pipeline with preprocessing and model\n    pipeline = Pipeline([\n        ('scaler', StandardScaler()),\n        ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n    ])\n    \n    # Train the model\n    pipeline.fit(X_train, y_train)\n    \n    return pipeline\nCreate another function, evaluate_model, that returns the random forest model‚Äôs evaluation metrics, mse, and r2. This function takes the model, test features, and outcome as arguments.\ndef evaluate_model(model, X_test, y_test):\n    \"\"\"Evaluate the trained model\"\"\"\n    y_pred = model.predict(X_test)\n    \n    # Calculate metrics\n    mse = mean_squared_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n    \n    print(f\"Model Evaluation:\")\n    print(f\"Mean Squared Error: {mse:.4f}\")\n    print(f\"R¬≤ Score: {r2:.4f}\")\n    \n    return mse, r2\nNext, create the function that saves the model into your project directory. This is important if your model takes time to build. Instead of constantly re-running the model, you have all the model weights saved to a file. The joblib file stores all the model weights and makes it easy to make predictions for a new dataset.\ndef save_model(model, filename=\"tip_predictor_model.joblib\"):\n    \"\"\"Save the trained model to a file\"\"\"\n    joblib.dump(model, filename)\n    print(f\"Model saved as {filename}\")\nFinally, create the function prepare_and_train_model(). This function calls all the functions we created earlier and returns the model and feature_names. We also save the feature_names to a joblib file to validate incoming inputs from clients in the API.\ndef prepare_and_train_model():\n    X_train, X_test, y_train, y_test, feature_names = prepare_dataset()\n    model = train_model(X_train, y_train)\n    evaluate_model(model, X_test, y_test)\n    save_model(model)\n    \n    # Save feature names for input validation\n    joblib.dump(feature_names, \"feature_names.joblib\")\n    \n    return model, feature_names"
  },
  {
    "objectID": "posts/Building Your First ML Model API with FastAPI A Step-by-Step Guide/index.html#step-4---create-the-api-endpoints",
    "href": "posts/Building Your First ML Model API with FastAPI A Step-by-Step Guide/index.html#step-4---create-the-api-endpoints",
    "title": "Building Your First ML Model API with FastAPI: A Step-by-Step Guide",
    "section": "Step 4 - Create the API Endpoints",
    "text": "Step 4 - Create the API Endpoints\nGo to app.py and paste the following imports.\nimport pandas as pd\n\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel, Field\n\nimport joblib\nfrom utilis import prepare_and_train_model\nThe code above also imports the prepare_and_train_model from the utilis.py file. Next, create an instance of the FastAPI class.\napp = FastAPI(\n    title=\"Restaurant Tip Predictor API\",\n    description=\"API for predicting tips using the Seaborn tips dataset\",\n    version=\"1.0.0\"\n)\nThe FastAPI class provides the title, description, and version of the ML Model API. To validate client input, define a Pydantic model TipsPredictionRequest.\nclass TipPredictionRequest(BaseModel):\n    total_bill: float = Field(..., description=\"Total bill amount\", gt=0)\n    size: int = Field(..., description=\"Party size\", gt=0)\n    sex_Female: bool = Field(False, description=\"Customer is female\")\n    sex_Male: bool = Field(False, description=\"Customer is male\")\n    smoker_No: bool = Field(False, description=\"Non-smoker\")\n    smoker_Yes: bool = Field(False, description=\"Smoker\")\n    day_Fri: bool = Field(False, description=\"Friday\")\n    day_Sat: bool = Field(False, description=\"Saturday\")\n    day_Sun: bool = Field(False, description=\"Sunday\")\n    day_Thur: bool = Field(False, description=\"Thursday\")\n    time_Dinner: bool = Field(False, description=\"Dinner time\")\n    time_Lunch: bool = Field(False, description=\"Lunch time\")\nThe code above specifies several fields with types and validation rules, including total_bill (a required buoyant float representing the total bill amount) and size (a required positive integer representing the size of the dining party).\nThe model also includes a series of boolean fields, all of which default to False, that encode categorical features such as the customer‚Äôs sex (sex_Female, sex_Male), smoking status (smoker_No, smoker_Yes), day of the week (day_Fri, day_Sat, day_Sun, day_Thur), and meal time (time_Dinner, time_Lunch).\nCreate a Pydantic model TipPredictionResponse for the API response, also.\nclass TipPredictionResponse(BaseModel):\n    predicted_tip: float\nCreate a function that loads the ML model joblib file.\ndef load_model(model_path=\"tip_predictor_model.joblib\"):\n    \"\"\"Load the trained model from file\"\"\"\n    try:\n        model = joblib.load(model_path)\n        return model\n    except:\n        raise HTTPException(\n            status_code=500, \n            detail=\"Model not found. Please train the model first.\"\n        )\nThe load_model function loads the joblib file from the specified path. If an error occurs, an HTTPException is raised.\nPaste the following code also into the utilis.py file.\n\nmodel = None\nfeature_names = None\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    \"\"\"Load model on startup\"\"\"\n    global model, feature_names\n    try:\n        model = joblib.load(\"tip_predictor_model.joblib\")\n        feature_names = joblib.load(\"feature_names.joblib\")\n        print(\"Model loaded successfully\")\n    except Exception as e:\n        print(f\"Error loading model: {e}\")\n        print(\"Training new model...\")\n        model, feature_names = prepare_and_train_model()\nThe code above defines an asynchronous function, startup_event(), that runs when the application starts, using the @app.on_event(\"startup\") decorator.\nThe function aims to load the machine learning model tip_predictor_model.joblib and its associated feature names feature_names.joblib from disk into global variables model and feature_names.\nIf loading the model files fails, the function catches the exception, logs an error message, and calls prepare_and_train_model() to train a new model and set the global variables accordingly. This ensures the app always has a ready-to-use model, either loaded or freshly trained, upon startup.\nCopy and paste the following code also.\n@app.get(\"/\")\ndef read_root():\n    \"\"\"Root endpoint\"\"\"\n    return {\"message\": \"Welcome to the Restaurant Tip Predictor API\"}\nThe code above defines a simple HTTP GET endpoint at the application‚Äôs root URL (\"/\"), using the @app.get(\"/\") decorator.\nThe function read_root() is called whenever someone accesses the root path. It returns a JSON response containing a message: {\"message\": \"Welcome to the Restaurant Tip Predictor API\"}.\nThis serves as a basic welcome or health check endpoint to confirm that the API is running and reachable. Next, create the model endpoint as follows.\n@app.post(\"/predict\", response_model=TipPredictionResponse)\ndef predict_tip(request: TipPredictionRequest):\n    \"\"\"Predict tip amount based on input features\"\"\"\n    # Convert input data to DataFrame with correct columns\n    input_dict = request.model_dump()\n    input_df = pd.DataFrame([input_dict])\n    \n    # Reorder columns to match the model's expected feature names\n    try:\n        input_df = input_df[feature_names]\n    except KeyError as e:\n        raise HTTPException(\n            status_code=400,\n            detail=f\"Input data is missing required features: {e}\"\n        )\n    \n    # Make prediction\n    predicted_tip = model.predict(input_df)[0]\n    \n    return TipPredictionResponse(\n        predicted_tip=round(float(predicted_tip), 2)\n    )\nThis code defines a POST endpoint at \"/predict\" in the application, using the @app.post decorator with a specified response_model of TipPredictionResponse.\nThe predict_tip function accepts a request payload of type TipPredictionRequest, which contains the input features needed for the prediction. It converts the request data into a dictionary and then a Pandas DataFrame.\nThe columns of the DataFrame are reordered to match the expected order defined by the global feature_names. If any required feature is missing, it raises a 400 Bad Request error using an HTTPException.\nOnce the input is validated and properly formatted, the function uses the pre-loaded machine learning model to predict the tip amount, rounds the prediction to two decimal places, and returns it as a TipPredictionResponse object.\nTo be able to run the model API, copy and paste the following code at the end of your app.py file.\nif __name__ == \"__main__\":\n    try:\n        model = joblib.load(\"tip_predictor_model.joblib\")\n        feature_names = joblib.load(\"feature_names.joblib\")\n    except:\n        print(\"No model found. Training new model...\")\n        model, feature_names = prepare_and_train_model()\n        \n    # Run the API server\n    uvicorn.run(\"app:app\", host=\"0.0.0.0\", port=8000, reload=True)\nThe code above is the application‚Äôs entry point. It runs only when the script is executed directly, not when imported as a module.\nIt first tries to load a pre-trained model and its feature names using joblib. If the files are not found or loading fails, they don‚Äôt exist; it catches the exception and trains a new model by calling prepare_and_train_model().\nAfter that, it starts a FastAPI server using uvicorn, binding it to all network interfaces 0.0.0.0 on port 8000 with auto-reload enabled. This is helpful for development as it restarts the server when code changes."
  },
  {
    "objectID": "posts/Building Your First ML Model API with FastAPI A Step-by-Step Guide/index.html#step-5---test-the-api-endpoints",
    "href": "posts/Building Your First ML Model API with FastAPI A Step-by-Step Guide/index.html#step-5---test-the-api-endpoints",
    "title": "Building Your First ML Model API with FastAPI: A Step-by-Step Guide",
    "section": "Step 5 - Test the API endpoints",
    "text": "Step 5 - Test the API endpoints\nTo test the API, call the following shell command on your terminal.\npython -m app\nYour application will automatically open up at the URL https://0.0.0.0:8000\n\n\n\nAPI application root url. Image by Author\n\n\nTo view the automatically generated interactive API documentation, add /docs to the end of the API URL https://0.0.0.0:8000/docs .\n\n\n\nAPI application documentation page. Image by Author.\n\n\nYou will notice that we have two API endpoints. The first one is the root URL that opens when the API launches, while the second is the prediction endpoint.\nClick on the dropdown of the prediction endpoint, and give the various appropriate values in the JSON fields.\n\n\n\nInput feature values to get a prediction. Image by Author.\n\n\nClick the Execute button to get the predicted tip value.\n\n\n\nExecute the API endpoint. Image by Author."
  },
  {
    "objectID": "posts/Building Your First ML Model API with FastAPI A Step-by-Step Guide/index.html#conclusion",
    "href": "posts/Building Your First ML Model API with FastAPI A Step-by-Step Guide/index.html#conclusion",
    "title": "Building Your First ML Model API with FastAPI: A Step-by-Step Guide",
    "section": "Conclusion",
    "text": "Conclusion\nBuilding an API shows how you have made your machine learning model interactive. You can test your API, integrate it with various frontend applications, or deploy it on various cloud platforms. I discussed this more in my course ML Model Deployment with FastAPI and Streamlit.\n\nNeed Help with Data? Let‚Äôs Make It Simple.\nAt LearnData.xyz, we‚Äôre here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we‚Äôve got your back.\nüìß Shoot us an email at admin@learndata.xyz‚Äîlet‚Äôs chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/Building Your First ML Model API with FastAPI A Step-by-Step Guide/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/Building Your First ML Model API with FastAPI A Step-by-Step Guide/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "Building Your First ML Model API with FastAPI: A Step-by-Step Guide",
    "section": "Your next breakthrough could be one email away. Let‚Äôs make it happen!",
    "text": "Your next breakthrough could be one email away. Let‚Äôs make it happen!"
  },
  {
    "objectID": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html",
    "href": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html",
    "title": "A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn",
    "section": "",
    "text": "Working with numerical data helps us understand the distribution of values in a numerical variable. This gives us a sense of the frequently occurring values and how these values vary from each other. It also highlights the presence of extreme values in a variable, if any.\nIn this article, you will learn how to plot a histogram using Seaborn, a Python library built on Matplotlib for statistical data visualization. You will also learn how to customize and interpret your histogram plots to derive precise insights from your data."
  },
  {
    "objectID": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#what-is-a-histogram",
    "href": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#what-is-a-histogram",
    "title": "A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn",
    "section": "What is a Histogram?",
    "text": "What is a Histogram?\nA histogram is used to plot numeric data. It splits a numeric variable into various equal ranges known as bins and plots the total number of observations that fall under each bin as bars. These bars are adjacent to each other, with no space between them. This differs from a bar plot that plots the frequency of categorical data, with space between the bars.\n\n\n\nDifference between histogram and bar chart. Source: Syncfusion"
  },
  {
    "objectID": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#building-a-histogram-with-seaborn",
    "href": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#building-a-histogram-with-seaborn",
    "title": "A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn",
    "section": "Building a Histogram with Seaborn",
    "text": "Building a Histogram with Seaborn\nWe will build our histogram using the tips dataset in the seaborn library. This dataset contains information about tips received by a waiter over a period of months. Before proceeding, ensure you have imported the pandas, Seaborn, and Matplotib libraries.\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nLet‚Äôs view the columns in the dataset.\ntips = sns.load_dataset('tips')\ntips.head()\n\n\n\nPreview of the tips dataset. Image by Author.\n\n\nWe will use the histogram to answer questions from our dataset regarding the tip variable, such as:\n\nWhat is the distribution of the tip?\nWhat is the highest and lowest tip amount received?\nWhat is the tip amount most frequently received by the waiter?\nIs there a difference between the distribution of tips from female and male customers?\n\n\nCreating the histogram\nTo build a seaborn histogram, you need to use the sns.histplot() function. The following are the key arguments to provide:\n\ndata: the pandas dataframe.\nx: the variable of interest in the data.\ncolor: the color of the bars.\nalpha: the transparency of the bars.\nbins: the number of bins in the histogram.\nbinwidth: the width of each bin.\nkde: A boolean to add a kernel density estimation\nhue: to differentiate data subset based on another variable\n\nUse the following code to build a simple histogram for us to start with, adding title and axis labels to the plot.\nsns.histplot(data=tips, x='tip')\nplt.title(\"Distribution of tips received by the waiter\")\nplt.xlabel(\"Tip amount\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\nA simple histogram that shows the distribution of tips given to the waiter. Image by Author."
  },
  {
    "objectID": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#enhancing-the-histogram",
    "href": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#enhancing-the-histogram",
    "title": "A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn",
    "section": "Enhancing the histogram",
    "text": "Enhancing the histogram\nYou can improve the histogram by providing some of the arguments explained in the previous section.\n\nAdjusting bin sizes and width\nCurrently, the default number of bins is set to auto based on the number of observations, and the bandwidth is set to None by default.\n\nNote\nUsing a bin width overrides the bins argument.\n\nLet‚Äôs increase the number of bins by giving a higher value, say 100.\nsns.histplot(data=tips, x='tip', bins=100)\nAlternatively, you can set the binwidth to 0.1, you will still get the same plot.\nsns.histplot(data=tips, x='tip', binwidth=0.1)\n\n\n\nAdjust the bin size or bandwidth to spread or thin the distribution. Image by Author.\n\n\n\n\nChanging the bin color\nTo set the bin‚Äôs color, add a value to the color argument. Let‚Äôs give the histogram the color green.\nsns.histplot(data=tips, x='tip', color=\"green\")\nplt.title(\"Distribution of tips received by the waiter\")\nplt.xlabel(\"Tip amount\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.show()\n\n\n\nAdd a color to the histogram. Image by Author.\n\n\n\n\nAdd a Kernel Density Estimation (KDE)\nTo ease interpretation, we can set the kde argument to True. This will apply a kernel density estimation that smooths the histogram points, revealing the shape of the distribution.\nsns.histplot(data=tips, x='tip', color=\"green\", kde=True)\nplt.title(\"Distribution of tips received by the waiter\")\nplt.xlabel(\"Tip amount\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.show()\n\n\n\nSet kde to true. Image by Author\n\n\n\n\nAdd grouping variables\nYou can add a grouping variable to the histogram to see the distribution for each respective group. For example, let‚Äôs group the histogram by sex by adding a sex group to the hue argument to see the distribution of tips given by male and female customers.\n\nNote\nThis is going to override the color argument you specified earlier.\n\nsns.histplot(data=tips, x='tip', color=\"green\", kde=True, hue=\"sex\")\nplt.title(\"Distribution of tips received by the waiter\")\nplt.xlabel(\"Tip amount\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.show()\n\n\n\nAdd a group variable to the histogram. Image by Author."
  },
  {
    "objectID": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#interpreting-histograms",
    "href": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#interpreting-histograms",
    "title": "A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn",
    "section": "Interpreting Histograms",
    "text": "Interpreting Histograms\nInterpreting a histogram is not difficult, especially once you use the kernel density estimation on the plot. The highest bars are the frequently occurring values and the highest points on the kernel density plot are the highest bars. We can deduce from our histogram that the most frequent amount of tips received by the waiter ranges from $2 to $3.\n\n\n\nIdentifying frequently occurring values through the plot peak. Image by Author.\n\n\nThe tail of the kernel density plot indicates extreme values. The longer the tail, the more extreme values present in the data are. This tail is referred to as a skew. A long tail to the left means the distribution is left-skewed. A long tail to the right means the histogram distribution is right-skewed, just like in the case of our plot. This means the waiter received tips above $6 from a few customers.\n\n\n\nIdentifying the skewness of the histogram plot. Image by Author.\n\n\nThe difference between the two kernel density plots also tells us that male customers give higher tips to the waiter than female customers, though the number of male customers is greater than that of female customers. The minimum tip the waiter receives is $1, while the highest is $10.\n\n\n\nDifference between the two KDE plots. Image by Author."
  },
  {
    "objectID": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#best-practices-when-creating-histograms",
    "href": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#best-practices-when-creating-histograms",
    "title": "A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn",
    "section": "Best Practices when Creating Histograms",
    "text": "Best Practices when Creating Histograms\nA histogram is a powerful plot that can tell you much about your numeric variables. Here are some best practices to ensure you craft accurate and precise histograms.\n\nChoose an Appropriate Bin Size: When choosing a bin size, try various values and ensure that the selected bin size conveys as much information as possible.\nLabel Axes: Ensure you provide informative labels when labeling your axes to help your viewers understand the plot.\nUse Contrasting Colors: When comparing groups, use contrasting colors to make it easy to identify group differences.\nUse Legends: When working with group variables, it is advisable to have a legend to identify the group and their respective colors in the plot.\nUse Consistent Bin Widths: When setting bin width size, ensure your bin width is consistent throughout to avoid distorting the information conveyed by the plot."
  },
  {
    "objectID": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#conclusion",
    "href": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#conclusion",
    "title": "A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn",
    "section": "Conclusion",
    "text": "Conclusion\nWhen conducting predictive analysis on a dataset with numerical variables, it is crucial to view the distribution of these variables to identify outliers. This is where a histogram comes into play. You can use a histogram to see how the values in a variable are dispersed from each other, making the histogram a very important plot for exploratory data analysis.\nIn this article, you have learned about histograms and how to build and interpret them using the Seaborn library in Python. If you want to learn more about histograms, here are some valuable resources.\nHow to Read Histograms: 9 Steps (with Pictures)\nInterpreting Histograms\nEverything about Density Plot\nShapes of Distributions: Definitions, Examples\nMeasures of shape\n\nNeed Help with Data? Let‚Äôs Make It Simple.\nAt LearnData.xyz, we‚Äôre here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we‚Äôve got your back.\nüìß Shoot us an email at admin@learndata.xyz‚Äîlet‚Äôs chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn",
    "section": "Your next breakthrough could be one email away. Let‚Äôs make it happen!",
    "text": "Your next breakthrough could be one email away. Let‚Äôs make it happen!\n\nNeed Help with Data? Let‚Äôs Make It Simple.\nAt LearnData.xyz, we‚Äôre here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we‚Äôve got your back.\nüìß Shoot us an email at admin@learndata.xyz‚Äîlet‚Äôs chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen-1",
    "href": "posts/A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen-1",
    "title": "A Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn",
    "section": "Your next breakthrough could be one email away. Let‚Äôs make it happen!",
    "text": "Your next breakthrough could be one email away. Let‚Äôs make it happen!"
  },
  {
    "objectID": "posts/20 Python Frameworks for Frontend Development Java/index.html",
    "href": "posts/20 Python Frameworks for Frontend Development Java/index.html",
    "title": "20 Python Frameworks for Frontend Development: JavaScript Alternatives (2025)",
    "section": "",
    "text": "JavaScript has monopolized the field of front web development and has become what one will call the official language of the web. This is because it was built for that purpose.\nBut not everyone knows JavaScript. Does this mean that you can‚Äôt build a fully functional site? No.\nAs someone who started with Python programming, I wanted to build applications that users could interact with and see the visualization I have created. Thanks to these frameworks, I was able to achieve this.\nIn this article, you will learn the frameworks to use when you want to build a cool application by writing Python code.\nThis article is for Python developers who are not open to the steep learning curve of front-end development."
  },
  {
    "objectID": "posts/20 Python Frameworks for Frontend Development Java/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/20 Python Frameworks for Frontend Development Java/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "20 Python Frameworks for Frontend Development: JavaScript Alternatives (2025)",
    "section": "Your next breakthrough could be one email away. Let‚Äôs make it happen!",
    "text": "Your next breakthrough could be one email away. Let‚Äôs make it happen!"
  },
  {
    "objectID": "navs/services.html",
    "href": "navs/services.html",
    "title": "Our Services",
    "section": "",
    "text": "At LearnData, we offer a comprehensive suite of data-driven solutions to help businesses and individuals harness the power of their data. Our expertise in R, Python, and statistical analysis, combined with our proficiency in Shiny web applications, allows us to deliver tailored services that meet your unique needs.\n\n\n\nAdvanced statistical analysis using R and Python\nCustom data visualization and dashboards\nExploratory data analysis (EDA) to uncover insights\nTime series analysis and forecasting\n\n\n\n\n\nDevelopment of predictive models and algorithms\nImplementation of supervised and unsupervised learning techniques\nModel evaluation, validation, and optimization\nNatural Language Processing (NLP) solutions\n\n\n\n\n\nCreation of interactive, data-driven web applications\nCustom dashboard design and implementation\nReal-time data processing and visualization\nIntegration of statistical models into user-friendly interfaces\n\n\n\n\n\nExpert advice on experimental design and sampling methods\nHypothesis testing and statistical inference\nPower analysis and sample size determination\nInterpretation and reporting of statistical results\n\n\n\n\n\nCustomized R and Python programming workshops\nData analysis and visualization best practices\nIntroduction to machine learning and predictive modeling\nShiny app development training\n\n\n\n\n\nAssessment and improvement of existing R and Python code\nPerformance optimization for data processing pipelines\nBest practices implementation for maintainable code\nVersion control and collaborative development guidance\n\nWhether you‚Äôre looking to gain insights from your data, build predictive models, or create interactive web applications, our team at LearnData is here to support your data science journey. Contact us today to discuss how we can help you transform your data into actionable intelligence."
  },
  {
    "objectID": "navs/services.html#data-analysis-and-visualization",
    "href": "navs/services.html#data-analysis-and-visualization",
    "title": "Our Services",
    "section": "",
    "text": "Advanced statistical analysis using R and Python\nCustom data visualization and dashboards\nExploratory data analysis (EDA) to uncover insights\nTime series analysis and forecasting"
  },
  {
    "objectID": "navs/services.html#machine-learning-and-predictive-modeling",
    "href": "navs/services.html#machine-learning-and-predictive-modeling",
    "title": "Our Services",
    "section": "",
    "text": "Development of predictive models and algorithms\nImplementation of supervised and unsupervised learning techniques\nModel evaluation, validation, and optimization\nNatural Language Processing (NLP) solutions"
  },
  {
    "objectID": "navs/services.html#shiny-web-application-development",
    "href": "navs/services.html#shiny-web-application-development",
    "title": "Our Services",
    "section": "",
    "text": "Creation of interactive, data-driven web applications\nCustom dashboard design and implementation\nReal-time data processing and visualization\nIntegration of statistical models into user-friendly interfaces"
  },
  {
    "objectID": "navs/services.html#statistical-consulting",
    "href": "navs/services.html#statistical-consulting",
    "title": "Our Services",
    "section": "",
    "text": "Expert advice on experimental design and sampling methods\nHypothesis testing and statistical inference\nPower analysis and sample size determination\nInterpretation and reporting of statistical results"
  },
  {
    "objectID": "navs/services.html#training-and-workshops",
    "href": "navs/services.html#training-and-workshops",
    "title": "Our Services",
    "section": "",
    "text": "Customized R and Python programming workshops\nData analysis and visualization best practices\nIntroduction to machine learning and predictive modeling\nShiny app development training"
  },
  {
    "objectID": "navs/services.html#code-review-and-optimization",
    "href": "navs/services.html#code-review-and-optimization",
    "title": "Our Services",
    "section": "",
    "text": "Assessment and improvement of existing R and Python code\nPerformance optimization for data processing pipelines\nBest practices implementation for maintainable code\nVersion control and collaborative development guidance\n\nWhether you‚Äôre looking to gain insights from your data, build predictive models, or create interactive web applications, our team at LearnData is here to support your data science journey. Contact us today to discuss how we can help you transform your data into actionable intelligence."
  },
  {
    "objectID": "navs/privacy.html",
    "href": "navs/privacy.html",
    "title": "Privacy Policy",
    "section": "",
    "text": "Last updated: 2nd October, 2024\n\n\nWelcome to LearnData. We respect your privacy and are committed to protecting your personal data. This privacy policy will inform you about how we look after your personal data when you visit our website and tell you about your privacy rights and how the law protects you.\n\n\n\n\n\nThis privacy policy aims to give you information on how LearnData collects and processes your personal data through your use of this website, including any data you may provide through this website when you sign up for our newsletter, purchase a product or service, or take part in a survey.\n\n\n\nLearnData is the controller and responsible for your personal data (collectively referred to as ‚ÄúLearnData‚Äù, ‚Äúwe‚Äù, ‚Äúus‚Äù or ‚Äúour‚Äù in this privacy policy).\n\n\n\n\nWe may collect, use, store and transfer different kinds of personal data about you which we have grouped together as follows:\n\nIdentity Data\nContact Data\nTechnical Data\nUsage Data\nMarketing and Communications Data\n\n\n\n\nWe will only use your personal data when the law allows us to. Most commonly, we will use your personal data in the following circumstances:\n\nWhere we need to perform the contract we are about to enter into or have entered into with you.\nWhere it is necessary for our legitimate interests and your interests and fundamental rights do not override those interests.\nWhere we need to comply with a legal obligation.\n\n\n\n\nWe have put in place appropriate security measures to prevent your personal data from being accidentally lost, used or accessed in an unauthorized way, altered or disclosed.\n\n\n\nWe will only retain your personal data for as long as reasonably necessary to fulfil the purposes we collected it for, including for the purposes of satisfying any legal, regulatory, tax, accounting or reporting requirements.\n\n\n\nUnder certain circumstances, you have rights under data protection laws in relation to your personal data, including the right to:\n\nRequest access to your personal data\nRequest correction of your personal data\nRequest erasure of your personal data\nObject to processing of your personal data\nRequest restriction of processing your personal data\nRequest transfer of your personal data\nRight to withdraw consent\n\n\n\n\nWe may update our privacy policy from time to time. We will notify you of any changes by posting the new privacy policy on this page.\n\n\n\nIf you have any questions about this privacy policy or our privacy practices, please contact us at:\nEmail: admin@learndata.xyz"
  },
  {
    "objectID": "navs/privacy.html#introduction",
    "href": "navs/privacy.html#introduction",
    "title": "Privacy Policy",
    "section": "",
    "text": "Welcome to LearnData. We respect your privacy and are committed to protecting your personal data. This privacy policy will inform you about how we look after your personal data when you visit our website and tell you about your privacy rights and how the law protects you."
  },
  {
    "objectID": "navs/privacy.html#important-information-and-who-we-are",
    "href": "navs/privacy.html#important-information-and-who-we-are",
    "title": "Privacy Policy",
    "section": "",
    "text": "This privacy policy aims to give you information on how LearnData collects and processes your personal data through your use of this website, including any data you may provide through this website when you sign up for our newsletter, purchase a product or service, or take part in a survey.\n\n\n\nLearnData is the controller and responsible for your personal data (collectively referred to as ‚ÄúLearnData‚Äù, ‚Äúwe‚Äù, ‚Äúus‚Äù or ‚Äúour‚Äù in this privacy policy)."
  },
  {
    "objectID": "navs/privacy.html#the-data-we-collect-about-you",
    "href": "navs/privacy.html#the-data-we-collect-about-you",
    "title": "Privacy Policy",
    "section": "",
    "text": "We may collect, use, store and transfer different kinds of personal data about you which we have grouped together as follows:\n\nIdentity Data\nContact Data\nTechnical Data\nUsage Data\nMarketing and Communications Data"
  },
  {
    "objectID": "navs/privacy.html#how-we-use-your-personal-data",
    "href": "navs/privacy.html#how-we-use-your-personal-data",
    "title": "Privacy Policy",
    "section": "",
    "text": "We will only use your personal data when the law allows us to. Most commonly, we will use your personal data in the following circumstances:\n\nWhere we need to perform the contract we are about to enter into or have entered into with you.\nWhere it is necessary for our legitimate interests and your interests and fundamental rights do not override those interests.\nWhere we need to comply with a legal obligation."
  },
  {
    "objectID": "navs/privacy.html#data-security",
    "href": "navs/privacy.html#data-security",
    "title": "Privacy Policy",
    "section": "",
    "text": "We have put in place appropriate security measures to prevent your personal data from being accidentally lost, used or accessed in an unauthorized way, altered or disclosed."
  },
  {
    "objectID": "navs/privacy.html#data-retention",
    "href": "navs/privacy.html#data-retention",
    "title": "Privacy Policy",
    "section": "",
    "text": "We will only retain your personal data for as long as reasonably necessary to fulfil the purposes we collected it for, including for the purposes of satisfying any legal, regulatory, tax, accounting or reporting requirements."
  },
  {
    "objectID": "navs/privacy.html#your-legal-rights",
    "href": "navs/privacy.html#your-legal-rights",
    "title": "Privacy Policy",
    "section": "",
    "text": "Under certain circumstances, you have rights under data protection laws in relation to your personal data, including the right to:\n\nRequest access to your personal data\nRequest correction of your personal data\nRequest erasure of your personal data\nObject to processing of your personal data\nRequest restriction of processing your personal data\nRequest transfer of your personal data\nRight to withdraw consent"
  },
  {
    "objectID": "navs/privacy.html#changes-to-the-privacy-policy",
    "href": "navs/privacy.html#changes-to-the-privacy-policy",
    "title": "Privacy Policy",
    "section": "",
    "text": "We may update our privacy policy from time to time. We will notify you of any changes by posting the new privacy policy on this page."
  },
  {
    "objectID": "navs/privacy.html#contact-us",
    "href": "navs/privacy.html#contact-us",
    "title": "Privacy Policy",
    "section": "",
    "text": "If you have any questions about this privacy policy or our privacy practices, please contact us at:\nEmail: admin@learndata.xyz"
  },
  {
    "objectID": "navs/courses.html#ml-model-deployment-with-fastapi-and-streamlit",
    "href": "navs/courses.html#ml-model-deployment-with-fastapi-and-streamlit",
    "title": "Courses",
    "section": "ML Model Deployment with FastAPI and Streamlit",
    "text": "ML Model Deployment with FastAPI and Streamlit"
  },
  {
    "objectID": "navs/courses.html#advanced-data-wrangling-with-pandas",
    "href": "navs/courses.html#advanced-data-wrangling-with-pandas",
    "title": "Courses",
    "section": "Advanced Data Wrangling with Pandas",
    "text": "Advanced Data Wrangling with Pandas"
  },
  {
    "objectID": "navs/courses.html#building-interactive-shiny-web-apps-with-r-programming",
    "href": "navs/courses.html#building-interactive-shiny-web-apps-with-r-programming",
    "title": "Courses",
    "section": "Building Interactive Shiny Web Apps with R Programming",
    "text": "Building Interactive Shiny Web Apps with R Programming"
  },
  {
    "objectID": "navs/courses.html#data-wrangling-and-exploratory-data-analysis-with-r",
    "href": "navs/courses.html#data-wrangling-and-exploratory-data-analysis-with-r",
    "title": "Courses",
    "section": "Data Wrangling and Exploratory Data Analysis with R",
    "text": "Data Wrangling and Exploratory Data Analysis with R"
  },
  {
    "objectID": "navs/courses.html#power-bi-dax-practice-test-and-solutions",
    "href": "navs/courses.html#power-bi-dax-practice-test-and-solutions",
    "title": "Courses",
    "section": "Power BI DAX Practice Test and Solutions",
    "text": "Power BI DAX Practice Test and Solutions"
  },
  {
    "objectID": "navs/chaos.html",
    "href": "navs/chaos.html",
    "title": "Chaos",
    "section": "",
    "text": "Real-world data is often messy and complex, quickly becoming overwhelming for individuals seeking to improve their data cleaning and management skills.\nAccessing authentic, messy datasets can be challenging, as most datasets available on platforms like Kaggle and other repositories are pre-cleaned, making them far removed from the realities of working with raw data.\nTo address this gap, we developed Chaos‚Äîa web application designed to generate messy datasets from clean data. Inspired by Nicola Rennie‚Äôs brilliant work in the messy R package, this tool is ideal for data scientists, educators, and developers who want to stress-test their data pipelines or teach data cleaning in a controlled environment."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to LearnData",
    "section": "",
    "text": "Are you ready to transform your data into actionable insights? At LearnData, we specialize in turning complex data into clear, powerful solutions.\n\n\n\nR & Python Consulting: Harness the full potential of these leading data science languages.\nStatistical Analysis: Make informed decisions with robust statistical methodologies.\nShiny Web Applications: Create interactive, data-driven web apps that captivate and inform.\n\n\n\n\n\nExpert Team: Our statisticians and data scientists bring years of experience to every project.\nTailored Solutions: We craft custom solutions that address your unique challenges.\nCutting-Edge Techniques: Stay ahead with the latest in machine learning and data visualization.\nEducation-Focused: We don‚Äôt just deliver results; we empower your team with knowledge.\n\n\n\n\n\nData Analysis and Visualization\nMachine Learning and Predictive Modeling\nShiny Web Application Development\nStatistical Consulting\nTraining and Workshops\nCode Review and Optimization\n\n\n\n\nContact Us Now to discuss how we can elevate your data game.\nExplore Our Services to learn more about our offerings."
  },
  {
    "objectID": "index.html#unlock-the-power-of-your-data",
    "href": "index.html#unlock-the-power-of-your-data",
    "title": "Welcome to LearnData",
    "section": "",
    "text": "Are you ready to transform your data into actionable insights? At LearnData, we specialize in turning complex data into clear, powerful solutions.\n\n\n\nR & Python Consulting: Harness the full potential of these leading data science languages.\nStatistical Analysis: Make informed decisions with robust statistical methodologies.\nShiny Web Applications: Create interactive, data-driven web apps that captivate and inform.\n\n\n\n\n\nExpert Team: Our statisticians and data scientists bring years of experience to every project.\nTailored Solutions: We craft custom solutions that address your unique challenges.\nCutting-Edge Techniques: Stay ahead with the latest in machine learning and data visualization.\nEducation-Focused: We don‚Äôt just deliver results; we empower your team with knowledge.\n\n\n\n\n\nData Analysis and Visualization\nMachine Learning and Predictive Modeling\nShiny Web Application Development\nStatistical Consulting\nTraining and Workshops\nCode Review and Optimization\n\n\n\n\nContact Us Now to discuss how we can elevate your data game.\nExplore Our Services to learn more about our offerings."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n10 AI Data Analysis Tools That Will Redefine How You Make Decisions\n\n\n\nai\n\nartificial intelligence\n\ndata analysis\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\n20 Python Frameworks for Frontend Development: JavaScript Alternatives (2025)\n\n\n\npython\n\ndata applications\n\nweb development\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\n9 Python Libraries for Managing Missing Data Efficiently\n\n\n\npython\n\ndata science\n\nmachine learning\n\ndata wrangling\n\ndata cleaning\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Comprehensive Guide to Plotting and Interpreting Histogram with Python Seaborn\n\n\n\npython\n\nseaborn\n\nhistogram\n\ndensity plot\n\nkde\n\ndistribution\n\nstatistics\n\ndata visualization\n\ndata science\n\nmachine learning\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdding User Authentication to Python Shiny Applications using JWT\n\n\n\npython\n\nshiny\n\nauthentication\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Your First ML Model API with FastAPI: A Step-by-Step Guide\n\n\n\npython\n\nmachine learning\n\ndata science\n\napi\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nCI/CD with Python Shiny and GitHub Actions\n\n\n\ngithub actions\n\npytest\n\nunit testing\n\nautomation\n\nCI/CD\n\npython\n\npython shiny\n\npython testing frameworks\n\nprogramming\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnd-to-end Testing with Playwright and Python Shiny\n\n\n\nE2E\n\nend-to-end testing\n\npython\n\npython shiny\n\npython testing frameworks\n\nprogramming\n\nplaywright\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nForecasting Time Series Data With Facebook Prophet in R\n\n\n\nr\n\nstatistics\n\nmachine learning\n\ndata science\n\ndata visualization\n\ntime series\n\nforecasting\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow R Helps You Catch Errors Before They Become Headlines\n\n\n\nR\n\ndata cleaning\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Create and Read a Forest Plot in R\n\n\n\ndata visualization\n\nR\n\ndata analysis\n\ndata storytelling\n\nplot\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Dockerize a Python Shiny Application\n\n\n\npython\n\nshiny\n\ndocker\n\napp development\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Forecast Your YouTube Channel Views for the Next 30 Days in Python\n\n\n\npython\n\nyoutube analytics\n\ntime series analysis\n\nforecasting\n\ndata visualization\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Plot a Time Series Plot with Python Plotnine\n\n\n\npython\n\ndata visualization\n\nplotnine\n\ntime series\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Conduct Unit Tests in Python Shiny with Pytest\n\n\n\npython unit testing\n\npytest\n\npython shiny\n\npython testing frameworks\n\nprogramming\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Kaplan-Meier Survival Analysis Estimation with Python\n\n\n\npython\n\nstatistics\n\ndata visualization\n\ndata science\n\nmachine learning\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Build a Language Translator Application with Strapi, Streamlit, and Hugging Face Models\n\n\n\napi\n\ndata science\n\nmachine learning\n\nprogramming\n\npython\n\nstreamlit\n\nstrapi\n\nheadless cms\n\nhugging face\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression with Python Statsmodels: Assumptions and Interpretation\n\n\n\npython\n\nstatsmodels\n\nstatistics\n\nmachine learning\n\ndata science\n\ndata visualization\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Deployment in R with Plumber\n\n\n\nmachine learning\n\nR\n\nAPI\n\nmodel deployment\n\ndata science\n\nmlops\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonitoring Model Performance and Data Drift for Diabetes Classification\n\n\n\nmlops\n\ndata drift\n\nclassification\n\ntutorial\n\npython\n\ndata science\n\nmachine learning\n\nhealthcare\n\ndeployment\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen Not To Use Parametric or Non-Parametric Statistics\n\n\n\nstatistics\n\ndata analysis\n\ndata science\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\nRecent Posts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecuring ML APIs with FastAPI\n\n\n\napi\n\ndata science\n\nmachine learning\n\nprogramming\n\nsecurity\n\nfastapi\n\npython\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest Driven Development with Python Shiny\n\n\n\npython\n\nshiny\n\nweb development\n\ndata visualization\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Engineer‚Äôs Guide to Low Code/No Code ELT Tools\n\n\n\ndata engineering\n\nlow code\n\nguide\n\nno code\n\nELT\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Hidden Cost of Missing Data in Machine Learning Models\n\n\n\nstatistics\n\ndata science\n\nmachine learning\n\ndata cleaning\n\n\n\nDiscover the hidden costs of missing data in machine learning models and learn effective strategies to mitigate its impact on model performance and bias.\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nTypes of Missing Data: MCAR, MAR, and MNAR Explained\n\n\n\nstatistics\n\nmachine learning\n\ndata science\n\ndata cleaning\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Traditional Web Development Skills Matter for Modern ML Engineers\n\n\n\nweb development\n\nmachine learning\n\nmlops\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nZ-Score vs IQR vs DBSCAN: Choosing the Right Outlier Detection Method\n\n\n\nstatistics\n\nmachine learning\n\ndata cleaning\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearn how to create, customize, and interpret Gantt charts in Power BI to track project timelines and tasks. Follow our detailed instructions and look at real examples.\n\n\n\nterminal\n\nproductivity\n\n\n\nLearn to manage dotfiles for a consistent configuration across systems and for an improved workflow. Ensure compatibility with Unix-based systems.\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nPower BI Gantt Chart: A Complete How-To\n\n\n\nbusiness intelligence\n\npower bi\n\ndata visualization\n\ndata analysis\n\ndata storytelling\n\n\n\nLearn how to create, customize, and interpret Gantt charts in Power BI to track project timelines and tasks. Follow our detailed instructions and look at real examples.\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to GitHub Codespaces\n\n\n\nversion control\n\ngithub\n\nproductivity\n\n\n\nDiscover GitHub Codespaces, the development environment that allows you to write, run, and deploy your code anywhere.\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Serverless Computing?\n\n\n\ndevops\n\nmlops\n\ncloud computings\n\n\n\nLearn about serverless computing, a cloud model where cloud providers manage infrastructure and allow software developers to ship applications quickly.\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Kafka Certification Guide for Data Professionals\n\n\n\ncertification\n\nkafka\n\n\n\nLearn how to advance your career with the Confluent Certified Developer (CCDAK) and Administrator (CCAAK) certifications, gaining the expertise and recognition needed to‚Ä¶\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nPandas Iterate Over Rows: Handle Row-by-Row Operations\n\n\n\npython\n\npandas\n\ndata analysis\n\ndata wrangling\n\n\n\nLearn the various methods of iterating over rows in Pandas DataFrame, exploring best practices, performance considerations, and everyday use cases.\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nMastering COUNTIF() in Power BI: A DAX-Based Approach\n\n\n\npower bi\n\ndata visualization\n\ndata storytelling\n\nbusiness intelligence\n\n\n\nPower BI users from Excel always find out that Power BI does not have a native COUNTIF() function. This article will show various ways to implement COUNTIF() in Power BI and‚Ä¶\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nPower BI Tooltips: Enhance Reports with Interactive Insights\n\n\n\npower bi\n\ndata visualization\n\ndata storytelling\n\nbusiness intelligence\n\n\n\nLearn how to create and customize Power BI tooltips to reveal deeper insights and improve the report‚Äôs user experience.\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Paginated Reports in Power BI: A Step-by-Step Guide\n\n\n\npower bi\n\ndata visualization\n\ndata storytelling\n\nbusiness intelligence\n\n\n\nLearn how to create pixel-perfect paginated reports in Power BI, from setup to design and publishing, using Power BI Report Builder.\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Create a Power BI Waterfall Chart: 5 Easy Steps\n\n\n\npower bi\n\ndata visualization\n\ndata storytelling\n\nbusiness intelligence\n\n\n\nLearn how to create a Power BI waterfall chart, including how to customize the chart to make it look nice. Follow best practices and avoid common mistakes.\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython Dependency Injection: Build Modular and Testable Code\n\n\n\npython\n\napi\n\ntesting\n\n\n\nLearn how to make your code modular, testable, and maintainable by understanding and implementing various Python dependency injection frameworks.\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Extract YouTube Analytics Data and Analyze in Python\n\n\n\npython\n\ndata analysis\n\nyoutube analytics\n\npandas\n\n\n\nLearn how to extract and analyze your YouTube data in Python.\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Create a DeepSeek R1 API in R with Plumber\n\n\n\nR\n\nplumber\n\ndeepseek\n\nai\n\n\n\nLearn how to use create your own AI chatbot in R using DeepSeek R1 API.\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Build a WhatsApp Dictionary Chatbot using Twilio, FastAPI, and MongoDB\n\n\n\napi development\n\nfastapi\n\n\n\nLearn how to use create a whatsapp dictionary chatbot using Twilio, FastAPI, and MongoDB.\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Forecast Time Series Data with Python Darts\n\n\n\nmachine learning\n\ndata science\n\nstatistics\n\npython\n\ntime series\n\n\n\nLearn how to forecast time series data using darts.\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "courses.html",
    "href": "courses.html",
    "title": "Courses",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nBuilding User Interfaces with HTML, CSS and JS\n\n\n\nR\n\nShiny\n\napp development\n\n\n\nThis course walks you through on how to use web development technologies such as JS, CSS and HTML to build Shiny applications.\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Wrangling and Exploratory Data Analysis with R\n\n\n\nR\n\ndata visualization\n\ndata storytelling\n\n\n\nThis course walks you through cleaning and exploring data using various libraries in R such as dplyr, tidyr, and so on.\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Publication Ready Tables with R\n\n\n\nR\n\ndata visualization\n\ndata storytelling\n\n\n\nThis course walks you through on how you can create various complex tables, and beautify your tables using various libraries in R.\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nML Model Deployment with FastAPI and Streamlit.\n\n\n\npython\n\nmlops\n\nmachine learning\n\ndata science\n\napp development\n\n\n\nThis course walks you through building and deploying real ML applications using the tools you already know: Python, FastAPI, and Streamlit. So, you can stop waiting for‚Ä¶\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractical Biostatistics with R\n\n\n\nstatistics\n\nR\n\ndata analysis\n\n\n\nThis course walks you through the applications of biostatistics using the R software.\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Interactive Shiny Web Apps with R Programming\n\n\n\nR\n\nShiny\n\napp development\n\n\n\nThis course walks you how to build web applications using the R Shiny framework.\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\nHandling Missing Data in R\n\n\n\nR\n\ndata cleaning\n\ndata wrangling\n\ndata visualization\n\n\n\nThis course walks you how to recognise and handle various types of missing data in R.\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Courses"
    ]
  },
  {
    "objectID": "navs/about.html",
    "href": "navs/about.html",
    "title": "About",
    "section": "",
    "text": "LearnData is a cutting-edge data science consulting firm specializing in R and Python programming, statistical analysis, and Shiny web application development. Founded by a team of passionate statisticians and data scientists, we bridge the gap between complex data and actionable insights."
  },
  {
    "objectID": "navs/about.html#who-we-are",
    "href": "navs/about.html#who-we-are",
    "title": "About",
    "section": "",
    "text": "LearnData is a cutting-edge data science consulting firm specializing in R and Python programming, statistical analysis, and Shiny web application development. Founded by a team of passionate statisticians and data scientists, we bridge the gap between complex data and actionable insights."
  },
  {
    "objectID": "navs/about.html#about-founder",
    "href": "navs/about.html#about-founder",
    "title": "About",
    "section": "About Founder",
    "text": "About Founder\nRidwan Suleiman Adejumo is a skilled Data Scientist, Technical Writer, and Social Media Marketing Analytics specialist. He is the Founder of LearnData, a consulting firm that empowers businesses and individuals with data-driven insights, training, and tailored analytics solutions.\nWith deep expertise in data analysis, machine learning, and digital strategy, Ridwan helps organizations turn complex data into actionable insights that drive growth and performance, especially in the evolving landscape of social media and digital marketing.\nAs a Udemy Instructor, Ridwan has taught thousands of students around the world, creating accessible, practical courses in data science and analytics. His work as a Technical Writer showcases his ability to simplify complex concepts and make them clear, engaging, and useful to both technical and non-technical audiences.\nRidwan is passionate about transforming raw data into meaningful impact and continues to work at the intersection of analytics, education, and digital innovation."
  },
  {
    "objectID": "navs/about.html#our-mission",
    "href": "navs/about.html#our-mission",
    "title": "About",
    "section": "Our Mission",
    "text": "Our Mission\nOur mission is to empower businesses and individuals to harness the full potential of their data. We believe that data, when properly analyzed and visualized, can drive innovation, inform decision-making, and unlock new opportunities across all sectors."
  },
  {
    "objectID": "navs/about.html#our-expertise",
    "href": "navs/about.html#our-expertise",
    "title": "About",
    "section": "Our Expertise",
    "text": "Our Expertise\nAt the heart of LearnData is our deep expertise in:\n\nR and Python Programming: We leverage the power of these leading data science languages to deliver robust, efficient, and scalable solutions.\nStatistical Analysis: Our team of statisticians brings rigorous methodologies to every project, ensuring reliable and meaningful results.\nShiny Web Applications: We specialize in creating interactive, data-driven web applications that make complex data accessible and actionable.\nMachine Learning: From predictive modeling to natural language processing, we implement cutting-edge machine learning techniques to solve real-world problems."
  },
  {
    "objectID": "navs/about.html#our-approach",
    "href": "navs/about.html#our-approach",
    "title": "About",
    "section": "Our Approach",
    "text": "Our Approach\nWe believe in a collaborative, client-centered approach. Every project begins with a thorough understanding of your unique needs and goals. We then apply our technical expertise and industry knowledge to deliver tailored solutions that drive real value for your organization."
  },
  {
    "objectID": "navs/about.html#why-choose-us",
    "href": "navs/about.html#why-choose-us",
    "title": "About",
    "section": "Why Choose Us?",
    "text": "Why Choose Us?\n\nExpertise: Our team combines academic rigor with practical industry experience.\nInnovation: We stay at the forefront of data science developments, constantly exploring new techniques and technologies.\nCustomization: We don‚Äôt believe in one-size-fits-all solutions. Every project is tailored to meet your specific needs.\nEducation: We‚Äôre committed to not just delivering results, but also to empowering our clients with knowledge and skills.\nResults-Driven: Our focus is always on delivering actionable insights and measurable outcomes.\n\nAt LearnData, we‚Äôre more than just consultants ‚Äì we‚Äôre your partners in navigating the complex world of data. Whether you‚Äôre looking to optimize your processes, predict market trends, or transform your data into compelling visualizations, we‚Äôre here to guide you every step of the way.\nReady to unlock the power of your data? Contact us today to start your data-driven journey."
  },
  {
    "objectID": "navs/correlateai.html",
    "href": "navs/correlateai.html",
    "title": "CorrelateAI",
    "section": "",
    "text": "At CorrelateAI, we believe that data should work for you‚Äînot the other way around. That‚Äôs why we created CorrelateAI, a cutting-edge SaaS platform that seamlessly connects to various data sources, including CSV files, databases, and Google Sheets. Powered by AI, our platform transforms raw data into actionable insights, empowering businesses to make smarter, data-driven decisions with ease."
  },
  {
    "objectID": "navs/home.html",
    "href": "navs/home.html",
    "title": "Welcome to LearnData",
    "section": "",
    "text": "Are you ready to transform your data into actionable insights? At LearnData, we specialize in turning complex data into clear, powerful solutions.\n\n\n\nR & Python Consulting: Harness the full potential of these leading data science languages.\nStatistical Analysis: Make informed decisions with robust statistical methodologies.\nShiny Web Applications: Create interactive, data-driven web apps that captivate and inform.\n\n\n\n\n\nExpert Team: Our statisticians and data scientists bring years of experience to every project.\nTailored Solutions: We craft custom solutions that address your unique challenges.\nCutting-Edge Techniques: Stay ahead with the latest in machine learning and data visualization.\nEducation-Focused: We don‚Äôt just deliver results; we empower your team with knowledge.\n\n\n\n\n\nData Analysis and Visualization\nMachine Learning and Predictive Modeling\nShiny Web Application Development\nStatistical Consulting\nTraining and Workshops\nCode Review and Optimization\n\n\n\n\nContact Us Now to discuss how we can elevate your data game.\nExplore Our Services to learn more about our offerings."
  },
  {
    "objectID": "navs/home.html#unlock-the-power-of-your-data",
    "href": "navs/home.html#unlock-the-power-of-your-data",
    "title": "Welcome to LearnData",
    "section": "",
    "text": "Are you ready to transform your data into actionable insights? At LearnData, we specialize in turning complex data into clear, powerful solutions.\n\n\n\nR & Python Consulting: Harness the full potential of these leading data science languages.\nStatistical Analysis: Make informed decisions with robust statistical methodologies.\nShiny Web Applications: Create interactive, data-driven web apps that captivate and inform.\n\n\n\n\n\nExpert Team: Our statisticians and data scientists bring years of experience to every project.\nTailored Solutions: We craft custom solutions that address your unique challenges.\nCutting-Edge Techniques: Stay ahead with the latest in machine learning and data visualization.\nEducation-Focused: We don‚Äôt just deliver results; we empower your team with knowledge.\n\n\n\n\n\nData Analysis and Visualization\nMachine Learning and Predictive Modeling\nShiny Web Application Development\nStatistical Consulting\nTraining and Workshops\nCode Review and Optimization\n\n\n\n\nContact Us Now to discuss how we can elevate your data game.\nExplore Our Services to learn more about our offerings."
  },
  {
    "objectID": "navs/tos.html",
    "href": "navs/tos.html",
    "title": "Terms of Service",
    "section": "",
    "text": "Last updated: 2nd October, 2024\nPlease read these Terms of Service (‚ÄúTerms‚Äù, ‚ÄúTerms of Service‚Äù) carefully before using the https://learndata.xyz website (the ‚ÄúService‚Äù) operated by LearnData (‚Äúus‚Äù, ‚Äúwe‚Äù, or ‚Äúour‚Äù).\nYour access to and use of the Service is conditioned on your acceptance of and compliance with these Terms. These Terms apply to all visitors, users and others who access or use the Service.\nBy accessing or using the Service you agree to be bound by these Terms. If you disagree with any part of the terms then you may not access the Service.\n\n\nLearnData provides data science consulting services, including but not limited to data analysis, statistical modeling, machine learning, and Shiny web application development. The specific services to be provided will be agreed upon in separate contracts or statements of work.\n\n\n\nWhen you create an account with us, you must provide us information that is accurate, complete, and current at all times. Failure to do so constitutes a breach of the Terms, which may result in immediate termination of your account on our Service.\nYou are responsible for safeguarding the password that you use to access the Service and for any activities or actions under your password, whether your password is with our Service or a third-party service.\n\n\n\nThe Service and its original content, features and functionality are and will remain the exclusive property of LearnData and its licensors. The Service is protected by copyright, trademark, and other laws of both the United States and foreign countries. Our trademarks and trade dress may not be used in connection with any product or service without the prior written consent of LearnData.\n\n\n\nOur Service may contain links to third-party web sites or services that are not owned or controlled by LearnData.\nLearnData has no control over, and assumes no responsibility for, the content, privacy policies, or practices of any third party web sites or services. You further acknowledge and agree that LearnData shall not be responsible or liable, directly or indirectly, for any damage or loss caused or alleged to be caused by or in connection with use of or reliance on any such content, goods or services available on or through any such web sites or services.\n\n\n\nWe may terminate or suspend access to our Service immediately, without prior notice or liability, for any reason whatsoever, including without limitation if you breach the Terms.\nAll provisions of the Terms which by their nature should survive termination shall survive termination, including, without limitation, ownership provisions, warranty disclaimers, indemnity and limitations of liability.\n\n\n\nYour use of the Service is at your sole risk. The Service is provided on an ‚ÄúAS IS‚Äù and ‚ÄúAS AVAILABLE‚Äù basis. The Service is provided without warranties of any kind, whether express or implied, including, but not limited to, implied warranties of merchantability, fitness for a particular purpose, non-infringement or course of performance.\n\n\n\nThese Terms shall be governed and construed in accordance with the laws of [Your State/Country], without regard to its conflict of law provisions.\nOur failure to enforce any right or provision of these Terms will not be considered a waiver of those rights. If any provision of these Terms is held to be invalid or unenforceable by a court, the remaining provisions of these Terms will remain in effect.\n\n\n\nWe reserve the right, at our sole discretion, to modify or replace these Terms at any time. If a revision is material we will try to provide at least 30 days notice prior to any new terms taking effect. What constitutes a material change will be determined at our sole discretion.\n\n\n\nIf you have any questions about these Terms, please contact us at:\nEmail: admin@learndata.xyz"
  },
  {
    "objectID": "navs/tos.html#services",
    "href": "navs/tos.html#services",
    "title": "Terms of Service",
    "section": "",
    "text": "LearnData provides data science consulting services, including but not limited to data analysis, statistical modeling, machine learning, and Shiny web application development. The specific services to be provided will be agreed upon in separate contracts or statements of work."
  },
  {
    "objectID": "navs/tos.html#user-accounts",
    "href": "navs/tos.html#user-accounts",
    "title": "Terms of Service",
    "section": "",
    "text": "When you create an account with us, you must provide us information that is accurate, complete, and current at all times. Failure to do so constitutes a breach of the Terms, which may result in immediate termination of your account on our Service.\nYou are responsible for safeguarding the password that you use to access the Service and for any activities or actions under your password, whether your password is with our Service or a third-party service."
  },
  {
    "objectID": "navs/tos.html#intellectual-property",
    "href": "navs/tos.html#intellectual-property",
    "title": "Terms of Service",
    "section": "",
    "text": "The Service and its original content, features and functionality are and will remain the exclusive property of LearnData and its licensors. The Service is protected by copyright, trademark, and other laws of both the United States and foreign countries. Our trademarks and trade dress may not be used in connection with any product or service without the prior written consent of LearnData."
  },
  {
    "objectID": "navs/tos.html#links-to-other-web-sites",
    "href": "navs/tos.html#links-to-other-web-sites",
    "title": "Terms of Service",
    "section": "",
    "text": "Our Service may contain links to third-party web sites or services that are not owned or controlled by LearnData.\nLearnData has no control over, and assumes no responsibility for, the content, privacy policies, or practices of any third party web sites or services. You further acknowledge and agree that LearnData shall not be responsible or liable, directly or indirectly, for any damage or loss caused or alleged to be caused by or in connection with use of or reliance on any such content, goods or services available on or through any such web sites or services."
  },
  {
    "objectID": "navs/tos.html#termination",
    "href": "navs/tos.html#termination",
    "title": "Terms of Service",
    "section": "",
    "text": "We may terminate or suspend access to our Service immediately, without prior notice or liability, for any reason whatsoever, including without limitation if you breach the Terms.\nAll provisions of the Terms which by their nature should survive termination shall survive termination, including, without limitation, ownership provisions, warranty disclaimers, indemnity and limitations of liability."
  },
  {
    "objectID": "navs/tos.html#disclaimer",
    "href": "navs/tos.html#disclaimer",
    "title": "Terms of Service",
    "section": "",
    "text": "Your use of the Service is at your sole risk. The Service is provided on an ‚ÄúAS IS‚Äù and ‚ÄúAS AVAILABLE‚Äù basis. The Service is provided without warranties of any kind, whether express or implied, including, but not limited to, implied warranties of merchantability, fitness for a particular purpose, non-infringement or course of performance."
  },
  {
    "objectID": "navs/tos.html#governing-law",
    "href": "navs/tos.html#governing-law",
    "title": "Terms of Service",
    "section": "",
    "text": "These Terms shall be governed and construed in accordance with the laws of [Your State/Country], without regard to its conflict of law provisions.\nOur failure to enforce any right or provision of these Terms will not be considered a waiver of those rights. If any provision of these Terms is held to be invalid or unenforceable by a court, the remaining provisions of these Terms will remain in effect."
  },
  {
    "objectID": "navs/tos.html#changes",
    "href": "navs/tos.html#changes",
    "title": "Terms of Service",
    "section": "",
    "text": "We reserve the right, at our sole discretion, to modify or replace these Terms at any time. If a revision is material we will try to provide at least 30 days notice prior to any new terms taking effect. What constitutes a material change will be determined at our sole discretion."
  },
  {
    "objectID": "navs/tos.html#contact-us",
    "href": "navs/tos.html#contact-us",
    "title": "Terms of Service",
    "section": "",
    "text": "If you have any questions about these Terms, please contact us at:\nEmail: admin@learndata.xyz"
  },
  {
    "objectID": "posts/9 Python Libraries for Managing Missing Data Efficiently/index.html",
    "href": "posts/9 Python Libraries for Managing Missing Data Efficiently/index.html",
    "title": "9 Python Libraries for Managing Missing Data Efficiently",
    "section": "",
    "text": "Data cleaning is one of the most tedious tasks of a data scientist before building any model, especially when it comes to imputing missing data. Most real-world data have missing observations, which can arise due to various factors.\nIt‚Äôs your role as a data scientist to find how to solve the missing data problem in your dataset, because building a model without missing data imputation can lead to wrong predictions.\nThere are various ways to impute missing data, from simple techniques such as mean/median imputation to advanced techniques like multiple imputation or KNN imputation. In this article, you will learn about the various libraries you can use for missing data imputation in Python"
  },
  {
    "objectID": "posts/9 Python Libraries for Managing Missing Data Efficiently/index.html#key-criteria-for-selection",
    "href": "posts/9 Python Libraries for Managing Missing Data Efficiently/index.html#key-criteria-for-selection",
    "title": "9 Python Libraries for Managing Missing Data Efficiently",
    "section": "Key Criteria for Selection",
    "text": "Key Criteria for Selection\nWhen selecting a library for missing data imputation, ensure you look at the following criteria.\n\nEase of use and documentation quality: Ensure the library is not complex, it‚Äôs easy to use with well-written documentation. The library explains all the imputation algorithms used, and also the functions for implementing them. This makes it easy to know the right scenario to use a particular imputation method.\nSupport for different imputation methods: Some data might warrant simpler methods, while others will need complex imputation methods. You should use libraries that have a wide range of support for various simple and advanced methods. This keeps your workflow lean and simple, and avoids using multiple libraries.\nIntegration with popular data science workflows: Some popular Python libraries already support various imputation methods, hence avoiding the need for an external library. Some missing data imputation libraries also make it easy to integrate well with classes and functions from other Python libraries, making it easy to add to your workflow.\nCommunity support and performance: You want to make sure you are using a library with a lot of help and support online, and it‚Äôs also actively maintained. Whenever you get stuck, you can easily reach out for help."
  },
  {
    "objectID": "posts/9 Python Libraries for Managing Missing Data Efficiently/index.html#python-libraries-for-missing-data-imputation",
    "href": "posts/9 Python Libraries for Managing Missing Data Efficiently/index.html#python-libraries-for-missing-data-imputation",
    "title": "9 Python Libraries for Managing Missing Data Efficiently",
    "section": "Python Libraries for Missing Data Imputation",
    "text": "Python Libraries for Missing Data Imputation\nLet‚Äôs look at some of the libraries you can use for missing data imputation.\n\n1. Pandas\nPandas is one of the most popular libraries among data professionals working with Python. Designed for data wrangling, it comes with various imputation methods, mostly for simple use cases. It‚Äôs easy to use, fast, and well-integrated for exploratory data analysis.\nHere are some of its built-in methods for missing data imputation:\n\nfillna(): replaces missing values in a dataframe either by the mean, median, or a custom value.\ninterpolate(): fills missing values by interpolating between existing data points, essentially estimating values between known ones.\ndropna(): removes rows or columns that contain missing (NaN) values.\n\nExamples\nHere are some examples demonstrating the above functions.\n\nfillna() :\ndf = pd.DataFrame({\n    'A': [1, 2, None, 4],\n    'B': [None, 2, 3, None]\n})\n\nprint(\"Original DataFrame:\")\nprint(df)\n\n# Replace NaN with 0\ndf_filled = df.fillna(0)\nprint(\"\\nAfter fillna(0):\")\nprint(df_filled)\n\n# Forward fill (propagate last valid value forward)\ndf_ffill = df.fillna(method='ffill')\nprint(\"\\nAfter forward fill:\")\nprint(df_ffill)\nOriginal DataFrame:\n     A    B\n0  1.0  NaN\n1  2.0  2.0\n2  NaN  3.0\n3  4.0  NaN\n\nAfter fillna(0):\n     A    B\n0  1.0  0.0\n1  2.0  2.0\n2  0.0  3.0\n3  4.0  0.0\n\nAfter forward fill:\n     A    B\n0  1.0  NaN\n1  2.0  2.0\n2  2.0  3.0\n3  4.0  3.0\nIn the first missing value imputation, the missing values are filled with 0 , while in the second missing value imputation, they are filled with the value of the observation in the previouis row.\ninterpolate():\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'A': [1, None, 3, None, 5]\n})\n\nprint(\"Original DataFrame:\")\nprint(df)\n\n# Linear interpolation\ndf_interp = df.interpolate()\nprint(\"\\nAfter interpolation:\")\nprint(df_interp)\nOriginal DataFrame:\n     A\n0  1.0\n1  NaN\n2  3.0\n3  NaN\n4  5.0\n\nAfter interpolation:\n     A\n0  1.0\n1  2.0\n2  3.0\n3  4.0\n4  5.0\nThere .interpolate() method supports various types of interpolation, but the example above used a linear interpolation whereby the missing values are filled with the assumption that they change at a constant rate between two points, either an increase by 1,2, and so on.\ndropna():\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'A': [1, None, 3, 4],\n    'B': [None, 2, None, 4]\n})\n\nprint(\"Original DataFrame:\")\nprint(df)\n\n# Drop rows with any NaN values\ndf_drop_any = df.dropna()\nprint(\"\\nAfter dropna() (any NaN):\")\nprint(df_drop_any)\n\n# Drop columns with any NaN values\ndf_drop_col = df.dropna(axis=1)\nprint(\"\\nAfter dropna(axis=1):\")\nprint(df_drop_col)\nOriginal DataFrame:\n     A    B\n0  1.0  NaN\n1  NaN  2.0\n2  3.0  NaN\n3  4.0  4.0\n\nAfter dropna() (any NaN):\n     A    B\n3  4.0  4.0\n\nAfter dropna(axis=1):\nEmpty DataFrame\nColumns: []\nIndex: [0, 1, 2, 3]\nJust as it‚Äôs shown in the example above, you can apply dropna() , either row or column-wise, to drop missing observations from your data.\n\n\n\n\n2. Scikit-learn\nScikit-learn, which is popular for machine learning, also has various classes for handling missing data imputations. This makes it easy to implement missing data imputation in both the train and test data when building your model workflow. Some of the missing imputation classes are:\n\nSimpleImputer: for filling missing values using a simple rule like mean, median, most frequent value, and so on.\nKNNImputer: fills missing values using the values of nearest neighbors.\nIterativeImputer: fills missing values by modeling each feature with missing values as a function of the other features.\n\nExamples\n\nSimpleImputer:\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\nimport pandas as pd\n\n# Original data with missing values\ndata = np.array([[1, 2],\n                 [np.nan, 3],\n                 [7, np.nan]])\n\nprint(\"Original data:\")\nprint(pd.DataFrame(data, columns=[\"A\", \"B\"]))\n\n# Imputer replaces missing values with column mean\nimputer = SimpleImputer(strategy='mean')\nimputed_data = imputer.fit_transform(data)\n\nprint(\"\\nImputed data (SimpleImputer - mean):\")\nprint(pd.DataFrame(imputed_data, columns=[\"A\", \"B\"]))\nOriginal data:\n     A    B\n0  1.0  2.0\n1  NaN  3.0\n2  7.0  NaN\n\nImputed data (SimpleImputer - mean):\n     A    B\n0  1.0  2.0\n1  4.0  3.0\n2  7.0  2.5\nThe above example shows mean imputation, which is mostly used for numerical variables, while the mode is used for categorical variables.\nKNNImputer:\nfrom sklearn.impute import KNNImputer\nimport numpy as np\nimport pandas as pd\n\ndata = np.array([[1, 2],\n                 [np.nan, 3],\n                 [7, 6]])\n\nprint(\"Original data:\")\nprint(pd.DataFrame(data, columns=[\"A\", \"B\"]))\n\nimputer = KNNImputer(n_neighbors=2)\nimputed_data = imputer.fit_transform(data)\n\nprint(\"\\nImputed data (KNNImputer):\")\nprint(pd.DataFrame(imputed_data, columns=[\"A\", \"B\"]))\nOriginal data:\n     A    B\n0  1.0  2.0\n1  NaN  3.0\n2  7.0  6.0\n\nImputed data (KNNImputer):\n     A    B\n0  1.0  2.0\n1  4.0  3.0\n2  7.0  6.0\nIn the code above, the n_neighbors argument in the KNNImputer() class takes the number of nearest neighbors needed to run the missing data imputation.\nIterativeImputer:\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nimport numpy as np\nimport pandas as pd\n\ndata = np.array([[1, 2],\n                 [np.nan, 3],\n                 [7, np.nan]])\n\nprint(\"Original data:\")\nprint(pd.DataFrame(data, columns=[\"A\", \"B\"]))\n\nimputer = IterativeImputer(max_iter=10, random_state=0)\nimputed_data = imputer.fit_transform(data)\n\nprint(\"\\nImputed data (IterativeImputer):\")\nprint(pd.DataFrame(imputed_data, columns=[\"A\", \"B\"]))\nOriginal data:\n     A    B\n0  1.0  2.0\n1  NaN  3.0\n2  7.0  NaN\n\nImputed data (IterativeImputer):\n           A    B\n0   1.000000  2.0\n1  12.999998  3.0\n2   7.000000  2.5\n\n\n\n\nimage.png\n\n\n\n\n3. Fancyimpute\nFancyimpute is a standalone library for missing-data imputation that includes various algorithms. It also includes popular algorithms, which are already implemented in scikit-learn such as SimpleFill, KNN and IterativeImputer. Other complex algorithms in it are:\n\nSoftImpute: Which implements the Spectral Regularization Algorithms for Learning Large Incomplete Matrices for missing data imputation.\nIterativeSVD: Which implements the iterative low-rank SVD decomposition for imputing missing data.\nMatrixFactorization: This uses direct factorization to fill missing data.\nBiScaler: This uses iterative estimation of row/column means and standard deviations to get a doubly normalized matrix.\n\nExample\nHere is a simple example implementing the SoftImpute() class on missing data.\nimport numpy as np\nimport pandas as pd\nfrom fancyimpute import SoftImpute\n\n# Sample data with missing values\ndata = np.array([\n    [5, 3, np.nan, 1],\n    [4, np.nan, np.nan, 1],\n    [1, 1, np.nan, 5],\n    [1, np.nan, np.nan, 4],\n    [np.nan, 1, 5, 4]\n])\n\nprint(\"Original data:\")\nprint(pd.DataFrame(data, columns=[\"Item1\", \"Item2\", \"Item3\", \"Item4\"]))\n\n# SoftImpute: matrix factorization-based imputation\nsoft_imputer = SoftImpute()\ndata_imputed = soft_imputer.fit_transform(data)\n\nprint(\"\\nImputed data (SoftImpute):\")\nprint(pd.DataFrame(data_imputed, columns=[\"Item1\", \"Item2\", \"Item3\", \"Item4\"]))\nOriginal data:\n   Item1  Item2  Item3  Item4\n0    5.0    3.0    NaN    1.0\n1    4.0    NaN    NaN    1.0\n2    1.0    1.0    NaN    5.0\n3    1.0    NaN    NaN    4.0\n4    NaN    1.0    5.0    4.0\n\nImputed data (SoftImpute):\n      Item1     Item2     Item3  Item4\n0  5.000000  3.000000  0.270492    1.0\n1  4.000000  2.270316  0.120304    1.0\n2  1.000000  1.000000  2.475254    5.0\n3  1.000000  0.896283  1.941855    4.0\n4  0.455978  1.000000  5.000000    4.0\n\n\n\nimage.png\n\n\n\n\n4. Missingno\nMissingno is a visualization library for missing observations that allows you to study the pattern of missing observations in your dataset. Through this visualization, you can understand why and how the observations in your data are missing. It‚Äôs easy to use and works seamlessly with Pandas. Missingno also supports visualizations such as matrix plot, bar plot, heatmap, and dendogram.\nExample\nimport pandas as pd\nimport numpy as np\nimport missingno as msno\nimport matplotlib.pyplot as plt\n\n# Sample dataset with missing values\ndata = pd.DataFrame({\n    'A': [1, 2, np.nan, 4, 5],\n    'B': [np.nan, 2, 3, np.nan, 5],\n    'C': [1, np.nan, np.nan, 4, 5],\n    'D': [1, 2, 3, 4, 5]\n})\n\nprint(\"Original Data:\")\nprint(data)\n\n# Matrix plot\nplt.figure(figsize=(6,3))\nmsno.matrix(data)\nplt.show()\n\n\n\nMissing data visualization with missingno. Image by Author.\n\n\nThe above example is a matrix plot of missing observations from the dataset. The matrix plot is like a visual display of the data, where the rows are the observations, while the columns are the variables. Shaded regions represent an observation with complete data, while the white regions represent missing data. The curve on the right shows the distribution of the rows with complete and incomplete data.\n\n\n5. MICEforest\nMICEforest is also a Python library for missing data imputation that uses Multiple Imputation by Chained Equations(MICE) through an iterative series of predictive models, based on other variables in the dataset. This method of imputation is better when you are handling complex missing observations in your data or missing data of mixed data types.\nExample\nimport numpy as np\nimport pandas as pd\nimport miceforest as mf\n\n# --- Create sample data ---\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'Age': [25, 30, np.nan, 40, 22, np.nan, 35],\n    'Salary': [50000, 60000, 55000, np.nan, 52000, 58000, np.nan],\n    'Experience': [1, 3, 2, 5, np.nan, 4, np.nan]\n})\nprint(\"Original data with missing values:\")\nprint(data)\n\n# --- Create the imputation kernel ---\nkernel = mf.ImputationKernel(\n    data,\n    num_datasets=1,\n    random_state=42\n)\n\n# --- Run MICE for 3 iterations ---\nkernel.mice(3)\n\n# --- Get completed data ---\nimputed_data = kernel.complete_data(dataset=0)\nprint(\"\\nImputed data:\")\nprint(imputed_data)\nOriginal data with missing values:\n    Age   Salary  Experience\n0  25.0  50000.0         1.0\n1  30.0  60000.0         3.0\n2   NaN  55000.0         2.0\n3  40.0      NaN         5.0\n4  22.0  52000.0         NaN\n5   NaN  58000.0         4.0\n6  35.0      NaN         NaN\n\nImputed data:\n    Age   Salary  Experience\n0  25.0  50000.0         1.0\n1  30.0  60000.0         3.0\n2  40.0  55000.0         2.0\n3  40.0  50000.0         5.0\n4  22.0  52000.0         2.0\n5  35.0  58000.0         4.0\n6  35.0  55000.0         4.0\nFrom the code above, .ImputationKernel() creates a model that imputes each column by taking the number of imputed datasets as arguments, and also the random_state for reproducibility.\nThe .mice() method runs the iterations based on the number given, which is 3 in our case. It first of all imputes the missing values, then run another model based on the previous imputed data till the number of iterations is fulfilled.\n\n\n\nHow miceforest works. Source: MICEforest documentation.\n\n\n\n\n6. Autoimpute\nAutoimpute is another Python library that supports a wide range of imputation methods such as univariate, multivariate, and interpolation methods. It works well with scikit-learn and scipy, and also supports missing data visualization using missingno.\nExample\n**import pandas as pd\nfrom autoimpute.imputations import SingleImputer\n\n# Example data\ndf = pd.DataFrame({\n    \"age\": [25, None, 30, 22, None],\n    \"salary\": [50000, 60000, None, None, 70000],\n    \"gender\": [\"M\", \"F\", \"F\", None, \"M\"]\n})\n\nimputer = SingleImputer(\n    strategy={\n        \"age\": \"mean\", \n        \"salary\": \"median\", \n        \"gender\": \"mode\"  # use mode instead of logistic regression\n    }\n)\n\ndf_imputed = imputer.fit_transform(df)\n\nprint(df)\nprint(df_imputed)**\n    age   salary gender\n0  25.0  50000.0      M\n1   NaN  60000.0      F\n2  30.0      NaN      F\n3  22.0      NaN   None\n4   NaN  70000.0      M\n         age   salary gender\n0  25.000000  50000.0      M\n1  25.666667  60000.0      F\n2  30.000000  60000.0      F\n3  22.000000  60000.0      F\n4  25.666667  70000.0      M\nThe above is a simple imputation using the SingleImputer class, where it imputes various rows with various measures of spread.\n\n\n\nimage.png\n\n\n\n\n7. H2O.ai\nPopularly known for it‚Äôs automl capabilities, H2O can also handle missing data observations both in static or streaming data. You can also use some of its methods to impute missing data, where it‚Äôs a simple imputation like mean, median, or mode, or advanced imputation techniques. H20 is also great in production. If you have incoming data that can contain missing values, you can use the H2O platform to handle that, ensuring that your ML model does not break and give wrong predictions in production.\n\n\n\nimage.png\n\n\n\n\n8. TensorFlow Data Validation (TFDV)\nTensorFlow Data Validation (TFDV) is a powerful library designed to analyze, validate, and monitor data used in machine learning (ML) pipelines. It is useful in production environments, where data consistency and quality are critical for maintaining reliable model performance.\nTFDV allows you to detect issues such as missing features, data drift, and anomalies that may occur as new data flows into your production pipeline. If a particular observation or feature is missing, TFDV can flag it and help you decide whether to impute, correct, or exclude it based on your defined schema.\n\n\n\nimage.png\n\n\n\n\n9. Datawig (Amazon)\nDatawig is an open-source library developed by Amazon for data imputation. It offers advanced data imputation by learning from other variables. While offering complex missing data imputation, it‚Äôs easy to use and not verbose. It also handles both numerical and categorical missing data very well. Datawig uses various deep-learning-based imputation methods to fill missing data.\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/9 Python Libraries for Managing Missing Data Efficiently/index.html#comparison-table",
    "href": "posts/9 Python Libraries for Managing Missing Data Efficiently/index.html#comparison-table",
    "title": "9 Python Libraries for Managing Missing Data Efficiently",
    "section": "Comparison Table",
    "text": "Comparison Table\nYou might wonder, with all the options above, which is the best for your use case. Here is a table summarizing the above Python libraries and what they are best for.\n\n\n\n\n\n\n\n\n\nLibrary\nApproach\nBest For\nLearning curve\n\n\n\n\nPandas\nSimple\nSmall data with few missing observations.\nLow\n\n\nScikit-learn\nStatistical/ML\nIntegration of missing data imputations into ML pipelines.\nLow\n\n\nFancyimpute\nAdvanced\nHigh-dimensional data\nLow\n\n\nMissingno\nVisualization\nMissing data visualization and diagnostics.\nLow\n\n\nMICEforest\nRandom Forest\nRobust imputation\nLow\n\n\nAutoimpute\nStatistical\nReproducibility\nLow\n\n\nH2O.ai\nML/AutoML\nBig data\nHigh\n\n\nTFDV\nProduction-scale\nML pipelines\nHigh\n\n\nDatawig\nDeep Learning\nComplex patterns\nHigh"
  },
  {
    "objectID": "posts/9 Python Libraries for Managing Missing Data Efficiently/index.html#best-practices-for-missing-data-imputation",
    "href": "posts/9 Python Libraries for Managing Missing Data Efficiently/index.html#best-practices-for-missing-data-imputation",
    "title": "9 Python Libraries for Managing Missing Data Efficiently",
    "section": "Best Practices for Missing Data Imputation",
    "text": "Best Practices for Missing Data Imputation\nWhen imputing missing data, here are some best practices you should adhere to:\n\nAlways visualize missingness patterns first: Visualizing missing data lets you see the nature of missing observations in your data. Through the visualizations, you can know the kind of missing data you are dealing with and the relationship of the missing data with other variables. This lets you know the kind of imputation technique to go for.\nAvoid blind imputation without domain knowledge: Don‚Äôt just drop missing data or impute data anyhow; try to understand why it‚Äôs missing. Some data have to be missing, for example, if in a survey given to both men and women, and there is a question regarding pregnancy or menstruation, it‚Äôs obvious that for males, those questions will contain missing values. This kind of missing value is understandable, and imputing it will mean the male has been pregnant or menstruating, which is not possible.\nCompare multiple methods for robustness: Test various imputation methods; some imputation methods are more robust than others. Also, it‚Äôs advisable that you go for multiple imputations to get accurate results.\nValidate imputation impact on model performance: If possible, try to run your model with and without missing values imputed, which lets you see the impact of the missing values on the model and if they are worth imputing."
  },
  {
    "objectID": "posts/9 Python Libraries for Managing Missing Data Efficiently/index.html#conclusion",
    "href": "posts/9 Python Libraries for Managing Missing Data Efficiently/index.html#conclusion",
    "title": "9 Python Libraries for Managing Missing Data Efficiently",
    "section": "Conclusion",
    "text": "Conclusion\nChoosing the right imputation library is important; some libraries offer more advanced imputation methods, while others offer integration into production ML systems. Your choice depends on your use case.\nYou can combine these libraries to achieve better results for example, you can use Missingno with Scikit-learn, where Missingno will handle diagnostics of missing observations, and you use Scikit-learn‚Äôs diverse set of missing values imputation algorithms for imputation.\nIn the future, we hope to see the ease in imputing missing data with the rise of AI-based imputation, and also causal inference methods.\n\nNeed Help with Data? Let‚Äôs Make It Simple.\nAt LearnData.xyz, we‚Äôre here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we‚Äôve got your back.\nüìß Shoot us an email at admin@learndata.xyz‚Äîlet‚Äôs chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/9 Python Libraries for Managing Missing Data Efficiently/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/9 Python Libraries for Managing Missing Data Efficiently/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "9 Python Libraries for Managing Missing Data Efficiently",
    "section": "Your next breakthrough could be one email away. Let‚Äôs make it happen!",
    "text": "Your next breakthrough could be one email away. Let‚Äôs make it happen!"
  },
  {
    "objectID": "posts/Adding User Authentication to Python Shiny Applications using JWT/index.html",
    "href": "posts/Adding User Authentication to Python Shiny Applications using JWT/index.html",
    "title": "Adding User Authentication to Python Shiny Applications using JWT",
    "section": "",
    "text": "LearnData Branding.png\nImagine a house without a lock, how safe do you think the individuals in the house are at night, or when no one is at home. That‚Äôs why we need locks also in our applications, and in software we refer to these locks as authentication.\nAuthentication let‚Äôs you prevent unauthorized access to sensitive information on your application, by letting users authenticate before using your application, you are able to identify each user, and block users trying to compromise the system.\nIn this tutorial, you will learn how to authenticate users into your Python Shiny applications using JWT. With JWT, you don‚Äôt need third party applications or any paid authentication service, once you grasp the concepts of JWT, you can easily handle authentication on your site."
  },
  {
    "objectID": "posts/Adding User Authentication to Python Shiny Applications using JWT/index.html#what-is-jwt",
    "href": "posts/Adding User Authentication to Python Shiny Applications using JWT/index.html#what-is-jwt",
    "title": "Adding User Authentication to Python Shiny Applications using JWT",
    "section": "What is JWT?",
    "text": "What is JWT?\nJWT (JSON Web Token) is a compact, URL-safe way of representing claims between two parties, typically used for secure authentication and data exchange. It consists of three parts:\n\nHeader: Specifying the token type and hashing algorithm\nPayload: Containing the claims (like user ID or roles)\nSignature: Used to verify the token‚Äôs integrity and authenticity. Once issued, a JWT can be sent with each client request (often in the HTTP header) so the server can validate the user without querying the database every time.\n\nWhen you log into a website like Amazon, the server verifies your credentials and responds with a JWT. This token is stored in your browser (e.g., in local storage). On each subsequent request‚Äîlike adding an item to your cart‚Äîthe JWT is sent along, proving your identity without logging in again, until it expires.\n\n\n\nHow JWT works. Image by Author"
  },
  {
    "objectID": "posts/Adding User Authentication to Python Shiny Applications using JWT/index.html#prerequisites",
    "href": "posts/Adding User Authentication to Python Shiny Applications using JWT/index.html#prerequisites",
    "title": "Adding User Authentication to Python Shiny Applications using JWT",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore we get started, ensure you have the following:\n\nPython 3.9+ installed\nVS Code or any other IDE or code editor\nInstall the following libraries:\n\nshiny - a Python library for building web applications\nPyJWT - a Python library for working with JWT\n\nBasic understanding of how Python Shiny works"
  },
  {
    "objectID": "posts/Adding User Authentication to Python Shiny Applications using JWT/index.html#set-up-your-working-environment",
    "href": "posts/Adding User Authentication to Python Shiny Applications using JWT/index.html#set-up-your-working-environment",
    "title": "Adding User Authentication to Python Shiny Applications using JWT",
    "section": "Set Up Your Working Environment",
    "text": "Set Up Your Working Environment\nCreate a folder for your project, give it a suitable name, and inside create the file app.py.\nmkdir new_project\ncd new_project\ntouch app.py\nNext create a virtual environment and activate\n# for linux\npython -m venv venv\nsource venv/bin/activate\n\n# for windows \npy -m venv venv\nvenv/Scripts/Activate\nInstall the needed libraries\npip install shiny PyJWT"
  },
  {
    "objectID": "posts/Adding User Authentication to Python Shiny Applications using JWT/index.html#password-hashing",
    "href": "posts/Adding User Authentication to Python Shiny Applications using JWT/index.html#password-hashing",
    "title": "Adding User Authentication to Python Shiny Applications using JWT",
    "section": "Password Hashing",
    "text": "Password Hashing\nInside app.py, create a SECRET_KEY variable.\nSECRET_KEY = \"your-secret-key-for-jwt\"\nThis secret string allows you to securely create and verify JWT tokens for login and authentication. Think of it like a private password used to sign and check your digital ID cards.\nCreate the following function hash_password inside app.py.\n# Simple password hashing function using HMAC\ndef hash_password(password, salt=None):\n    if salt is None:\n        # Generate a random salt - converting to bytes\n        salt = base64.b64encode(hashlib.sha256(str(datetime.datetime.now().timestamp()).encode()).digest())\n    \n    # Create an HMAC using SHA-256\n    hash_obj = hmac.new(salt, password.encode('utf-8'), hashlib.sha256).digest()\n    return {\"hash\": hash_obj, \"salt\": salt}\nThis function takes a plain password and converts it into a secure hash. The salt is a random value added to make the hash harder to guess. If none is provided, the function creates a new one.\nThe hash_password function uses HMAC with SHA-256 to hash the password + salt securely, and returns both the hash and the salt so we can verify the password later.\n# Verify a password against a stored hash\ndef verify_password(stored_password, provided_password):\n    hash_obj = hmac.new(stored_password[\"salt\"], provided_password.encode('utf-8'), hashlib.sha256).digest()\n    return hash_obj == stored_password[\"hash\"]\nThe verify_passwordfunction checks if the password someone types in matches the stored hashed one. It re-hashes the entered password using the original salt, and returns True if the hashes match, otherwise False.\nhashed_password = hash_password(\"password123\")\nThis hashes the password \"password123\" once when setting up the example user. Since we are not using a real databse, let‚Äôs use the following Python dictionary as our database and store the hashed_password .\nUSERS = {\n    \"user@example.com\": {\n        \"password\": hashed_password,\n        \"name\": \"Demo User\"\n    }\n}"
  },
  {
    "objectID": "posts/Adding User Authentication to Python Shiny Applications using JWT/index.html#building-the-user-interface",
    "href": "posts/Adding User Authentication to Python Shiny Applications using JWT/index.html#building-the-user-interface",
    "title": "Adding User Authentication to Python Shiny Applications using JWT",
    "section": "Building the User Interface",
    "text": "Building the User Interface\nNext, copy and paste the following code into app.py, to create the login_ui .\n# UI for login page\nlogin_ui = ui.div(\n    ui.card(\n        ui.card_header(\"Login\"),\n        ui.input_text(\"email\", \"Email\", placeholder=\"Enter your email\"),\n        ui.input_password(\"password\", \"Password\", placeholder=\"Enter your password\"),\n        ui.div(\n            ui.output_ui(\"login_message\"),\n            style=\"color: red; margin-top: 10px;\"\n        ),\n        ui.input_action_button(\"login_btn\", \"Login\", class_=\"btn-primary\"),\n        width=\"400px\",\n        style=\"margin: 0 auto; margin-top: 100px;\"\n    )\n)\nThe code above builds the login card for users to enter their credentials.\nCopy and paste the following code to create the UI for the protected content.\n# UI for protected content\nprotected_ui = ui.div(\n    ui.card(\n        ui.card_header(ui.output_text(\"welcome_message\")),\n        ui.p(\"This is protected content that requires authentication.\"),\n        ui.input_action_button(\"logout_btn\", \"Logout\", class_=\"btn-danger\"),\n        width=\"800px\",\n        style=\"margin: 0 auto; margin-top: 50px;\"\n    )\n)\nThe code above is what shows after a user successfully logs in.\nTo create the root layout of the application, copy and paste the following code.\n# Main UI that will switch between login and protected content\napp_ui = ui.page_fluid(\n    ui.panel_title(\"JWT Authentication Example\"),\n    ui.output_ui(\"main_content\")\n)\nYou will use the ui.output_ui(\"main_content\") as a placeholder to dynamicallly switch between login_ui and protected_ui ."
  },
  {
    "objectID": "posts/Adding User Authentication to Python Shiny Applications using JWT/index.html#handling-jwt",
    "href": "posts/Adding User Authentication to Python Shiny Applications using JWT/index.html#handling-jwt",
    "title": "Adding User Authentication to Python Shiny Applications using JWT",
    "section": "Handling JWT",
    "text": "Handling JWT\nThe next steps of building the application is going are under the server() side function of the shiny application.\ndef server(input, output, session): \n    # all other remaining code goes here \nInside the server function, copy and paste the following code.\n# Store the JWT token\ntoken_value = reactive.value(None)\n\n# Function to generate JWT token\ndef generate_token(email):\n    payload = {\n        'email': email,\n        'name': USERS[email]['name'],\n        'exp': datetime.datetime.utcnow() + datetime.timedelta(hours=1)\n    }\n    return jwt.encode(payload, SECRET_KEY, algorithm=\"HS256\")\nThe code above defines a reactive variable, token_value, to store a JWT, which starts as None. It also includes a function generate_token() that creates a JWT token for a given email. Inside the function, a payload dictionary is created that contains the user‚Äôs email, their name from the USERS dictionary using the email as a key, and an expiration time set to one hour from the current UTC. The function then returns the encoded JWT using the HS256 algorithm and a SECRET_KEY for signing. This token can is used for authenticating or identifying the user in subsequent requests.\nNext, create the function to verify tokens inside the server function.\n# Function to verify JWT token\ndef verify_token(token):\n    try:\n        payload = jwt.decode(token, SECRET_KEY, algorithms=[\"HS256\"])\n        return payload\n    except jwt.ExpiredSignatureError:\n        return None\n    except jwt.InvalidTokenError:\n        return None\nThis function above checks the validity of a given JWT token. It attempts to decode the token using the same SECRET_KEY and the HS256 algorithm used during encoding.\nIf successful, it returns the decoded payload , which contains user information and the token‚Äôs expiration time.\nIf the token has expired (ExpiredSignatureError) or is otherwise invalid (InvalidTokenError), the function returns None, indicating that the token is not valid or cannot be trusted."
  },
  {
    "objectID": "posts/Adding User Authentication to Python Shiny Applications using JWT/index.html#displaying-the-ui",
    "href": "posts/Adding User Authentication to Python Shiny Applications using JWT/index.html#displaying-the-ui",
    "title": "Adding User Authentication to Python Shiny Applications using JWT",
    "section": "Displaying the UI",
    "text": "Displaying the UI\nInside the server function you created earlier on, paste the following at the end.\n    @render.ui\n    def main_content():\n        \"\"\"Render either login UI or protected content based on authentication status\"\"\"\n        if token_value() is None:\n            return login_ui\n        else:\n            # Verify token before showing protected content\n            payload = verify_token(token_value())\n            if payload:\n                return protected_ui\n            else:\n                # Token is invalid or expired\n                token_value.set(None)\n                return login_ui\nThis code above defines a UI rendering function main_content() using the @render.ui decorator, which dynamically displays either a login interface or protected content based on the user‚Äôs authentication status.\nIf token_value() is None, meaning no token is stored, it returns the login_ui, prompting the user to log in. If a token exists, it attempts to verify the token using verify_token(token_value()).\nIf the token is valid, it returns protected_ui, giving the user access to restricted content. If the token is invalid or expired, it clears the stored token by setting token_value to None and returns the login_ui again, forcing the user to log in.\n@render.ui\ndef login_message():\n    \"\"\"Display login error messages\"\"\"\n    if input.login_btn() &gt; 0:\n        email = input.email()\n        password = input.password()\n        \n        # Basic validation\n        if not email or not password:\n            return ui.p(\"Please enter both email and password\")\n        \n        # Check credentials\n        if email not in USERS:\n            return ui.p(\"Invalid email or password\")\n        \n        # Verify password\n        if not verify_password(USERS[email][\"password\"], password):\n            return ui.p(\"Invalid email or password\")\n        \n        # Generate token on successful login\n        token_value.set(generate_token(email))\n        return ui.p(\"\")\n    return ui.p(\"\")\nThe above code defines a reactive UI function login_message() using the @render.ui decorator, which displays login-related messages based on user input.\nIt triggers when the login buttoninput.login_btn() is clicked, that is its value is greater than 0. The function retrieves the entered email and password, then performs basic validation: if either field is empty, it shows an error message prompting the user to fill both. If the email doesn‚Äôt exist in the USERS dictionary or the password verification via verify_password()fails, it returns a generic \"Invalid email or password\" message.\nIf both credentials are valid, it generates a JWT token using generate_token() and stores it in token_value, effectively logging in the user. If no login attempt has been made yet, or after a successful login, it returns an empty paragraph.\nAdd the following code as the end of the server function.\n@render.text\ndef welcome_message():\n    \"\"\"Welcome message for authenticated user\"\"\"\n    if token_value() is not None:\n        payload = verify_token(token_value())\n        if payload:\n            return f\"Welcome, {payload['name']}\"\n    return \"\"\n\n@reactive.effect\n@reactive.event(input.logout_btn)\ndef _():\n    \"\"\"Handle logout\"\"\"\n    token_value.set(None)\nThe welcome_message() function, decorated with @render.text, displays a personalized welcome message for authenticated users. It checks if a JWT token exists in token_value, verifies it using verify_token(), and if valid, returns a greeting using the user‚Äôs name from the token‚Äôs payload. If the token is absent or invalid, it returns an empty string.\nThe second part is a reactive logout handler defined with @reactive.effect and triggered by the input.logout_btn event. When the logout button is clicked, this function runs and clears the JWT token by setting token_value to None, effectively logging the user out and reverting the UI to an unauthenticated state.\nTo make the application runnable, add the following line of code at the end of app.py.\napp = App(app_ui, server)"
  },
  {
    "objectID": "posts/Adding User Authentication to Python Shiny Applications using JWT/index.html#running-the-application",
    "href": "posts/Adding User Authentication to Python Shiny Applications using JWT/index.html#running-the-application",
    "title": "Adding User Authentication to Python Shiny Applications using JWT",
    "section": "Running the Application",
    "text": "Running the Application\nRun the following command on your terminal to start the application.\nshiny run --reload app.py\nLogin using the email user@example.com and the password password123.\n\nBy using the correct details, you should see the protected content displayed to you."
  },
  {
    "objectID": "posts/Adding User Authentication to Python Shiny Applications using JWT/index.html#conclusion",
    "href": "posts/Adding User Authentication to Python Shiny Applications using JWT/index.html#conclusion",
    "title": "Adding User Authentication to Python Shiny Applications using JWT",
    "section": "Conclusion",
    "text": "Conclusion\nIn this tutorial, you learned how to implement user authentication in Python Shiny applications using JWT. You explored the structure of JWTs, built a basic login interface, securely hashed passwords, and created a system to issue and validate tokens. Most importantly, you saw how to control access to different parts of your app based on authentication status, all without relying on third-party services.\nJWT provides a lightweight and secure method for managing user sessions, making it an excellent choice for small to medium-sized applications. By understanding the core concepts demonstrated here, you can extend this setup to include role-based access control, token refresh mechanisms, and integration with real databases. Here is the full code for the tutorial.\n\nNeed Help with Data? Let‚Äôs Make It Simple.\nAt LearnData.xyz, we‚Äôre here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we‚Äôve got your back.\nüìß Shoot us an email at admin@learndata.xyz‚Äîlet‚Äôs chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/Adding User Authentication to Python Shiny Applications using JWT/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/Adding User Authentication to Python Shiny Applications using JWT/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "Adding User Authentication to Python Shiny Applications using JWT",
    "section": "Your next breakthrough could be one email away. Let‚Äôs make it happen!",
    "text": "Your next breakthrough could be one email away. Let‚Äôs make it happen!"
  },
  {
    "objectID": "posts/CICD with Python Shiny and GitHub Actions/index.html",
    "href": "posts/CICD with Python Shiny and GitHub Actions/index.html",
    "title": "CI/CD with Python Shiny and GitHub Actions",
    "section": "",
    "text": "When working on a project with many collaborators, running tests before merging is essential to ensure that none of the application functionality is broken. This is handled by automating an application‚Äôs testing, building, and deployment process.\nCI stands for continuous integration, which is the automation of an application‚Äôs testing and deployment process, while CD stands for continuous deployment, which is the automation of an application‚Äôs deployment.\nGitHub Actions make it easy to implement CI/CD on applications, primarily if the application repository is hosted on GitHub. In a short period of time, you can set up a workflow that automates your test, build, and deployment process.\nIn this article, you will learn how to use GitHub Actions to create a workflow in your Python Shiny code repository that only merges pull requests from collaborators if tests pass, using Pytest for unit testing."
  },
  {
    "objectID": "posts/CICD with Python Shiny and GitHub Actions/index.html#what-are-github-actions",
    "href": "posts/CICD with Python Shiny and GitHub Actions/index.html#what-are-github-actions",
    "title": "CI/CD with Python Shiny and GitHub Actions",
    "section": "What are GitHub Actions?",
    "text": "What are GitHub Actions?\nMany CI/CD platforms exist, such as Jenkins, Circle CI, and so on. However, GitHub Actions is one of the easiest to work with since it integrates well with projects hosted on GitHub. GitHub Actions is a feature within GitHub that enables you to automate tasks within your software development lifecycle. By using GitHub Actions, you also benefit from GitHub security and code scanning features. GitHub Action uses YAML to create an automation workflow, with the option to run it on the GitHub cloud or your local server.\nGitHub Actions is made up of the following core components:\n\nWorkflow: A configurable automated process comprising one or more jobs. Workflows are defined in YAML files within your repository‚Äôs .github/workflows/ directory.\nEvent: A specific activity that triggers the workflow. Events can be GitHub events (like push, pull_request), scheduled events (cron), or manual triggers.\nJob: A set of steps that execute on the same runner. Jobs can run sequentially or in parallel and depend on other jobs.\nStep: An individual task within a job. Steps can run commands or use actions.\nAction: A reusable extension that can simplify your workflow by performing specific tasks such as checking out code, setting up languages, or deploying.\nRunner: A server that runs your workflows when they‚Äôre triggered. GitHub provides hosted runners, or you can host your own.\n\n\n\n\nGitHub Action Workflow. Image by Author."
  },
  {
    "objectID": "posts/CICD with Python Shiny and GitHub Actions/index.html#a-basic-python-shiny-application",
    "href": "posts/CICD with Python Shiny and GitHub Actions/index.html#a-basic-python-shiny-application",
    "title": "CI/CD with Python Shiny and GitHub Actions",
    "section": "A Basic Python Shiny Application",
    "text": "A Basic Python Shiny Application\nLet‚Äôs build a simple Python Shiny application that lets users compute the total profit and revenue from supermarket sales data and implement some unit tests.\n\nStep 1 - Install dependencies\nCreate a project folder, and copy and paste the following library names into a requirements.txt file.\nshiny\npandas\npytest\npytest-cov\n\nshiny library for building web applications.\npandas library for loading and wrangling data.\npytest library for unit testing.\npytest-cov library for producing pytest coverage reports.\n\nRun the following code on your terminal to install the required libraries.\npython3 pip install -r requirements.txt\n\n\nStep 2 - Setup utility functions\nCreate a new folder, utilis.py. This folder will contain the application‚Äôs business logic.\nimport pandas as pd\n\n# Business Logic\ndef fetch_data():\n    data = pd.read_csv(\"supermarket_sales.csv\")\n    return data\n\ndef total_profit(data):\n    profit = sum(data[\"gross income\"])\n    return profit\n\ndef total_revenue(data):\n    revenue = sum(data[\"cogs\"])\n    return revenue\n\nThe fetch_data() function loads the dataset.\nThe total_profit(data) calculates the total profit.\nThe total_revenue(data) calculates the total revenue.\n\n\n\nStep 3 - Create the application user interface\nCreate a simple user interface that will give users options regarding the type of aggregate they want from the sales data. Copy and paste the following into a file named main.py.\nfrom shiny import reactive\n\nfrom shiny.express import input, render, ui\n\nfrom utilis import fetch_data, total_profit, total_revenue\n\nwith ui.card():\n    ui.h2(\"Sales Dashboard\"),\n    ui.input_select(\n        \"calculation\", \"Select calculation:\", choices=[\"Total Profit\", \"Total Revenue\"]\n    )\n    ui.input_action_button(\"compute\", \"Compute!\")\n\nwith ui.card():\n    with ui.h1():\n        @render.text\n        @reactive.event(input.compute)\n        def result():\n            if input.calculation() == \"Total Profit\":\n                value = total_profit(fetch_data())\n            else:\n                value = total_revenue(fetch_data())\n\n            return value\n\n\nStep 4 - Run the application\nRun the application to ensure that it is working as expected.\n shiny run --reload main.py\n\n\n\nLive Python Shiny Application. Image by Author.\n\n\n\n\nStep 5 - Add unit tests\nAdd the following unit tests into a file test_main.py\nfrom utilis import fetch_data, total_profit, total_revenue\nimport pandas as pd\n\n# Business Logic\ndata = fetch_data()\n\ndef test_total_profit():\n    assert total_profit(data) == 15379.369\n\ndef test_total_revenue():\n    assert total_revenue(data) == 307587.38\ntest_total_profit()and test_total_revenue() check if the values on the Python Shiny application are equal to 1539.369 and 307587.38, respectively.\nRun pytest on your terminal to ensure that the test passes.\n\n\n\nCheck if all unit tests are passed. Image by Author."
  },
  {
    "objectID": "posts/CICD with Python Shiny and GitHub Actions/index.html#setting-up-an-automation-workflow-with-github-actions",
    "href": "posts/CICD with Python Shiny and GitHub Actions/index.html#setting-up-an-automation-workflow-with-github-actions",
    "title": "CI/CD with Python Shiny and GitHub Actions",
    "section": "Setting Up an Automation Workflow with GitHub Actions",
    "text": "Setting Up an Automation Workflow with GitHub Actions\nLet‚Äôs set up an automation workflow that is triggered anytime a push is made. Before proceeding, ensure your project is hosted on GitHub."
  },
  {
    "objectID": "posts/CICD with Python Shiny and GitHub Actions/index.html#step-1---set-up-workflow-directory",
    "href": "posts/CICD with Python Shiny and GitHub Actions/index.html#step-1---set-up-workflow-directory",
    "title": "CI/CD with Python Shiny and GitHub Actions",
    "section": "Step 1 - Set up workflow directory",
    "text": "Step 1 - Set up workflow directory\nIn your project directory, create a new folder, .github; in it, create another folder, workflows. Inside the workflows folder, create a file called run_test.yml. Copy and paste the following code into it.\nname: Run Unit Test via Pytest\n\non: push\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: 3.12.1\n\n      - name: Install dependencies and lint\n        run: |\n          python -m pip install --upgrade pip\n          pip install -r requirements.txt\n          pip install ruff\n          ruff --format=github --target-version=py310 . || true\n\n      - name: Test with pytest\n        run: |\n          coverage run -m pytest -v -s\n          coverage report -m\nHere is a breakdown of the above YAML file.\n\nname: Run Unit Test via Pytest is the label identifying what the workflow does.\non: push means the workflow will start automatically whenever new changes are pushed to the GitHub repository.\njobs contain tasks or steps that the workflow will execute. You can specify various numbers of jobs, here we only have one job named build.\nruns-on: ubuntu-latest means that the job should run on the latest version of Ubuntu, which is the environment in which the code is tested.\nuses: actions/checkout@v3, the first step, clones your code onto the virtual machine where the test will run.\nThe next step, which is name: Set up Python, installs Python on the virtual machine, specifically version 3.12.1, to ensure the code runs in the correct Python environment.\nThe next step name: Install dependencies and lint, install the project dependencies, and check for linting.\n\npython -m pip install --upgrade pip upgrades pip.\npip install -r requirements.txt installs the required libraries.\npip install ruff installs Ruff, a tool that checks your code for potential errors and enforces linting.\nruff --format=github --target-version=py310 . || true runs Ruff to analyze your code. If Ruff detects an issue, ||true ensures that the workflow continues running, allowing you to see listing results without stopping the testing process.\n\nThe last step name: Test with pytest runs the unit tests and checks how much code was covered by the tests.\n\n\nStep 2 - Push to GitHub to run the workflow\nPush your project to GitHub to run the workflow, then go to the Actions tab under the project repository. This will give you a list of all workflows with runners under that specific repository.\n\n\n\nGo to the Actions tab to check running workflows. Image by Author.\n\n\nGo to the Actions tab to check running workflows. Image by Author.\nA green check means the workflow run was successful, while a red cross means it failed. You can click on a specific workflow run and then click on build to get more details about the workflow.\n\n\n\nClick the build under a workflow run to view various tasks under the workflow run. Image by Author.\n\n\n\n\nStep 3 - Create a branch to make a merge request.\nLet‚Äôs create a branch and break some code functionality to ensure the test fails. Create a new branch, change any numbers used in the test function of the test_main.py file, and push to GitHub under the new branch.\n\n\n\nDifference between the main and new branch. Image by Author.\n\n\nClick Create Pull request on GitHub to merge the branch into the main branch. The workflow will be executed, and an error message will show that the unit tests failed.\n\n\n\nThe message shows that the unit test failed in the new branch. Image by Author.\n\n\nNow, return to the branch and fix it as before. If you make a pull request, you will see that all tests have been successfully passed.\n\n\n\nAll checks were successfully passed. Image by Author.\n\n\nCollaborators can still make merge requests even if the tests fail. You need to enforce a rule that prevents any pull request from merging if all tests have not been passed. Go to the GitHub settings and select branch. Under the branch section, give a name to the rule you are about to set and check the option Require status checks to pass.\n\n\n\nGo to Branch rules to enforce a rule. Image by Author."
  },
  {
    "objectID": "posts/CICD with Python Shiny and GitHub Actions/index.html#conclusion",
    "href": "posts/CICD with Python Shiny and GitHub Actions/index.html#conclusion",
    "title": "CI/CD with Python Shiny and GitHub Actions",
    "section": "Conclusion",
    "text": "Conclusion\nThere is more to GitHub actions than just automating the testing of pull requests. You can integrate GitHub Actions with various tools such as Mail, Slack, and Discord to get notifications on GitHub Actions events. Many of these tools are available on the GitHub Marketplace.\nIn his article, you learned how to set up a simple CI/CD workflow with GitHub Actions and Python Shiny. I hope this motivates you to build on this in your next project. Here are some valuable resources on testing and GitHub Actions.\nHow to Conduct Unit Tests in Python Shiny with Pytest\nEnd-to-end Testing with Playwright and Python Shiny\nGitHub Actions and MakeFile: A Hands-on Introduction\nCI/CD in Data Engineering: A Guide for Seamless Deployment\nHow to use Github Actions for Data Science\nHow to Send Detailed Slack Notifications from GitHub Actions?\n\nNeed Help with Data? Let‚Äôs Make It Simple.\nAt LearnData.xyz, we‚Äôre here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we‚Äôve got your back.\nüìß Shoot us an email at admin@learndata.xyz‚Äîlet‚Äôs chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/CICD with Python Shiny and GitHub Actions/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/CICD with Python Shiny and GitHub Actions/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "CI/CD with Python Shiny and GitHub Actions",
    "section": "Your next breakthrough could be one email away. Let‚Äôs make it happen!",
    "text": "Your next breakthrough could be one email away. Let‚Äôs make it happen!"
  },
  {
    "objectID": "posts/Forecasting Time Series Data With Facebook Prophet/index.html",
    "href": "posts/Forecasting Time Series Data With Facebook Prophet/index.html",
    "title": "Forecasting Time Series Data With Facebook Prophet in R",
    "section": "",
    "text": "Have you ever wondered how future prices of currencies, cryptocurrencies, or any other measures are predicted and accurately obtained? Well, this is forecasting at large: forecasting lets you get future values of a measure based on historical data. This historical data is also referred to as time series data, and this article will explain how to use the Facebook Prophet package in R to forecast future values of a measure."
  },
  {
    "objectID": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#prerequisites",
    "href": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#prerequisites",
    "title": "Forecasting Time Series Data With Facebook Prophet in R",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis tutorial will use India‚Äôs daily climate data from 2013 to 2017 to build a facebook prophet model. The data has four parameters;\n\nmeantemp\nhumidity\nwind_speed\nmeanpressure\n\nWe will limit the focus of this article to the meantemp measured in Celsius; you can try for other parameters as an exercise after going through this tutorial. The following are the packages we will need\n\ndplyr for data transformation\nggplot2 for data visualization\nprophet for building the forecasting model"
  },
  {
    "objectID": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#what-is-time-series-data",
    "href": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#what-is-time-series-data",
    "title": "Forecasting Time Series Data With Facebook Prophet in R",
    "section": "What is Time Series Data?",
    "text": "What is Time Series Data?\nTime series data is a collection of data points collected sequentially or chronologically, often with equal time intervals. These intervals are either hourly, daily, weekly, and so on. An example is the yearly child mortality in a country or the daily close price of Bitcoin. Time series data is applied in various fields, from forecasting mortality rates in healthcare to fraud detection in finance and pattern recognition.\n\nKey features of time series data\nBefore calling data, a time series data must have some features. Here is an outline of these features.\n\nChronological Order: Observations are ordered in time.\nTime Dependency: The current value in the series depends on past values.\nStationarity: The mean and variance are constant over time, indicating a consistent pattern.\nTrend: Long-term upward or downward movement.\nSeasonality: Regular and repeating patterns over specific intervals.\nNoise: Random variation or irregular patterns."
  },
  {
    "objectID": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#preparing-the-dataset",
    "href": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#preparing-the-dataset",
    "title": "Forecasting Time Series Data With Facebook Prophet in R",
    "section": "Preparing the Dataset",
    "text": "Preparing the Dataset\nLet‚Äôs load the libraries and data into our R session.\nlibrary(prophet)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nclimate_train &lt;- read_csv(\"/kaggle/input/daily-climate-time-series-data/DailyDelhiClimateTrain.csv\")\nclimate_test &lt;- read_csv(\"/kaggle/input/daily-climate-time-series-data/DailyDelhiClimateTest.csv\")\nprint(head(climate_train))\nprint(head(climate_test))\n\n\n\nPreview of the training and testing set. Image by Author.\n\n\nWe will use the climate_train to train the model and the climate_test to test it. The climate_train model contains data from 2013-01-01 to 2017-01-01, while the climate_test data ranges from 2017-01-01 to 2017-04-24.\nBefore building the Facebook prophet model, we need to subset the date and mean temp variables and rename them ds and y, respectively, as required by the prophet() function.\nclimate_train &lt;- climate_train |&gt; \n        select(date, meantemp) |&gt;\n        rename(ds = date, y = meantemp) |&gt;\n        mutate(ds = as.Date(ds))\n        \nclimate_test &lt;- climate_test |&gt; \n        select(date, meantemp) |&gt;\n        rename(ds = date, y = meantemp) |&gt;\n        mutate(ds = as.Date(ds))\n        \nprint(head(climate_train))\nprint(head(climate_test))\n\n\n\nPreview of the renamed training and test sets. Image by Author."
  },
  {
    "objectID": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#visualizing-time-series-data",
    "href": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#visualizing-time-series-data",
    "title": "Forecasting Time Series Data With Facebook Prophet in R",
    "section": "Visualizing Time Series Data",
    "text": "Visualizing Time Series Data\nLet‚Äôs plot the training data to see how the average daily temperature changes with time.\nggplot(climate_train, aes(x = ds, y = y)) +\n  geom_line(color = \"darkblue\") +\n  labs(\n    title = \"Average Daily Temperature\", \n    x = \"Time\",                          \n    y = \"Temperature\"                    \n  ) +\n  theme_minimal()\n\n\n\nAverage daily temperature, from 2013 - 2017. Image by Author.\n\n\nThe plot above shows temperatures ranging from around 10¬∞C to 40¬∞C. It shows a clear periodic pattern, indicating that temperatures rise and fall consistently yearly, typical of seasonal climate changes. Peaks represent summer months with higher temperatures around 30-40¬∞C, while valleys represent winter months with lower temperatures around 10-15¬∞C. The lowest temperature was recorded in the early months of 2013."
  },
  {
    "objectID": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#building-the-forecasting-model",
    "href": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#building-the-forecasting-model",
    "title": "Forecasting Time Series Data With Facebook Prophet in R",
    "section": "Building the Forecasting Model",
    "text": "Building the Forecasting Model\nLet‚Äôs build a forecasting model to predict future temperature values. We can achieve this by calling the prophet() function on the climate_train train dataset.\nmodel &lt;- prophet(climate_train)\nNext, we need to predict future values. But before doing that, we need to define the future dates we want to predict, which are the dates on the climate_test dataset.\nfuture &lt;- climate_test |&gt;\n        select(ds)\n        \nhead(future)\n\n\n\nCreate future dates variable to use for forecast. Image by Author.\n\n\nThen, we use the predict() function to predict future temperature values based on the model we built earlier.\nforecast &lt;- predict(model, future)\nThe predict() function returns a data frame with several variables, but we will subset the ones we need.\nforecast &lt;- forecast[c('ds', 'yhat', 'yhat_lower', 'yhat_upper')]\nhead(forecast)\n\n\n\nPreview of the forecast values. Image by Author.\n\n\nds - is the date column\nyhat - is the predicted temperature value\nyhat_lower and yhat_upper - represent the lower and upper intervals where the actual temperature value should fall."
  },
  {
    "objectID": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#visualizing-future-forecast",
    "href": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#visualizing-future-forecast",
    "title": "Forecasting Time Series Data With Facebook Prophet in R",
    "section": "Visualizing Future Forecast",
    "text": "Visualizing Future Forecast\nYou can also visualize the forecast values.\nplot(model, forecast)\n\n\n\nVisualization of the forecast values. Image by Author.\n\n\nThe plot above shows that the forecast values exhibit a similar pattern to the trained values.\nThe Facebook prophet model also breaks down forecasts into components, where you can view the model trend and seasonality.\nprophet_plot_components(model, forecast)\n\n\n\nComponents of the forecast values. Image by Author.\n\n\nThe top panel in the plot above shows that the overall trend in average daily temperature increases from January to April, suggesting that the average temperature is gradually rising over this period.\nThe middle panel, which displays the weekly seasonality, illustrates how the average temperature varies by day of the week. Temperature peaks midweek (around Wednesday) and decreases slightly towards the weekend.\nThe bottom pane captures the average temperature across the year, indicating a seasonal cycle. The temperature rises from January, peaking around the middle of the year (likely summer), and then declines towards the end of the year (winter)."
  },
  {
    "objectID": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#measuring-forecast-accuracy",
    "href": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#measuring-forecast-accuracy",
    "title": "Forecasting Time Series Data With Facebook Prophet in R",
    "section": "Measuring Forecast Accuracy",
    "text": "Measuring Forecast Accuracy\nThere are various ways of measuring forecast accuracy, but we will use the Root Mean Square Error (RMSE) approach for this tutorial. RMSE measures forecasting accuracy by taking the mean of the squared difference of the actual value from the subtracted value and taking the square root.\n\\[\nRMSE = \\sqrt{\\frac{\\sum{(y - \\hat{y})^2}}{n}}\n\\]\nLet‚Äôs create a new data frame called comparison, which joins our forecast values with those of the climate_test data frame and subsets just ds, y, and yhat.\ncomparison &lt;- climate_test |&gt;\n        left_join(forecast, by=\"ds\") |&gt;\n        select(ds, y, yhat)\n        \nhead(comparison)\n\n\n\nPreview of the comparison dataframe. Image by Author.\n\n\nWe are going to use the formula above to calculate the RMSE.\ncomparison$error &lt;- comparison$y - comparison$yhat\n\n# Calculate performance RMSE\nrmse &lt;- sqrt(mean(comparison$error^2))\nprint(paste(\"RMSE: \", rmse))\n[1] \"RMSE:  2.76289062761507\"\nThe RMSE value is approximately 2.76, which means that, on average, the model‚Äôs prediction deviates from the actual observed values by approximately 2.76¬∞C. This indicates that the model is performing well."
  },
  {
    "objectID": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#conclusion",
    "href": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#conclusion",
    "title": "Forecasting Time Series Data With Facebook Prophet in R",
    "section": "Conclusion",
    "text": "Conclusion\nForecasting lets us answer many questions based on historical data. Unlike other prediction models, which are feature-based, forecasting is time-based and considers the change of a variable with time.\nThis tutorial taught you how to visualize and forecast time series data. Using what you learned in this tutorial, you can consider forecasting other weather parameters in the dataset or use any existing time series data. If you want to go further and have a deeper understanding of forecasting and time series, here are some invaluable resources.\nCreating Time Series Visualizations in R\nR Dygraphs: How to Visualize Time Series Data in R and R.\nTime Series Analysis in R: How to Read and Understand Time Series Data\nFacebook Prophet Documentation\nUsing R for Time Series Analysis\nFundamentals of time series analysis with R\n14 Time Series Analysis\n\nNeed Help with Data? Let‚Äôs Make It Simple.\nAt LearnData.xyz, we‚Äôre here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we‚Äôve got your back.\nüìß Shoot us an email at admin@learndata.xyz‚Äîlet‚Äôs chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/Forecasting Time Series Data With Facebook Prophet/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "Forecasting Time Series Data With Facebook Prophet in R",
    "section": "Your next breakthrough could be one email away. Let‚Äôs make it happen!",
    "text": "Your next breakthrough could be one email away. Let‚Äôs make it happen!"
  },
  {
    "objectID": "posts/How to Create and Read a Forest Plot in R/index.html",
    "href": "posts/How to Create and Read a Forest Plot in R/index.html",
    "title": "How to Create and Read a Forest Plot in R",
    "section": "",
    "text": "As a researcher trying to compare the results of a particular intervention or treatment from different studies, a forest plot makes it easy to view results from multiple studies. This makes it easy to see variations and trends in an intervention or treatment. In this article, you will learn how to create and customize a forest plot in R using the forestplot package. I will also show you how to read a forest plot."
  },
  {
    "objectID": "posts/How to Create and Read a Forest Plot in R/index.html#what-is-a-forest-plot",
    "href": "posts/How to Create and Read a Forest Plot in R/index.html#what-is-a-forest-plot",
    "title": "How to Create and Read a Forest Plot in R",
    "section": "What is a Forest plot?",
    "text": "What is a Forest plot?\nA forest plot is a type of graph used in research to show the results of a single topic from multiple studies line by line. Each study is represented by a line and a dot, where the dot shows the main result (how effective a treatment is) while the line shows the range of possible results, also known as the confidence intervals. The forest plot sometimes includes a diamond-shaped plot below, summarizing all the results of the studies. If all the lines are close to each other, then it means the studies agree on that particular topic or there are varying findings on that topic.\n\nComponents of a Forest plot\n\nStudy Labels: Located at the left side of the plot, these labels identify each respective study and contain the study author and year.\nEffect Size: This represents the observed effect of an outcome in each study, such as the prevalence of a disease. The position of the point on the plot indicates whether the effect is negative, neutral, or positive.\nConfidence Intervals: Each study shows a horizontal line showing the range of possible outcomes. The narrower the line, the higher the precision of the effect Size.\nNo Effect Line: This vertical line is either zero or one, representing no effect. If a study confidence interval crosses this line, the study is not statistically significant.\nOverall Effect (Diamond Shape): This represents the pooled effect from all studies at the plot‚Äôs bottom. The center of the diamond shows the overall effect, while the width of the diamond represents the confidence interval. The combined result is statistically significant if the diamond does not cross the no-effect line.\n\n\n\n\nForest Plot. Image by Author"
  },
  {
    "objectID": "posts/How to Create and Read a Forest Plot in R/index.html#forestplot-package-in-r",
    "href": "posts/How to Create and Read a Forest Plot in R/index.html#forestplot-package-in-r",
    "title": "How to Create and Read a Forest Plot in R",
    "section": "forestplot package in R",
    "text": "forestplot package in R\nforest plot is an r package for creating publication-ready forest plots and works seamlessly with the pipe (|&gt; or %&gt;%) operator. You can install the package by running the code below.\ninstall.packages(\"forestplot\")\nHere are some critical arguments for plotting a forest plot in R using the forestplot function from the forestplot package.\n\nmean: This takes the column containing the effect size of each study.\nlower: This takes the lower confidence interval of the effect Size.\nupper: This takes the higher confidence interval of the effect Size.\n\nYou can check for additional arguments by running ?forestplot on your R console."
  },
  {
    "objectID": "posts/How to Create and Read a Forest Plot in R/index.html#creating-a-forest-plot-in-r",
    "href": "posts/How to Create and Read a Forest Plot in R/index.html#creating-a-forest-plot-in-r",
    "title": "How to Create and Read a Forest Plot in R",
    "section": "Creating a Forest plot in R",
    "text": "Creating a Forest plot in R\nIn this tutorial, we will use fictional data from various studies studying the effect of a particular cancer treatment. First, load the forest plot package.\nlibrary(forestplot)\nNext, we will load the dataset we will work with.\ncancer &lt;- read.csv(\"https://raw.githubusercontent.com/adejumoridwan/datasets/refs/heads/main/cancer_data\")\nhead(cancer)\n\n\n\nTop 6 rows of the cancer studies dataset. Image by Author.\n\n\nCopy and paste the following code to create the forest plot and run.\ncancer |&gt; \n  forestplot(\n    mean = cancer$EffectSize,\n    lower = cancer$CI_lower,\n    upper = cancer$CI_upper,\n    labeltext = c(Study, EffectSize, CI_lower, CI_upper),\n    xlog = TRUE,\n    boxsize = 0.25,\n    vertices = TRUE,) \nHere are the roles of the arguments specified in the forestplot function:\n\nmean: This argument takes the effect size of the outcome of interest from the dataset.\nlower: This argument takes the lower confidence interval of the effect size.\nupper: This takes the upper confidence interval from the effect size.\nlabeltext: This specifies the columns appearing on the forest plot‚Äôs left side.\nxlog : If TRUE, the marks on the x-axis follow a logarithmic scale.\nboxsize: Overrides the default size of the box used in the plot.\nvertices: If TRUE, apply vertices to the end of the confidence intervals for each study.\n\n\n\n\nPlot the forest plot. Image by Author"
  },
  {
    "objectID": "posts/How to Create and Read a Forest Plot in R/index.html#customizing-forest-plot",
    "href": "posts/How to Create and Read a Forest Plot in R/index.html#customizing-forest-plot",
    "title": "How to Create and Read a Forest Plot in R",
    "section": "Customizing Forest Plot",
    "text": "Customizing Forest Plot\nAdding headers, changing the colors, or changing the theme are various ways of improving the forest plot. All these are done by piping additional functions to the main forestplot() function.\n\nAdd column labels\nLet‚Äôs add column labels to the plot above to create an identification for the columns. Use the pipe operator and pass the following function to the plot.\n|&gt; \nfp_add_header(Study = c(\"Study\"),\n                EffectSize = c(\"Effect Size\"),\n                CI_lower  = c(\"Lower CI\"),\n                CI_upper = c(\"Upper CI\"))\nThe fp_add_header() function assigns new column labels to the dataset column names.\n\n\n\nAdd column labels. Image by Author.\n\n\n\n\nAdd a summary\nYou can add a summary at the bottom of the plot summarizing the whole plot by taking the mean of the effect sizes and the confidence intervals. This will also add a diamond-shaped overall effect below the plot.\n|&gt;\nfp_append_row(mean  = mean(cancer$EffectSize),\n              lower = mean(cancer$CI_lower),\n              upper = mean(cancer$CI_upper),\n              Study = \"Summary\",\n              EffectSize = mean(cancer$EffectSize),\n              is.summary = TRUE)\nThe fp_append_row() adds another row to the forest plot, the mean of the effect size, lower, and upper lower are calculated, and a graph is plotted based on these values. The EffectSize argument adds the value of the overall effect size to the table; you can add the lower and upper confidence intervals by adding the following arguments to the fp_append_row() function.\n\nCI_lower = mean(cancer$CI_lower)\nCI_upper = mean(cancer$CI_upper)\n\n\n\n\nAdd summary row. Image by Author.\n\n\n\n\nAdd colors and demarcate lines\nLet‚Äôs add a line to the plot demarcating the column headers from the observations and change the dots‚Äô colors to blue. Then, pass the following function to the forest plot code.\n  |&gt;\n  fp_add_lines() |&gt; \n  fp_set_style(box = \"royalblue\",\n               line = \"darkblue\",\n               summary = \"royalblue\")\n\nfp_add_lines() adds a line demarcating the headers and summary from the observation rows.\nfp_set_style sets a color to the forest plot.\nThe box argument gives a color to the forest plot boxes.\nThe line argument gives a color to the confidence interval lines.\nThe summary gives a color to the diamond-shaped plot under the summary row.\n\n\n\n\nAdd colors and demarcate lines. Image by Author.\n\n\n\n\nAdd a theme\nYou can add a theme to the forest plot to make it easy to read. Add a zebra-styled theme by passing the following function to the forest plot function.\n|&gt;\nfp_set_zebra_style(\"#EFEFEF\")\n\n\n\nAdd a zebra-styled theme. Image by Author."
  },
  {
    "objectID": "posts/How to Create and Read a Forest Plot in R/index.html#reading-a-forest-plot",
    "href": "posts/How to Create and Read a Forest Plot in R/index.html#reading-a-forest-plot",
    "title": "How to Create and Read a Forest Plot in R",
    "section": "Reading a Forest Plot",
    "text": "Reading a Forest Plot\nEach study has a blue square that represents its effect size. An effect size greater than 1 means the study found a positive effect, while less than 1 means a negative effect. The horizontal lines around each square represent the range of values (‚Äúconfidence interval‚Äù) within which we expect the actual effect size to fall 95% of the time. If this line crosses 1, the effect could be neutral (neither positive nor negative).\nSome studies, like (Smith et al.¬†2015) and (Brown et al.¬†2017), have effect sizes above 1, and their confidence intervals do not cross 1. This implies that they found a significant positive effect.\n\n\n\nEffect size and CI above 1. Image by Author.\n\n\nStudies like (Wilson et al.¬†2020) have effect sizes below one and confidence intervals that do not cross 1, indicating a negative or weaker effect.\n\n\n\nEffect size and CI below 1. Image by Author.\n\n\nStudies such as (Davis et al.¬†2018) and (Jackson et al.¬†2023) have confidence intervals that cross 1, meaning the effect might not be significant.\n\n\n\nCI crossing 1. Image by Author.\n\n\nAt the bottom, a diamond shape represents the ‚Äúsummary effect‚Äù of all studies combined. In this plot, the summary effect size is about 1.14, with a confidence interval that crosses 1. This means that, overall, when we look at all the studies together, there is no effect\n\n\n\nDiamond shaped summary. Image by Author"
  },
  {
    "objectID": "posts/How to Create and Read a Forest Plot in R/index.html#conclusion",
    "href": "posts/How to Create and Read a Forest Plot in R/index.html#conclusion",
    "title": "How to Create and Read a Forest Plot in R",
    "section": "Conclusion",
    "text": "Conclusion\nThe forest plot is not limited to comparing results between studies. You can also use it to compare predictor variables in a regression analysis, where instead of study labels, you have predictor variables and their respective estimates and confidence intervals.\nIn this article, you have learned how to create a forest plot in R using the forest plot package and how to customize and derive insights from it. To learn more, check out the package vignette. There are other packages such as forestploter, forester, and if you want a custom forestplot, you can build yours using ggplot2. Here are further resources that dive deep into interpreting a forest plot. Hope you find them helpful.\nForest Plot Generation in R\nTutorial: How to read a forest plot\nThe 5‚Äâmin meta-analysis: understanding how to read and interpret a forest plot\nSeeing the Forest by Looking at the Trees: How to Interpret a Meta-Analysis Forest Plot\n\nNeed Help with Data? Let‚Äôs Make It Simple.\nAt LearnData.xyz, we‚Äôre here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we‚Äôve got your back.\nüìß Shoot us an email at admin@learndata.xyz‚Äîlet‚Äôs chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/How to Create and Read a Forest Plot in R/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/How to Create and Read a Forest Plot in R/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "How to Create and Read a Forest Plot in R",
    "section": "Your next breakthrough could be one email away. Let‚Äôs make it happen!",
    "text": "Your next breakthrough could be one email away. Let‚Äôs make it happen!"
  },
  {
    "objectID": "posts/How to Forecast Your YouTube Channel Views for the Next 30 Days/index.html",
    "href": "posts/How to Forecast Your YouTube Channel Views for the Next 30 Days/index.html",
    "title": "How to Forecast Your YouTube Channel Views for the Next 30 Days in Python",
    "section": "",
    "text": "Imagine, as a YouTube content creator, you know how many views your videos will have in the coming days. Do you know how powerful such information is?\nThis allows you to target days to post new videos, which increases viewership. You can also know the days of the week when you are likely to get high viewership, to avoid posting on days with low engagement.\nIn this article, you will learn how to use Python to forecast your YouTube daily viewership for the next thirty days. We will use the Prophet library, which is used for time series analysis and forecasting."
  },
  {
    "objectID": "posts/How to Forecast Your YouTube Channel Views for the Next 30 Days/index.html#prerequisites",
    "href": "posts/How to Forecast Your YouTube Channel Views for the Next 30 Days/index.html#prerequisites",
    "title": "How to Forecast Your YouTube Channel Views for the Next 30 Days in Python",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nAn active YouTube and YouTube Studio account\nJupyter Notebook, Google Colab, Kaggle, or any other environment that supports Python\nPandas¬†library installed\nSeaborn¬†library installed\nMatplotlib¬†library installed\nProphet library installed"
  },
  {
    "objectID": "posts/How to Forecast Your YouTube Channel Views for the Next 30 Days/index.html#step-1-extract-the-data",
    "href": "posts/How to Forecast Your YouTube Channel Views for the Next 30 Days/index.html#step-1-extract-the-data",
    "title": "How to Forecast Your YouTube Channel Views for the Next 30 Days in Python",
    "section": "Step 1: Extract the Data",
    "text": "Step 1: Extract the Data\nGo to your YouTube Studio account, under the Analytics tab, and click Advanced mode. Click on the Date tab (1) and select Lifetime under the Date dropdown (2). Next, click Download (3) to download the CSV file.\n\n\n\nYoutube Analytics Dashboard. Image by Author.\n\n\nOpen the Totals.csv file in the extracted folder. You will see the number of views you have from the first day you uploaded a video to your channel to the current date, the day the data was downloaded. The data has two columns: Date and Views. Check out this article if you want more detailed information on extracting YouTube studio data."
  },
  {
    "objectID": "posts/How to Forecast Your YouTube Channel Views for the Next 30 Days/index.html#step-2-load-and-visualize-the-data",
    "href": "posts/How to Forecast Your YouTube Channel Views for the Next 30 Days/index.html#step-2-load-and-visualize-the-data",
    "title": "How to Forecast Your YouTube Channel Views for the Next 30 Days in Python",
    "section": "Step 2: Load and Visualize the data",
    "text": "Step 2: Load and Visualize the data\nLet‚Äôs load the file into our notebook\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"/content/Totals.csv\")\ndf\n\n\n\nDaily channel views. Image by Author.\n\n\nNow let‚Äôs visualize the data to see the channel view over time.\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\ndef _plot_series(series, series_name, series_index=0):\n  palette = list(sns.palettes.mpl_palette('Dark2'))\n  xs = series['Date']\n  ys = series['Views']\n\n  plt.plot(xs, ys, label=series_name, color=palette[series_index % len(palette)])\n\nfig, ax = plt.subplots(figsize=(10, 5.2), layout='constrained')\ndf_sorted = df.sort_values('Date', ascending=True)\n_plot_series(df_sorted, '')\nsns.despine(fig=fig, ax=ax)\nplt.xlabel('Date')\n_ = plt.ylabel('Views')\n\n\n\nTime series plot of channel views over time. Image by Author.\n\n\nFrom the plot above, for a significant period in the beginning, the channel received very few or zero views and a clear inflection point where views suddenly started increasing, and from there, it maintained a high level of activity."
  },
  {
    "objectID": "posts/How to Forecast Your YouTube Channel Views for the Next 30 Days/index.html#step-3-build-the-forecasting-model",
    "href": "posts/How to Forecast Your YouTube Channel Views for the Next 30 Days/index.html#step-3-build-the-forecasting-model",
    "title": "How to Forecast Your YouTube Channel Views for the Next 30 Days in Python",
    "section": "Step 3: Build the Forecasting Model",
    "text": "Step 3: Build the Forecasting Model\nBefore building the time series model, let‚Äôs rename the Date and Views columns in the data to ds and y, respectively. We are doing that because the Prophet library only recognizes those column names. Then, convert the ds to a DateTime class.\nfrom prophet import Prophet\nimport matplotlib.pyplot as plt\n\ndf = df.rename(columns={\"Date\": \"ds\", \"Views\": \"y\"})\ndf[\"ds\"] = pd.to_datetime(df[\"ds\"])\nNow, let‚Äôs initialize and fit the model to our data.\nmodel = Prophet()\nmodel.fit(df)\nLet‚Äôs create a future data frame for the next 30 days\nfuture = model.make_future_dataframe(periods=30)\nforecast = model.predict(future)\n\nforecast.columns\n['ds',\n 'trend',\n 'yhat_lower',\n 'yhat_upper',\n 'trend_lower',\n 'trend_upper',\n 'additive_terms',\n 'additive_terms_lower',\n 'additive_terms_upper',\n 'weekly',\n 'weekly_lower',\n 'weekly_upper',\n 'yearly',\n 'yearly_lower',\n 'yearly_upper',\n 'multiplicative_terms',\n 'multiplicative_terms_lower',\n 'multiplicative_terms_upper',\n 'yhat']\nThe main column of interest is ds, the date of the forecasted point, and yhat, the forecasted value, is the predicted number of views on the YouTube channel for that day.\nLet‚Äôs plot another time series, showing the forecast for the next 30 days.\n# Plot the forecast\nfig1 = model.plot(forecast)\nplt.title(\"Forecast for Next 30 Days\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Value\")\nplt.show()\n\n\n\nForecast for the next 30 days. Image by Author.\n\n\n\nThe black dots above show the actual historical data y; these are the observed values in the dataset.\nThe dark blue line shows the forecasted values yhat.\nThe light blue shaded area is the uncertainty level, which shows the forecast‚Äôs confidence level. The wider the band, the less confident the model is.\n\nThe graph shows a clear upward trend starting in 2023, which Prophet successfully captured. There are also weekly fluctuations, which are the tiny ripples in the blue line; this reflects weekly cycles. The model is reasonably confident in the short term, but a significant spread indicates variability in the underlying data.\nLet‚Äôs break the model into components such as trend and seasonality.\nfig2 = model.plot_components(forecast)\nplt.show()\n\n\n\nForecast decomposition. Image by Author.\n\n\nThe first plot, which shows the trend, shows that the channel‚Äôs YouTube presence significantly grew in 2022, and Prophet recognizes that upward shift as a long-term trend.\nThe second plot, which shows the weekly seasonality, shows that people are less engaged on weekends, especially Saturdays. This means the channel should avoid uploads on Saturdays and focus more on midweek to Thursday when there is peak traffic.\nThe last plot, the yearly seasonality, shows that the viewership peaks at the start of the year, again around September, and in late fall. There are slumps during the summer, likely due to vacations, less screen time, and holiday weeks like December."
  },
  {
    "objectID": "posts/How to Forecast Your YouTube Channel Views for the Next 30 Days/index.html#conclusion",
    "href": "posts/How to Forecast Your YouTube Channel Views for the Next 30 Days/index.html#conclusion",
    "title": "How to Forecast Your YouTube Channel Views for the Next 30 Days in Python",
    "section": "Conclusion",
    "text": "Conclusion\nYou can see how we can extract much information that is not available on YouTube Studio Analytics just by using the daily channel statistics.\nNote that our results are based on the YouTube channel data; yours can differ. You can apply this to your channel subscribers, likes, YouTube earnings, and other important metrics.\nIf you find this article interesting, don‚Äôt forget to check out my¬†blog¬†for other interesting articles, follow me on¬†Medium, connect on¬†LinkedIn, and subscribe to my¬†YouTube channel.\n\nNeed Help with Data? Let‚Äôs Make It Simple.\nAt LearnData.xyz, we‚Äôre here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we‚Äôve got your back.\nüìß Shoot us an email at admin@learndata.xyz‚Äîlet‚Äôs chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/How to Forecast Your YouTube Channel Views for the Next 30 Days/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/How to Forecast Your YouTube Channel Views for the Next 30 Days/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "How to Forecast Your YouTube Channel Views for the Next 30 Days in Python",
    "section": "Your next breakthrough could be one email away. Let‚Äôs make it happen!",
    "text": "Your next breakthrough could be one email away. Let‚Äôs make it happen!"
  },
  {
    "objectID": "posts/How_to_conduct_unit_tests_in_Python_Shiny_with_Pytest/index.html",
    "href": "posts/How_to_conduct_unit_tests_in_Python_Shiny_with_Pytest/index.html",
    "title": "How to Conduct Unit Tests in Python Shiny with Pytest",
    "section": "",
    "text": "You are building a shiny application that processes and visualizes financial data, and a calculation function within the app for calculating financial ratios is incorrect; this could lead to misleading results and poor user decision-making. Unit tests allow you, the developer, to isolate and validate these functions independently, catch errors early, and ensure that changes in other application parts do not unintentionally break a function‚Äôs functionality.\nUnit testing, also known as module or component testing, is the process of testing smaller components of an application to ensure they work as expected. Because unit tests are applied to various components and modules, they make it easy to identify and fix bugs in an application. They are also easy to write and don‚Äôt require any input or output from your application for you to run them. In this article, you will learn about unit tests and how you can effectively implement unit tests in your Python Shiny applications."
  },
  {
    "objectID": "posts/How_to_conduct_unit_tests_in_Python_Shiny_with_Pytest/index.html#separating-concerns",
    "href": "posts/How_to_conduct_unit_tests_in_Python_Shiny_with_Pytest/index.html#separating-concerns",
    "title": "How to Conduct Unit Tests in Python Shiny with Pytest",
    "section": "Separating Concerns",
    "text": "Separating Concerns\nIf you have ever been in a situation where you wrote some lines of code, only to come back months later and find it difficult to fix a bug in a component because you don‚Äôt know which component controls which. You can solve this problem by writing good unit tests for your application, and you need to separate concerns to achieve that. That is why, in Python Shiny applications, it‚Äôs important to separate business logic from reactive logic to ensure that the application is maintainable and scalable in the long run.\nFor example, look at the financial application you are building; the business logic is the computations and data processing of the app‚Äôs user interface, such as data validation, financial calculations, and error handling. The reactive logic is user inputs and reactive outputs, such as file uploads and selecting the financial ratio to compute, display of calculated ratios, generating and updating graphs, etc. Separating the business logic from reactive logic ensures:\n\nAccuracy and Reliability: By separating the business logic from reactive logic. You can rigorously test the business logic to ensure the calculations are correct.\nMaintainability: In the future, you can change business logic without affecting the reactive logic and breaking the user interface.\nTestability: Isolating each calculation in the business logic ensures that every function is tested independently and its value is computed before it is used in the Shiny environment.\nReusability: One might want to use the business logic in one or more contexts or applications; this reduces redundancy and enhances code efficiency."
  },
  {
    "objectID": "posts/How_to_conduct_unit_tests_in_Python_Shiny_with_Pytest/index.html#choosing-a-python-unit-testing-framework",
    "href": "posts/How_to_conduct_unit_tests_in_Python_Shiny_with_Pytest/index.html#choosing-a-python-unit-testing-framework",
    "title": "How to Conduct Unit Tests in Python Shiny with Pytest",
    "section": "Choosing a Python Unit Testing Framework",
    "text": "Choosing a Python Unit Testing Framework\nMany Python unit testing frameworks exist, but you need to consider your project requirements, budget, and tech stack before choosing one. I recommend open-source unit testing frameworks, which are usually free and have strong, supportive communities with many resources to learn from.\nMany Python unit testing frameworks are out there, such as unitest, behave, robot, etc. However, one of the most popular and widely used is the Pytest framework. Pytest is simple, scalable, and powerful, especially when working on projects with external dependencies."
  },
  {
    "objectID": "posts/How_to_conduct_unit_tests_in_Python_Shiny_with_Pytest/index.html#build-a-demo-application",
    "href": "posts/How_to_conduct_unit_tests_in_Python_Shiny_with_Pytest/index.html#build-a-demo-application",
    "title": "How to Conduct Unit Tests in Python Shiny with Pytest",
    "section": "Build a Demo Application",
    "text": "Build a Demo Application\nLet me demonstrate how Pytest works by building a simple sales dashboard based on supermarket sales data. First, ensure you have installed Shiny and Pytest.\npip install shiny pytest\nPaste the following code into a file utilis.py.\nimport pandas as pd\n\n# Business Logic\ndef fetch_data():\n    data = pd.read_csv(\"supermarket_sales.csv\")\n    return data\n\ndef total_profit(data):\n    profit = sum(data[\"gross income\"])\n    return profit\n\ndef total_revenue(data):\n    revenue = sum(data[\"cogs\"])\n    return revenue\nThis is our business logic for the Python Shiny application. To keep it simple, let‚Äôs restrict it to three functions.\n\nfetch_data() to load the sales data.\ntotal_profit() to calculate the total profit.\ntotal_revenue() to calculate the total revenue.\n\nNext, let‚Äôs build the application‚Äôs user interface. Copy and paste the following code into a separate file, main.py.\nfrom shiny import reactive\n\nfrom shiny.express import input, render, ui\n\nfrom utilis import fetch_data, total_profit, total_revenue\n\nwith ui.card():\n    ui.h2(\"Sales Dashboard\"),\n    ui.input_select(\n        \"calculation\", \"Select calculation:\", choices=[\"Total Profit\", \"Total Revenue\"]\n    )\n    ui.input_action_button(\"compute\", \"Compute!\")\n\nwith ui.card():\n    with ui.h1():\n        @render.text\n        @reactive.event(input.compute)\n        def result():\n            if input.calculation() == \"Total Profit\":\n                value = total_profit(fetch_data())\n            else:\n                value = total_revenue(fetch_data())\n\n            return value\nRun this using the command.\nshiny run --reload main.py\n\n\n\nLive Application. Image by Author\n\n\nIf you are building a toy project, you could deploy the application straightaway, but in production, you need to test for each and every component, especially your business logic. If you run the tests in the future and it fails, the unit test will help you track which function has a bug.\n\nUsing Pytest with Shiny\nWhen writing tests with Pytest, your test files must start with either the prefix test_ prefix or end with the suffix_test. This ensures that Pytest identifies the files and functions to run the unit tests.\nCopy and paste the following into a file,\ntest_main.py.\nfrom utilis import fetch_data, total_profit, total_revenue\nimport pandas as pd\n\n# Business Logic\ndata = fetch_data()\n\ndef test_total_profit():\n    assert total_profit(data) == 15379.369\n\ndef test_total_revenue():\n    assert total_revenue(data) == 307587.38\n\nFirst of all, we have to import the data that loads the dataset and store it in the variable data\nThe test_total_profit() function tests if the total_profit function is giving the correct profit sum.\nThe same applies to the test_total_revenue() function, which must be checked to ensure that it returns the correct revenue sum.\n\nTo run the test, call pytest on your terminal.\npytest \n\n\n\nTwo unit tests successfully passed. Image by Author.\n\n\nChange the test_total_profit value in the test function to 16000 and run the test again.\ndef test_total_profit():\n    assert total_profit(data) == 16000\n\n\n\nOne successful test passed, and one failed . Image by Author.\n\n\nYou will notice that not only did the test fail, but it also pointed out the function where it failed. This is very handy when your application has many functions.\nNow, revert the test_total_profit()to its original value, change the name to total_test_profit(), and run the test.\n\n\n\nOne unit test ran and passed. Image by Author.\n\n\nYou can see that only one of the tests ran, and this is due to the absence of the test_ prefix or _test suffix."
  },
  {
    "objectID": "posts/How_to_conduct_unit_tests_in_Python_Shiny_with_Pytest/index.html#python-unit-testing-best-practices",
    "href": "posts/How_to_conduct_unit_tests_in_Python_Shiny_with_Pytest/index.html#python-unit-testing-best-practices",
    "title": "How to Conduct Unit Tests in Python Shiny with Pytest",
    "section": "Python Unit Testing Best Practices",
    "text": "Python Unit Testing Best Practices\nHere are some best practices for conducting unit tests on your Python Shiny applications.\n\nOne Assertion Per Test: Avoid combining different assertions in a single test function. This will make it difficult to pinpoint issues when a test fails. Always make sure each of your test functions has a single assertion.\nMaintain Independence: Make sure your test functions are not reliant on the outcome of other test functions.\nDocumentation: Document your tests to make them easier to maintain and update in the future. This will be handy for you or others who want to work on the project.\nCI Integration: You will notice earlier that we ran the test manually on the terminal; you can use continuous integration systems like GitHub Actions to automate the test periodically and even send alerts when something goes wrong.\nMaintenance: Don‚Äôt just write a test and forget about it. Regularly revisit the test and update it as you update your application codebase.\nSimplicity: Keep your test functions simple, so each test should test for a function in your application.\nProper Naming Conventions: Adopt proper and descriptive naming conventions. Naming your test functions right will avoid overcrowding your test files with comments.\nPay Attention to Code Coverage: High code coverage reduces the chances of having bugs in your application. When writing your test function, always account for every possible condition and error you might expect."
  },
  {
    "objectID": "posts/How_to_conduct_unit_tests_in_Python_Shiny_with_Pytest/index.html#conclusion",
    "href": "posts/How_to_conduct_unit_tests_in_Python_Shiny_with_Pytest/index.html#conclusion",
    "title": "How to Conduct Unit Tests in Python Shiny with Pytest",
    "section": "Conclusion",
    "text": "Conclusion\nWriting tests, especially unit tests, might seem daunting initially, but it pays off in the long run. When a codebase grows large, it becomes difficult to debug, and these tests make it easy to pinpoint the location of bugs. As a developer who develops data applications, adopting sound software engineering principles, such as testing, is a good skill. In this article, you learned about Python unit testing and how to apply Pytest to your Python Shiny application. If you want to read more, here are some additional resources.\n\nA Beginner‚Äôs Guide to Unit Tests in Python (2023)\nPython‚Äôs unittest: Writing Unit Tests for Your Code\nA Gentle Introduction to Unit Testing in Python\nTop 9 Python Unit Testing Frameworks In 2024\nAssert statement for unit testing\nUnit testing: Python Shiny documentation.\n\n\nNeed Help with Data? Let‚Äôs Make It Simple.\nAt LearnData.xyz, we‚Äôre here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we‚Äôve got your back.\nüìß Shoot us an email at admin@learndata.xyz‚Äîlet‚Äôs chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/How_to_conduct_unit_tests_in_Python_Shiny_with_Pytest/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/How_to_conduct_unit_tests_in_Python_Shiny_with_Pytest/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "How to Conduct Unit Tests in Python Shiny with Pytest",
    "section": "Your next breakthrough could be one email away. Let‚Äôs make it happen!",
    "text": "Your next breakthrough could be one email away. Let‚Äôs make it happen!"
  },
  {
    "objectID": "posts/language_translator/index.html",
    "href": "posts/language_translator/index.html",
    "title": "How to Build a Language Translator Application with Strapi, Streamlit, and Hugging Face Models",
    "section": "",
    "text": "If you‚Äôre building a cool side project or an MVP, you must store user and application content. This article will teach you about Strapi, a headless CMS you can use as your application backend. You will build a language translator application using Streamlit and a language translation model from Hugging Face that allows users to translate text written in any language to English, using Strapi as a back-end to store user inputs and outputs."
  },
  {
    "objectID": "posts/language_translator/index.html#prerequisites",
    "href": "posts/language_translator/index.html#prerequisites",
    "title": "How to Build a Language Translator Application with Strapi, Streamlit, and Hugging Face Models",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore starting, ensure you have;\n\nInstalled Python 3.9+ or above.\nInstalled Node.js\nInstalled npm\nSet up a Hugging Face account.\nSet up a Strapi account.\nSet up a Streamlit Cloud account.\nSet up a Github Account."
  },
  {
    "objectID": "posts/language_translator/index.html#set-up-the-project-directory",
    "href": "posts/language_translator/index.html#set-up-the-project-directory",
    "title": "How to Build a Language Translator Application with Strapi, Streamlit, and Hugging Face Models",
    "section": "Set up the Project Directory",
    "text": "Set up the Project Directory\n\nOn your terminal, create a new project language_translator.\n$ mkdir language_translator\nInside the project directory, create a Python virtual environment. This environment maintains the library versions used in your code, ensuring your code is reproducible every time it runs.\n$ python -m venv venv\nThe first venv command creates a virtual environment, while the second venv specifies the name of your virtual environment, which you can give any name you like. After running the above command, a venv folder should be created in your project directory.\n\nRun the following code to activate the virtual environment and start working on it.\n$ ./venv/Scripts/Activate\nUpon activation, you should see the name of your virtual environment in green, signifying that you are already in a virtual environment.\n\nInside the project directory, create a requirements.txt file\nnano requirements.txt\nCopy and paste the following Python Libraries into the requirements.txt file.\nstreamlit\npython-dotenv\nrequests\n\nstreamlit is a Python UI library for building interactive web applications.\npython-dotenv is a Python library for working with environment variables.\nrequests is an HTTP client library that you can use to make requests from an API.\n\nInstall the above Python libraries using the following command\n$ pip install -r requirements.txt\nCreate a .env file to store environment variables such as API keys and secrets.\n$ touch .env\nInside your language_translator directory, create a folder frontend for the application frontend.\n$ mkdir frontend\nIn the folder frontend, create the files main.py and utilis.py\ntouch main.py utilis.py\nIn the project folder, create a .gitignore file, to ignore the venv and .env path when pushing to git.\n/venv\n.env"
  },
  {
    "objectID": "posts/language_translator/index.html#build-the-front-end",
    "href": "posts/language_translator/index.html#build-the-front-end",
    "title": "How to Build a Language Translator Application with Strapi, Streamlit, and Hugging Face Models",
    "section": "Build the Front-end",
    "text": "Build the Front-end\nThe front end is where users will interact with the application. You will build it using Streamlit and translate user inputs using a language translator model from Hugging Face.\n\nSelect a Model from Hugging Face\n\nLog in to Hugging Face, go to the search bar, and search for facebook/mbart-large-50-many-to-one-mmt. This is the model you will use to allow users to give input in any language and translate it into English.\nClick on Deploy, then Inference API.\n\nCopy the code displayed to you and paste it into utilis.py.\n\nimport requests\n\nAPI_URL = \"&lt;https://api-inference.huggingface.co/models/facebook/mbart-large-50-many-to-one-mmt&gt;\"\nheaders = {\"Authorization\": \"Bearer &lt;your-api-token&gt;\"}\n\ndef query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.json()\n\noutput = query({\n    \"inputs\": \"The answer to the universe is\",\n})\n\nAPI_URL is the variable storing the link to the model API.\nheaders is the variable storing your Hugging Face authorization token to use the API.\nThe query function makes a POST request to the model API to translate a given input text and return the JSON output.\n\nCopy your API-TOKEN, go to the .env file, and create the variable. HUGGING_FACE_TOKEN.\nHUGGING_FACE_TOKEN=\"&lt;your-api-token&gt;\"\nGo back to the utilis.py file, copy and paste the following code to create a function of Step 3 that accepts a text in any language and translates it to English.\nimport requests\nimport os\nimport json\nfrom datetime import datetime\nfrom dotenv import load_dotenv\n\nload_dotenv(\".env\")\n\nHUGGING_FACE_TOKEN = os.getenv(\"HUGGING_FACE_TOKEN\")\n\nAPI_URL = \"&lt;https://api-inference.huggingface.co/models/facebook/mbart-large-50-many-to-one-mmt&gt;\"\nheaders = {\"Authorization\": f\"Bearer {HUGGING_FACE_TOKEN}\"}\n\ndef translate(inputs):\n    def query(payload):\n        response = requests.post(API_URL, headers=headers, json=payload)\n        return response.json()\n\n    output = query(\n        {\n            \"inputs\": inputs,\n        }\n    )\n    return output[0][\"generated_text\"]\n\nload_dotenv(\".env\") loads your .env file, making it visible for os.getenv() to get the environment variable specified.\nThe translate() function takes input text, while the query function processes it and returns it in JSON. Finally, the translate() function extracts the translated text.\nHere is an example of how the translate function works. Copy and run the text below.\n\ntext = \"‡§∏‡§Ç‡§Ø‡•Å‡§ï‡•ç‡§§ ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§ï‡•á ‡§™‡•ç‡§∞‡§Æ‡•Å‡§ñ ‡§ï‡§æ ‡§ï‡§π‡§®‡§æ ‡§π‡•à ‡§ï‡§ø ‡§∏‡•Ä‡§∞‡§ø‡§Ø‡§æ ‡§Æ‡•á‡§Ç ‡§ï‡•ã‡§à ‡§∏‡•à‡§®‡•ç‡§Ø ‡§∏‡§Æ‡§æ‡§ß‡§æ‡§® ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à\"\ntranslate(text)\n\n&gt;&gt;Output: 'The head of the UN says there is no military solution in Syria'\n\n\n\nBuild the User Interface with Streamlit\n\nInside your main.py file, import the translate function and streamlit.\nfrom utilis import translate\nimport streamlit as st\nCreate a title and input area.\nst.title(\"Language Translator to English\")\n\ninput_text = st.text_area(\"Enter the text you want to translate:\", height=150)\n\nst.title sets a text in bold and large size.\nst.text_area is a multi-line text input widget that allows users to give input.\n\nRun the code below on your terminal to launch the Streamlit application.\nstreamlit run main.py\n\nCreate a button to process any input given in the text area.\nif st.button(\"Translate\"):\n    if input_text:\n        translated_text = translate(input_text)\n        st.write(\"## Translated Text\")\n        st.write(translated_text)\n    else:\n        st.warning(\"Please enter some text to translate.\")\n\nst.button creates a button to translate a text.\nIf there is an input text, the translation function is called on the input_text and saved as translated_text; otherwise, a warning is given.\nSave your file and go to the Streamlit application to see the changes.\n\n\nPaste the Hindi text given earlier into the text area and click on translate.\n\nWhat is left is to set up a back-end on Strapi to save any translated text."
  },
  {
    "objectID": "posts/language_translator/index.html#build-the-back-end",
    "href": "posts/language_translator/index.html#build-the-back-end",
    "title": "How to Build a Language Translator Application with Strapi, Streamlit, and Hugging Face Models",
    "section": "Build the Back-end",
    "text": "Build the Back-end\n\nSet up Strapi\n\nIn your main directory language_translator, create a new Strapi project called backend.\nnpx create-strapi-app@latest backend --quickstart\nThis will install Strapi and an SQLite database in your project directory.\nRun the following code in your terminal to open the admin panel at http://localhost:1337/admin. Fill in your credentials, Sign up, and Log in.\nnpm run develop\n\nGo to Content Type Builder, click Create a new collection, create a new collection Translation, then Continue.\n\nCreate input_text and translated_text as Text fields while translation_date is a Date field and Save.\n\nGo to Settings on your admin menu, under User & Permissions Plugin, click on Roles, and edit the Public role.\n\nIn the Public role, set the Translation permission to create and find, then Save.\n\n\n\n\nConnect Strapi with Streamlit\n\nCopy and paste the following code to create a function save_translation that saves both the input_text and output_text to Strapi.\nSTRAPI_URL = \"&lt;http://localhost:1337/api&gt;\"\n\ndef save_translation(input_text, translated_text):\n    data = {\n        \"data\": {\n            \"input_text\": input_text,\n            \"translated_text\": translated_text,\n            \"translation_date\": datetime.now().isoformat(),\n        }\n    }\n\n    response = requests.post(\n        f\"{STRAPI_URL}/translations\",\n        headers={\"Content-Type\": \"application/json\"},\n        data=json.dumps(data),\n    )\n    return response.json()\n\nSTRAPI_URL is the link to the Strapi backend API. You will use this to send and receive data from Strapi.\nThe save_translation() function receives the user input_text, translated_text, and translation_date as JSON in the variable data and sends it for storage in the Strapi back-end.\ndatetime.now().isoformat() saves the current date and time an input_text was translated as \"translation_date\" into data.\nThe response variable makes a POST request to save the values into the Translation collection in Strapi.\n\nGo back to main.py and update the code just before the end of the inner if condition. This will ensure that anytime a user clicks the Translate button, the text displays, and the input and output text are saved to Strapi through the save_translation() function.\n\nif st.button(\"Translate\"):\n    if input_text:\n        #code to display text after input is given\n        #....\n        #code to update\n        #--------------\n        save_translation(input_text, translated_text)\n    else:\n        st.warning(\"Please enter some text to translate.\")\n\nTry to translate a text, and go back to your dashboard under the Content Manager menu to see the saved text.\n\nTo output the history from latest to oldest, add the get_history() function to utilis.py.\ndef get_history():\n    response = requests.get(f\"{STRAPI_URL}/translations\")\n    if response.status_code == 200:\n        return response.json()\n    return []\n\nThis function gets all the items stored in the Translation collection using a GET request and saves it in the variable response as JSON.\n\nUpdate main.py with the following code to create a History button where users can view previous translations.\nif st.button(\"History\"):\n    history = get_history()\n    if history:\n        for item in reversed(history[\"data\"]):\n            st.text(\n                f\"Date: {item['attributes']['translation_date']}\\\\nInput: {item['attributes']['input_text']}\\\\nTranslated: {item['attributes']['translated_text']}\"\n            )\n    else:\n        st.write(\"No history found.\")\n\nIf the History button is clicked, the get_history() is called, fetching and displaying all items in the Translation collection.\nThe reversed() function loops the list history[data] in a reverse order to display recently added text first.\nIf no history is found, the application returns a warning.\nIf you click the history button, you should have something like this."
  },
  {
    "objectID": "posts/language_translator/index.html#deployment",
    "href": "posts/language_translator/index.html#deployment",
    "title": "How to Build a Language Translator Application with Strapi, Streamlit, and Hugging Face Models",
    "section": "Deployment",
    "text": "Deployment\nThe application consists of two components: the back end and the front end. There are various ways to deploy these components, but for this tutorial, you will use Strapi Cloud to deploy the back end and Streamlit Cloud to deploy the front end.\nStrapi Cloud deployment is free for 14 days, but the Streamlit Cloud is completely free. Before proceeding, ensure you have already versioned your project on GitHub.\n\nDeploy the Backend on Strapi Cloud\n\nGo to Deploy on your admin dashboard menu, and click Deploy to Strapi Cloud.\n\nSign in to Strapi Cloud.\n\nClick on Create Project to create a new project.\n\nSelect the Free trial and click on GitHub to permit Strapi to authorize your GitHub account.\n\nAfter completing the authorization steps, you should have something like this. Select the account you want Strapi to access and the project repository.\n\nGive the application a display name, and leave other options as the default.\n\nUnder Show Advanced Settings, type the backend directory, which is /backend, leave other options as default, and click on Create Project.\n\nYou can see the application build and deploy logs displayed while the application is being built.\n\nClick on the Visit app to open up the live admin panel.\n\nLike before, when you open the admin panel locally, fill in your credentials and log in.\n\nSince the backend is deployed, change the STRAPI_URL variable in utilis.py to the live URL. Copy the URL of the live dashboard, excluding the part /admin, and include /API at the end.\nSTRAPI_URL=\"&lt;your-strapi-api-url&gt;/api\"\nEnsure you cross-check your settings to see if all the roles are set. If not, you can set them back using Step 6 in the Set up Strapi section.\n\nThat‚Äôs it, you now have your back-end live, let‚Äôs deploy the frontend and the back-end.\n\n\nDeploy the Frontend on Streamlit Cloud\n\nLog in to Streamlit Cloud and authorize Streamlit to access your GitHub account.\nClick on Create app.\n\nPlease complete the form by providing the project repository and the path of the streamlet application, which is /frontend/main.py. If you have authorized Streamlit to access your GitHub repositories, it will list all the repositories in your GitHub account.\n\nClick on Advanced Settings, type in your HUGGING_FACE_TOKEN as it is in your .env file, and Save.\n\nClick on Deploy! to start building the application. This is going to take a while.\nNow, you have successfully deployed the Streamlit application."
  },
  {
    "objectID": "posts/language_translator/index.html#conclusion",
    "href": "posts/language_translator/index.html#conclusion",
    "title": "How to Build a Language Translator Application with Strapi, Streamlit, and Hugging Face Models",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, you have learned how to use a model from Hugging Face to build a Streamlit application and store your user inputs and outputs on a Strapi backend. You also learned how to deploy the back-end and front-end components on Strapi and Streamlit Cloud. Strapi has a fun and active Discord community to help answer your questions whenever you feel stuck.\nHere is the GitHub repository for the language translator application. You can further extend the application by:\n\nAdd authorization to your Streamlit front end and set various user roles and permissions for your Strapi backend.\nExploring various language models on Hugging Face, such as English-to-many language translators or audio translators."
  },
  {
    "objectID": "posts/language_translator/index.html#recommended-reads",
    "href": "posts/language_translator/index.html#recommended-reads",
    "title": "How to Build a Language Translator Application with Strapi, Streamlit, and Hugging Face Models",
    "section": "Recommended Reads",
    "text": "Recommended Reads\n\nML Model Deployment with FastAPI and Streamlit\nUsing Python with Strapi\nDeploying to Strapi Cloud\nAdmin Panel Customization\nAnalyzing data from a Strapi API with Python\nDeploying to Streamlit Cloud\n\n\nNeed Help with Data? Let‚Äôs Make It Simple.\nAt LearnData.xyz, we‚Äôre here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we‚Äôve got your back.\nüìß Shoot us an email at admin@learndata.xyz‚Äîlet‚Äôs chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/language_translator/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/language_translator/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "How to Build a Language Translator Application with Strapi, Streamlit, and Hugging Face Models",
    "section": "Your next breakthrough could be one email away. Let‚Äôs make it happen!",
    "text": "Your next breakthrough could be one email away. Let‚Äôs make it happen!"
  },
  {
    "objectID": "posts/Model Deployment in R with Plumber/index.html",
    "href": "posts/Model Deployment in R with Plumber/index.html",
    "title": "Model Deployment in R with Plumber",
    "section": "",
    "text": "The primary aim of machine learning is to learn certain patterns from available data and to make decisions based on reliable facts.\nImagine building a machine learning model, and you get good accuracy. That does not end there; you must deploy this model for clients to interact with. Maybe it‚Äôs a classification or a regression model. What use do you think the model is if it‚Äôs just on your PC?\nDeploying machine learning models puts them into real-life scenarios, allowing you to monitor them in real-time. This is also important as you can easily track anomalies through the model‚Äôs accuracy metrics.\nIn this article, you will learn how to deploy a machine-learning model using Plumber, an R package for developing APIs. You will build a Facebook prophet model forecasting the closing price of Bitcoin and deploy it as a Plumber API such that when a date is given, you can get the closing price for that particular date."
  },
  {
    "objectID": "posts/Model Deployment in R with Plumber/index.html#prerequisites",
    "href": "posts/Model Deployment in R with Plumber/index.html#prerequisites",
    "title": "Model Deployment in R with Plumber",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo follow this tutorial, download the Bitcoin dataset containing Bitcoin‚Äôs closing price information from January 1st, 2020, to April 19th, 2021. You also need to install the following Python libraries.\n\nplumber - for generating APIs\nprophet - for time series analysis\nlubridate - for date manipulation\ndplyr - for data manipulation"
  },
  {
    "objectID": "posts/Model Deployment in R with Plumber/index.html#what-is-plumber",
    "href": "posts/Model Deployment in R with Plumber/index.html#what-is-plumber",
    "title": "Model Deployment in R with Plumber",
    "section": "What is Plumber?",
    "text": "What is Plumber?\nR plumber is an R package for creating web APIs by decorating your R functions. For example, you can have a normal function that calculates the area of a rectangle.\narea_of_rectangle &lt;- function(length, breadth) {\n  return(length * breadth)\n}\nAnd convert the function into an API by decorating it with roxygen2comments #*.\nlibrary(plumber)\n\n#* Return the Area of a rectangle\n#* @param length numeric\n#* @param breadth numeric\n#* @get /area_of_rectangle\nfunction(length, breadth) {\n  return(as.numeric(length) * as.numeric(breadth))\n}\n\n#* Return the Area of a rectangle is a comment describing what the function does.\n#* @param length numeric indicates that the function takes a numeric parameter called length.\n#* @param length numeric also shows that the function takes another numeric parameter called breadth.\n#* @get/area_of_rectangle tells Plumber to create a web endpoint at the URL path /area_of_rectangle. The function is triggered when this URL is visited with the correct parameters.\n\nTo run the newly created API, click Run API just above the R script to open the API endpoint docs automatically.\n\n\n\nExample of an R API. Image by Author.\n\n\nYou can interact with the API‚Äôs automatic documentation and see if they return valid results or errors.\n\n\n\nAutomatic API docs. Image by Author.\n\n\nYou can also assess the value of the area directly on your browser through the following URL &lt;your_local_host&gt;/area_of_rectangle?length=5&breadth=12. Because the endpoint is a GET verb, the function parameters are known as query parameters and are provided in the URL path after a question mark.\n\n\n\nSample API URL. Image by endpoint.\n\n\n\nCreating a Plumber project\nTo create a plumber project, go to File, then New Project. Under the New Project Wizard, select New Directory and the New Plumber API Project.\n\n\n\nCreate a Plumber project. Image by Author.\n\n\nOnce you have created the Plumber API Project, you will see a file called plumber.R . This is where you will write all the logic needed to create the API."
  },
  {
    "objectID": "posts/Model Deployment in R with Plumber/index.html#building-the-forecasting-model",
    "href": "posts/Model Deployment in R with Plumber/index.html#building-the-forecasting-model",
    "title": "Model Deployment in R with Plumber",
    "section": "Building the Forecasting Model",
    "text": "Building the Forecasting Model\nRun the following code in the plumber.R to load the libraries and data.\nlibrary(plumber)\nlibrary(prophet)\nlibrary(dplyr)\nlibrary(lubridate)\n\n# Load and preprocess the data\nbitcoin_data &lt;- read.csv(\"gemini_BTCUSD_2020_1min.csv\")\n\nhead(bitcoin_data)\n\n\n\nimage.png\n\n\nCopy and paste the following code below to build the forecasting model.\n# Ensure the Date column is in datetime format\nbitcoin_data$Date &lt;- mdy_hm(bitcoin_data$Date)\n\n# Extract only the date part (ignoring time)\nbitcoin_data$Date &lt;- floor_date(bitcoin_data$Date, unit = \"day\")\n\n# Calculate the daily average of the 'Close' prices\ndaily_data &lt;- bitcoin_data |&gt;\n  group_by(Date) |&gt;\n  summarise(Close = mean(Close))\n\n# Prepare the data for Prophet\nprophet_data &lt;- data.frame(ds = daily_data$Date, y = daily_data$Close)\n\n# Train the Prophet model\nmodel &lt;- prophet(prophet_data)\nCheck out the following article to learn more about building forecasting models with the Facebook Prophet."
  },
  {
    "objectID": "posts/Model Deployment in R with Plumber/index.html#creating-the-plumber-api",
    "href": "posts/Model Deployment in R with Plumber/index.html#creating-the-plumber-api",
    "title": "Model Deployment in R with Plumber",
    "section": "Creating the Plumber API",
    "text": "Creating the Plumber API\nInside the plumber.R file, copy and paste the following code.\n#* Predict closing prices for a given date\n#* @param date A future date (format: YYYY-MM-DD) to predict closing price for\n#* @get /predict\nfunction(date) {\n  # Convert the input date to a Date object\n  input_date &lt;- ymd(date)\n\n  # Get the last available date in the dataset\n  last_date &lt;- ymd(max(prophet_data$ds))\n\n  # Validate the input date\n  if (is.na(input_date)) {\n    return(list(\n      error = \"Invalid date format. Please provide a date in YYYY-MM-DD format.\"\n    ))\n  }\n\n  if (input_date &lt;= last_date) {\n    return(list(\n      error = \"Please provide a future date beyond the last available date in the dataset.\",\n      last_available_date = as.character(last_date)\n    ))\n  }\n\n  # Calculate the number of days to forecast\n  days_to_forecast &lt;- input_date - last_date\n\n  # Check if the number of days to forecast is valid\n  if (days_to_forecast &lt;= 0) {\n    return(list(\n      error = \"The calculated forecast period is negative. Please check your input.\"\n    ))\n  }\n\n  # Generate future predictions\n  future &lt;- make_future_dataframe(model, periods = days_to_forecast)\n  forecast &lt;- predict(model, future)\n\n  # Find the prediction for the input date\n  forecast_for_date &lt;- forecast |&gt;\n    filter(ds == input_date) |&gt;\n    select(ds, yhat, yhat_lower, yhat_upper)\n\n  # Return the prediction for the given date\n  if (nrow(forecast_for_date) == 0) {\n    return(list(\n      error = \"Unable to generate forecast for the given date. Please try a valid future date.\"\n    ))\n  } else {\n    return(forecast_for_date)\n  }\n}\nThe above function predicts the closing price of Bitcoin for a specified future date. It uses a GET verb at the endpoint /predict and takes the argument date in the format YYYY-MM-DD.\nFirst, input_date is converted to a date-time object using the lubridate function ymd(). The same is done for the last_date in the dataset.\nThe input_date is also validated in case no date was given, or the date given is less than or equal to the last_date in the dataset.\nThe days_to_forecast is calculated by subtracting the last date from the input date, which is validated if it is less than or equal to 0.\nThe predict function makes future predictions by taking the model and the future data frame as arguments.\nFinally, the date, actual predictions of the input data, and confidence intervals are filtered from the forecast data frame and returned.\nLet‚Äôs run the API to interact with the forecast model API docs.\n\n\n\nRun and open the API automatically generated docs. Image by Author."
  },
  {
    "objectID": "posts/Model Deployment in R with Plumber/index.html#hosting",
    "href": "posts/Model Deployment in R with Plumber/index.html#hosting",
    "title": "Model Deployment in R with Plumber",
    "section": "Hosting",
    "text": "Hosting\nR Plumber provides various ways to host your API. You can use Posit Connect, which is recommended as the most straightforward deployment method, Digital Ocean, or deploy using Docker. Once you have hosted your API, clients can interact with it to get real-time predictions. You can further set up tests and CI/CD for automated testing to ensure that as your application grows large, it does not break."
  },
  {
    "objectID": "posts/Model Deployment in R with Plumber/index.html#conclusion",
    "href": "posts/Model Deployment in R with Plumber/index.html#conclusion",
    "title": "Model Deployment in R with Plumber",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, you have learned how to use R Plumber to deploy your machine-learning models as an API. You can integrate these APIs into web applications and allow users to interact with the model. This way, you solve business problems and see how your model performs on real-world data. If you want to dive more into developing machine learning APIs, here are useful resources from which you can learn.\nRecreating the Shiny App tutorial with a Plumber API + React\nHow to put an R model in production\nDeploying a Machine Learning Model Using Plumber and Docker\nHow to deploy a Tensorflow Model in R with Plumber\nDeploying to RStudio Connect: A Beginner‚Äôs Guide\n**How to Make an R REST API: A Beginners Guide to Plumber**\n\nNeed Help with Data? Let‚Äôs Make It Simple.\nAt LearnData.xyz, we‚Äôre here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we‚Äôve got your back.\nüìß Shoot us an email at admin@learndata.xyz‚Äîlet‚Äôs chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/Model Deployment in R with Plumber/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/Model Deployment in R with Plumber/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "Model Deployment in R with Plumber",
    "section": "Your next breakthrough could be one email away. Let‚Äôs make it happen!",
    "text": "Your next breakthrough could be one email away. Let‚Äôs make it happen!"
  },
  {
    "objectID": "posts/Parametric vs Non-parametric Statistics/index.html",
    "href": "posts/Parametric vs Non-parametric Statistics/index.html",
    "title": "When Not To Use Parametric or Non-Parametric Statistics",
    "section": "",
    "text": "Data comes in various forms, and understanding the structure of your data helps you select the most suitable methodology for your analysis.\nFor example, you collected data on income from 10,000 samples from two different countries: Country A and Country B.\nCountry A is a country where most individuals in the sample fall within the average income category, whereas in Country B, most individuals are in the low-income category.\nIf you are to plot a histogram of Country A incomes, you will find out that it is normally distributed, while Country B is skewed to the left.\nIf you are to calculate the difference in income between genders, the statistical methodology for Country A would be different from Country B, due to the distribution of the income variable.\nThese differences are why we have parametric and non-parametric statistical techniques, and this is what you will learn in this article."
  },
  {
    "objectID": "posts/Parametric vs Non-parametric Statistics/index.html#what-are-parametric-statistics",
    "href": "posts/Parametric vs Non-parametric Statistics/index.html#what-are-parametric-statistics",
    "title": "When Not To Use Parametric or Non-Parametric Statistics",
    "section": "What are Parametric Statistics",
    "text": "What are Parametric Statistics\nParametric statistics is a branch of statistics that assumes the data follows an underlying population distribution, usually the normal distribution.\nIt uses parameters such as the mean, standard deviation, variance, and correlation, among others, to make inferences about the population.\n\nKey assumptions\nThese are the assumptions your data have to meet for you to use parametric statistics:\n\nNormality: The data should be approximately normally distributed. When the density plot is plotted, it should have a bell-shaped curve.\nEqual Variance: Parametric statistics expect the data to have equal variances. For example, if you want to compare men‚Äôs vs women‚Äôs reaction times with a t-test, both groups should have a similar spread in reaction times.\nIndependence of Observations: Each observation should not depend on another observation. For example, in a drug trial, one patient‚Äôs response should not affect another patient‚Äôs response.\nScale of Measurement: The data used should be on a quantitative scale, either interval or ratio. You can‚Äôt use parametric statistics for data that are categorical in nature.\nRandom Sampling: Data is obtained from a random sample of a population to ensure the results can be generalized. If you want to know the average blood pressure in a city, you shouldn‚Äôt just pick the patients from one clinic, but pick randomly from different clinics across the city.\n\n\n\n\nNormal distribution. Source: Image generated by Nano Banana.\n\n\n\n\nExamples\nThere are various examples of parametric tests, some of which are:\n\nt-test: Also known as the Student t-test, is used when you are interested in comparing two groups. For example, a pharmaceutical company who are interested in knowing if a new painkiller works better than the standard one.\nANOVA: The Analysis of Variance (ANOVA) is used to compare three or more groups. For example, a teacher wants to compare four different teaching methods and see if they lead to the same exam score.\nPearson Correlation: This is one of the various types of correlation in statistics, used to test the strength and relationship between two variables. For example, a researcher wants to test the relationship between income and education.\nZ-test: Mostly used in quality control to see if the sample mean equals the population mean. For example, a manufacturing company interested in determining if light bulbs are supposed to last 1,000 hours on average, randomly tests 50 bulbs and compares the sample mean to 1,000 hours.\n\n\n\n\nApplication of Z-test in lilght bulb manufacturing. Source: Image generated by Nano Banana.\n\n\n\n\nAdvantages\nParametric statistics are widely used in research due to the advantages they offer, some of which are:\n\nMore Statistical Power: They tend to detect a real effect if it exists. A medical researcher using a t-test to check if a new drug lowers blood pressure will have a better chance of detecting a real improvement by using parametric statistics.\nAbility to Estimate Parameters: They provide estimates of population characteristics like mean, variance, and standard deviation. A factory doesn‚Äôt just want to know if bulbs differ; it also wants to know the average lifespan and the spread, which is one of the use cases of parametric statistics.\nWider Range of Tests Available: There are a lot of parametric tests covering both simple and complex scenarios, giving researchers many tools to work with and compare.\nFlexibility and Efficiency with Large Samples: With larger datasets, parametric methods handle data efficiently, and assumptions are less strict due to the central limit theorem.\nResults are Easier to Interpret: The outputs of parametric tests are intuitive and directly tied to real-world quantities."
  },
  {
    "objectID": "posts/Parametric vs Non-parametric Statistics/index.html#what-are-non-parametric-statistics",
    "href": "posts/Parametric vs Non-parametric Statistics/index.html#what-are-non-parametric-statistics",
    "title": "When Not To Use Parametric or Non-Parametric Statistics",
    "section": "What are Non-Parametric Statistics?",
    "text": "What are Non-Parametric Statistics?\nUnlike parametric statistics, non-parametric statistics do not rely on assumptions about the population distribution. They do not require the data to follow a normal distribution.\nInstead of estimating parameters like means and variances, non-parametric statistics rely on medians, ranks, or counts.\nThey come in handy when the data is skewed, categorical, or has a small sample size\n\n\n\nNormal vs Skewed data. Source: Image generated by Nano Banana.\n\n\nImage generated by Nano Banana.\n\nKey assumptions\nHere is what your data needs to have for applying non-parametric statistics.\n\nIndependence of Observations: Just like parametric statistics, non-parametric statistics also expect the observations to be independent of each other.\nAppropriate Measurement Scale: Non-parametric statistics can handle data that is on either an interval, ratio, or ordinal scale.\nRandom Sampling: Non-parametric statistics expect the data to come randomly from a population, just like parametric statistics.\n\n\n\nExamples\nMost of the tests under parametric statistics have their non-parametric alternatives, in case the data you want to apply it to violates any of the parametric assumptions.\n\nMann-Whitney U Test: This is an alternative to the independent t-test, and is used to compare two groups when the data is not normally distributed. For example, comparing the customer satisfaction ratings between two restaurants.\nWilcoxon Signed-Rank Test: A non-parametric alternative to the paired t-test that compares two related samples. For example, measuring stress levels before and after yoga sessions, if the data is skewed.\nKruskal-Wallis Test: Non-parametric alternative to the ANOVA for comparing three or more groups without assuming normality. An example is comparing median waiting times across three banks.\nSpearman‚Äôs Rank Correlation: Measures association based on ranks and not raw values. It‚Äôs an alternative to Pearson‚Äôs Correlation Coefficient.\nChi-Square Test: It is used to test the relationship between two categorical variables. For example, testing if gender is related to preferred smartphone brand.\n\n\n\nAdvantages\n\nFewer Assumptions: Non-parametric statistics are suitable when the data is not normal, skewed, or ordinal. Customer satisfaction surveys often use ranks or ratings, which are perfect for non-parametric statistics.\nSuitable for Small Samples: When you don‚Äôt have sufficient data to check for normality, you can use the non-parametric tests to analyze your data.\nNot Affected by Outliers: Means are sensitive to outliers, which makes any data with outliers not a good fit for parametric statistics. Non-parametric statistics use the median because they are more robust. For example, income data where a few billionaires distort the mean is better analyzed with medians.\nWork with different data types: You can use non-parametric statistics to analyze different types of data, such as interval, ratio, and ordinal."
  },
  {
    "objectID": "posts/Parametric vs Non-parametric Statistics/index.html#key-differences",
    "href": "posts/Parametric vs Non-parametric Statistics/index.html#key-differences",
    "title": "When Not To Use Parametric or Non-Parametric Statistics",
    "section": "Key Differences",
    "text": "Key Differences\nThe following table summarizes the major differences between the parametric and non-parametric statistics.\n\n\n\n\n\n\n\n\nAspect\nParametric\nNon-Parametric\n\n\n\n\nAssumptions\nRequires normal distribution & equality of variance\nMinimal assumptions about data\n\n\nData Type\nInterval/ratio\nOrdinal, nominal, or non-normal data\n\n\nExamples\nt-test, ANOVA, Pearson correlation\nMann-Whitney U, Kruskal-Wallis, Spearman correlation\n\n\nPower\nMore powerful if assumptions are met\nLess powerful but more flexible"
  },
  {
    "objectID": "posts/Parametric vs Non-parametric Statistics/index.html#when-to-use-which",
    "href": "posts/Parametric vs Non-parametric Statistics/index.html#when-to-use-which",
    "title": "When Not To Use Parametric or Non-Parametric Statistics",
    "section": "When to Use Which",
    "text": "When to Use Which\nA medical researcher interested in comparing the response of patients to a new intervention when compared to an old one might not have sufficient samples for the study.\nIn such a case, the best alternative is to go for a non-parametric test. Many such cases researchers face, which is one of the reasons for the development of non-parametric statistics.\nAlthough non-parametric statistics are not as strong as their parametric counterparts, they are useful in situations where the assumptions of parametric statistics are not met.\nAlmost all parametric tests have their non-parametric counterparts, and the image below summarizes when to use them.\n\n\n\nWhen to apply various statistical tests. Source: Understanding Parametric and Nonparametric Tests: A Comprehensive Guide"
  },
  {
    "objectID": "posts/Parametric vs Non-parametric Statistics/index.html#conclusion",
    "href": "posts/Parametric vs Non-parametric Statistics/index.html#conclusion",
    "title": "When Not To Use Parametric or Non-Parametric Statistics",
    "section": "Conclusion",
    "text": "Conclusion\nMany researchers, when analyzing their data, fail to check for the assumptions of the statistical test they use on their data, which can lead to wrong results.\nThese assumptions serve as a guide to determine the right statistical test to use based on the characteristics of the data.\nAlthough parametric tests are strict, non-parametric tests are flexible and are usually considered the last option when the data fails to meet the assumptions of parametric tests.\nUnderstanding the differences between parametric and non-parametric will ensure you achieve realistic results, making a lot of difference in your analysis.\n\nNeed Help with Data? Let‚Äôs Make It Simple.\nAt LearnData.xyz, we‚Äôre here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we‚Äôve got your back.\nüìß Shoot us an email at admin@learndata.xyz‚Äîlet‚Äôs chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/Parametric vs Non-parametric Statistics/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/Parametric vs Non-parametric Statistics/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "When Not To Use Parametric or Non-Parametric Statistics",
    "section": "Your next breakthrough could be one email away. Let‚Äôs make it happen!",
    "text": "Your next breakthrough could be one email away. Let‚Äôs make it happen!"
  },
  {
    "objectID": "posts/Securing ML APIs with FastAPI/index.html",
    "href": "posts/Securing ML APIs with FastAPI/index.html",
    "title": "Securing ML APIs with FastAPI",
    "section": "",
    "text": "The end goal of a machine learning model is to serve end users. Still, due to machine learning models requiring regular updates to improve model accuracy and use in other applications, they are exposed as an API. An ML API is an application that serves as a gateway between your client requests or needs and your machine learning model.\nLet‚Äôs say you have a recommender model on an e-library platform, that recommends books for users based on user preferences. This recommender model works as an API by getting user preferences and recommending books to the user. The API also makes it easy for you to use the recommender model on another platform.\nDue to the sensitivity of training data in machine learning models, API security is important to avoid data breaches and prevent malicious clients from accessing the model. In this article, I will show you how to secure your machine-learning APIs using FastAPI - an open-source Python framework that allows you to build secured and scalable APIs. As a Python library, the learning curve is low for data scientists and machine learning engineers with Python backgrounds. If you are new to FastAPI check out this course on ML deployment with FastAPI."
  },
  {
    "objectID": "posts/Securing ML APIs with FastAPI/index.html#fundamentals-of-api-security",
    "href": "posts/Securing ML APIs with FastAPI/index.html#fundamentals-of-api-security",
    "title": "Securing ML APIs with FastAPI",
    "section": "Fundamentals of API Security",
    "text": "Fundamentals of API Security\nAPI is usually a target for data breaches and unauthorized access due to the information it contains, making it prone to security attacks, this is why API security is important. API security is a practice set to protect an API from unauthorized access. Here are some of the most common API security threats:\n\nInjection attacks (SQL, command): In this type of attack, someone injects malicious code into the API, using SQL or terminal commands to read or modify the database. These kinds of attacks are usually targeted at the application‚Äôs database.\nCross-site scripting (XSS): This is another type of attack where a hacker manipulates a vulnerable site by sending malicious JavaScript to users, which upon execution by a user, the attacker can masquerade as the user and manipulate the user‚Äôs data.\nCross-site request forgery (CSRF): In this attack, attackers make users perform actions they don‚Äôt intend to do.\nMan-in-the-middle (MITM) attacks: In this attack, hackers eavesdrop between the interaction of clients and the API, to steal relevant credentials such as login details and credit card information.\n\nIn this article, you will learn how to solve these issues and make your machine-learning API secure."
  },
  {
    "objectID": "posts/Securing ML APIs with FastAPI/index.html#prerequisites",
    "href": "posts/Securing ML APIs with FastAPI/index.html#prerequisites",
    "title": "Securing ML APIs with FastAPI",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nHave basic knowledge of Python and FastAPI framework\nEnsure you have installed scikit-learn , fastapi , pydantic, uvicorn , numpy,and joblib libraries."
  },
  {
    "objectID": "posts/Securing ML APIs with FastAPI/index.html#setting-up-fastapi-for-ml-apis",
    "href": "posts/Securing ML APIs with FastAPI/index.html#setting-up-fastapi-for-ml-apis",
    "title": "Securing ML APIs with FastAPI",
    "section": "Setting Up FastAPI for ML APIs",
    "text": "Setting Up FastAPI for ML APIs\n\nCreate a project folder and a virtual environment.\nCopy and paste the following code into a new file called utilis.py in your project directory. This will create a classification model and a model.pkl file based on the iris dataset.\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nimport joblib\n\n# Load the iris dataset\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Train a random forest classifier\nmodel = RandomForestClassifier()\nmodel.fit(X, y)\n\n# Save the trained model\njoblib.dump(model, 'model.pkl')\nCreate an API endpoint for the machine-learning model in a file main.py .\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nimport joblib\n\n# Load the trained model\nmodel = joblib.load(\"model.pkl\")\n\n# Define the request body using Pydantic\nclass PredictionRequest(BaseModel):\n    sepal_length: float\n    sepal_width: float\n    petal_length: float\n    petal_width: float\n\napp = FastAPI()\n\n@app.post(\"/predict\")\ndef predict(request: PredictionRequest):\n    # Convert request data to a format suitable for the model\n    data = [\n        [\n            request.sepal_length,\n            request.sepal_width,\n            request.petal_length,\n            request.petal_width,\n        ]\n    ]\n    # Make a prediction\n    prediction = model.predict(data)\n    # Return the prediction as a response\n    return {\"prediction\": int(prediction[0])}\n\n# To run the app, use the command: uvicorn script_name:app --reload\n# where `script_name` is the name of your Python file (without the .py extension)\n\nWe now have our ML model API, let‚Äôs see how we can implement security best practices using this API."
  },
  {
    "objectID": "posts/Securing ML APIs with FastAPI/index.html#implementing-authentication-and-authorization",
    "href": "posts/Securing ML APIs with FastAPI/index.html#implementing-authentication-and-authorization",
    "title": "Securing ML APIs with FastAPI",
    "section": "Implementing Authentication and Authorization",
    "text": "Implementing Authentication and Authorization\nTake API authentication like a passkey that allows a client to access your API, allowing only authorized users to use the API. There are various ways of implementing API authentication in FastAPI, which you will learn subsequently.\n\n\n\nAuthentication vs Authorization. Source: Medium\n\n\nAPI authentication is insufficient to protect your API, you also need to implement API authorization. API authentication is like giving someone a key to your house, while API authorization is like giving them access to specific rooms in the house.\n\nAPI Key-Based Authentication\nThis is the most basic and popular form of implementing API security.\n\nTo implement key-based authentication in FastAPI, add the following code before the @app.post(\"/predict\") endpoint in main.py file.\n# Define the API key\nAPI_KEY = \"your_api_key_here\"\n\n# Dependency to verify the API key\ndef get_api_key(api_key: str = Header(...)):\n    if api_key != API_KEY:\n        raise HTTPException(status_code=403, detail=\"Could not validate credentials\")\n\nAPI_KEY is the variable that contains your environment key, which is supposed to be stored as an environment variable in a .env file.\nget_api_key() function gets the API_KEY and verifies if the provided API key matches what‚Äôs on the database. If successful, the user is granted access to the API, else an HTTP error 403 is raised, telling the user that the provided credential is invalid.\n\nNext, go to the predict() function and add api_key as an argument to get the api_key from users.\n@app.post(\"/predict\")\ndef predict(request: PredictionRequest, \n                        #added argument to get API key from user\n            api_key: str = Depends(get_api_key)):\n\nDepends function prevents access to the /predict endpoint without the API key.\n\n\n\n\nHow authentication works in FastAPI\n\n\n\n\n\nOAuth2 with JWT Tokens\nUnlike API keys, OAuth2 is an authorization protocol, granting clients access to resources hosted by other web applications on behalf of the user. With OAuth2, users do not need to give out their password to access a resource.\nA practical example is a client accessing your machine learning API using their Google ID without giving away their details, and your API in turn sends a token back to the client to serve as a temporary password for the client to access the API. It‚Äôs very secure compared to the API key. Unlike the API key which grants a user access to all resources in an API, OAuth2 only grants the client access to specified resources.\nWhen a user wants to access a machine learning API through a client application, the process typically uses OAuth2 for secure authentication. The client application starts by redirecting the user to an authentication server, where the user grants permission for the application to access their resources. The authentication server then issues an access token, often, in the form of a JWT (JSON Web Token) to the client application. The application uses this token to make requests to the machine learning API. The API verifies the token to ensure the client is authorized to access the requested resources, thus providing secure and controlled access while protecting user data and privacy.\n\n\n\nOAuth2 Workflow: Source: GeeksforGeeks\n\n\nLet‚Äôs implement a simple OAuth2 with JWT on our machine learning API, by updating the main.py file as follows.\n\nEnsure you install pyjwt and import the following Python libraries.\nfrom fastapi import FastAPI, Depends, HTTPException\nfrom fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm\nfrom pydantic import BaseModel\nimport joblib\nfrom typing import Optional\nimport jwt\n\nOAuth2PasswordBearer and OAuth2PasswordRequestForm are used to implement OAuth2 in FastAPI.\njwt is used to create a JSON Web Token.\n\nDefine the user model, to allow the user to provide a username and password using the BaseModel class.\n# Define a user model\nclass User(BaseModel):\n    username: str\n    password: str\nCreate a function to authenticate users.\ndef authenticate_user(username: str, password: str) -&gt; Optional[User]:\n    if username == \"admin\" and password == \"password\":\n        return User(username=username, password=password)\n    return None\n\nThe authenticate_user() function takes in a client username and password to see if it matches what‚Äôs in the database and returns a User model.\n\nCreate a SECRET_KEY variable to encode the JWT and create an oauth2_scheme\nSECRET_KEY = \"your-secret-key\"\n\n# OAuth2 scheme using password flow\noauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"token\")\nCreate a function to access the token using JWT.\ndef create_access_token(data: dict):\n    return jwt.encode(data, SECRET_KEY)\n\nThe create_access_token() function takes in the user details and encodes it with the SECRET_KEY\n\nCreate an authentication route to generate the access token.\n@app.post(\"/token\")\nasync def login_for_access_token(form_data: OAuth2PasswordRequestForm = Depends()):\n    user = authenticate_user(form_data.username, form_data.password)\n    if not user:\n        raise HTTPException(\n            status_code=401,\n            detail=\"Incorrect username or password\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n    access_token = create_access_token({\"sub\": user.username})\n    return {\"access_token\": access_token, \"token_type\": \"bearer\"}\n\nThe login_for_access_token()function takes the user inputs; username and password with the OAuth2 flow as an argument to return an access token to give the client application.\nIf user details are right, an access token is created and returned, else a 401 warning is returned\n\nProtect the API route that requires JWT authentication\n@app.post(\"/predict\")\nasync def predict(request: PredictionRequest, token: str = Depends(oauth2_scheme)):\n    try:\n        # Decode JWT token\n        payload = jwt.decode(token, SECRET_KEY, algorithms=[\"HS256\"])\n        # Convert request data to a format suitable for the model\n        data = [\n            [\n                request.sepal_length,\n                request.sepal_width,\n                request.petal_length,\n                request.petal_width,\n            ]\n        ]\n        # Make a prediction\n        prediction = model.predict(data)\n        # Return the prediction as a response\n        return {\"prediction\": int(prediction[0])}\n    except jwt.exceptions.DecodeError:\n        raise HTTPException(\n            status_code=401,\n            detail=\"Could not validate credentials\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n\nThe argument token: str = Depends(oauth2_scheme) means the API endpoint is protected using OAuth2, and receives the access token from the client application.\nThe token is decoded to see if it contains the SECRET_KEY, if it does, access is given to the model prediction, else a warning is given stating that the provided credentials are invalid.\n\n\n\n\nKey takeaways\n\nUser logs in and their data is encoded with a secret key to create an access token\nThe secured API endpoint decodes this access token to see if it contains the secret key before providing access to the resource.\n\n\n\nHow OAuth2 works in FastAPI.\n\n\nHow OAuth2 works in FastAPI.\n\n\n\nRole-Based Access Control (RBAC)\nRBAC is an approach where users are given various roles that provide access to specific API resources. It is an efficient way of ensuring API security, instead of granting all users privileges, users are granted privileges based on their needs in an API.\n\nLet‚Äôs implement an RBAC into the OAuth we created recently, by creating a dummy user data inside main.py.\n# Dummy user data\nusers_db = {\n    \"admin\": {\"username\": \"admin\", \"password\": \"password\", \"role\": \"admin\"},\n    \"user\": {\"username\": \"user\", \"password\": \"password\", \"role\": \"user\"}\n}\nTo demonstrate RBAC, admin will have access to our model prediction API endpoint while user will not have access to it. Update the User model to have a role field.\n# Define a user model with role\nclass User(BaseModel):\n    username: str\n    password: str\n    role: str\nJust before the API endpoint, add the following function\n# Role-based access control dependency\ndef role_checker(required_role: str):\n    def role_dependency(current_user: User = Depends(get_current_user)):\n        if current_user.role != required_role:\n            raise HTTPException(\n                status_code=403,\n                detail=\"Operation not permitted\",\n            )\n        return current_user\n    return role_dependency\n\nThe function role_checker() checks for the required role, by taking the required role admin as an argument.\nThe role_dependency() function checks if a user meets a required role, by taking the User as an argument.\nIf the user meets the required role, then the user is granted access, else a 403 status code is returned with a warning \"Operation not permitted\"\n\nUpdate the API endpoint by adding a user argument.\n@app.post(\"/predict\")\nasync def predict(request: PredictionRequest, \n                  token: str = Depends(oauth2_scheme), \n                  current_user: User = Depends(role_checker(\"admin\"))):\n\nThe current_user argument ensures that no User can access an API endpoint unless given permission.\n\n\n\nHow RBAC works in FastAPI."
  },
  {
    "objectID": "posts/Securing ML APIs with FastAPI/index.html#input-validation-and-sanitization",
    "href": "posts/Securing ML APIs with FastAPI/index.html#input-validation-and-sanitization",
    "title": "Securing ML APIs with FastAPI",
    "section": "Input Validation and Sanitization",
    "text": "Input Validation and Sanitization\nInput validation involves checking all inputs in an API to ensure that they meet certain requirements, while sanitization is input modification to ensure validity. Validation checks involve checking for allowed characters, length, format, and range, at the same time, sanitization is the changing of the input to ensure it is valid, such as shortening an input, or the removal of HTML tags in an input.\nInput validation and sanitization help to prevent common attacks like SQL injection and Cross-site scripting, most times you use input validation when your user is to give a particular input type, for example, a mobile number which is all digits. Sanitization is used when the user is expected to provide varying input types such as a user‚Äôs profile.\n\nUsing Pydantic for Input Validation\npydantic is a Python library that allows you to define and validate user inputs. It makes it easy to perform schema validation and serialization using type annotations. Earlier on, we used Pyndantic to validate our User and PredictionRequest.\nclass PredictionRequest(BaseModel):\n    sepal_length: float\n    sepal_width: float\n    petal_length: float\n    petal_width: float\n\nclass User(BaseModel):\n    username: str\n    password: str\n    role: str"
  },
  {
    "objectID": "posts/Securing ML APIs with FastAPI/index.html#securing-data-transmission",
    "href": "posts/Securing ML APIs with FastAPI/index.html#securing-data-transmission",
    "title": "Securing ML APIs with FastAPI",
    "section": "Securing Data Transmission",
    "text": "Securing Data Transmission\nWhen exchanging data between systems, it‚Äôs important to use data transmission protocols to secure and protect the data from unauthorized access. Data transmission security ensures that only authorized users can transmit data, and protect the system from vulnerabilities. There are various protocols one can force to keep data transmission secured such as HTTPS(Hypertext Transfer Protocol Secure), TLS(Transport Layer Security), SSH(Secure Shell), and FTPS(File Transfer Protocol Secure), we will only talk about HTTPS.\n\nEnforcing HTTPS\nHTTPS is a secured version of HTTP, where the data is encrypted when data is exchanged between a client and an API. Especially, when confidential details are shared such as user login credentials or account details. Unlike HTTP which has no security layer and makes data vulnerable, HTTPS adds an SSL/TLS layer to ensure that data is encrypted and secured.\n\n\n\nHTTPS workflow. Source: GeeksForGeeks\n\n\n\nTo secure data in the API endpoint we created earlier, let‚Äôs generate a self-signed certificate for testing. Copy and paste the following code into your terminal.\nopenssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -days 365 -nodes\nThis will generate a self-signed SSL/TLS certificate with a private key using OpenSSL.\n\nopenssl: This is the command-line tool for using the various cryptography functions of OpenSSL‚Äôs library.\nreq: This sub-command is used to create and process certificate requests (CSRs) and, in this case, to create a self-signed certificate.\nx509: This option is used to generate a self-signed certificate instead of a certificate request.\nnewkey rsa:4096: This option does two things:\n\nnewkey: It generates a new private key along with the certificate.\nrsa:4096: This specifies the type of key to create, in this case, an RSA key with a size of 4096 bits.\n\nkeyout key.pem: This specifies the file where the newly generated private key will be saved (key.pem).\nout cert.pem: This specifies the file where the self-signed certificate will be saved (cert.pem).\ndays 365: This sets the certificate to be valid for 365 days (1 year).\nnodes: This option ensures that the private key will not be encrypted with a passphrase. Without this option, OpenSSL would prompt for a passphrase to encrypt the private key.\n\nProvide the necessary information to create the key.pem (private key) and cert.pem (certificate).\n\n\n\nGenerating a self-signed certificate using OpenSSL.\n\n\nGenerating a self-signed certificate using OpenSSL.\nAt the end of the main.py file, add the following code.\nimport uvicorn \n\nif __name__ == \"__main__\":\n    uvicorn.run(\n        app, host=\"127.0.0.1\", port=8000, ssl_keyfile=\"key.pem\", ssl_certfile=\"cert.pem\"\n    )\nuvicorn.run ensures your application runs on HTTPS using the generated key.pem and cert.pem.\nYou can now run the API using the following code on your terminal\npython main.py\n\n\n\nURL to the Model API displayed on terminal\n\n\nIn a production environment, it is recommended to use a reverse proxy server like Nginx to handle SSL termination and forwarded requests to the FastAPI application, to ensure better performance and security.\n\n\n\nEncrypting Sensitive Data\nEncryption is simply the encoding of sensitive information, such that even if the information were to leak, the content is secured and remains unknown, upon reaching its target destination the data is decoded. This is very useful in protecting sensitive data such as passwords, and only authorized users can decrypt the information using a decryption key. Here is a simple example of how encryption works.\n\nImport all necessary libraries and create an instance of the FastAPI class.\nfrom fastapi import FastAPI, HTTPException, Depends\nfrom pydantic import BaseModel\nfrom cryptography.fernet import Fernet\n\napp = FastAPI()\nNext is to generate key for encryption and decryption using the Fernet class.\n# Generate a key for encryption and decryption\nkey = Fernet.generate_key()\ncipher_suite = Fernet(key)\nCreate an Item model for receiving a text, and the EncryptedItem model for receiving the encrypted text.\n# Models\nclass Item(BaseModel):\n    plaintext: str\n\nclass EncryptedItem(BaseModel):\n    ciphertext: str\nCreate the encryption endpoint.\n@app.post(\"/encrypt/\", response_model=EncryptedItem)\nasync def encrypt_item(item: Item):\n    plaintext = item.plaintext.encode(\"utf-8\")\n    ciphertext = cipher_suite.encrypt(plaintext)\n    return {\"ciphertext\": ciphertext.decode(\"utf-8\")}\nThis takes the given item and encodes it to utf-8 , the cipher_suite key encrypts the plaintext to ciphertext which is a string of gibberish characters.\nCreate the decryption endpoint that decrypts the gibberish characters to the plaintext.\n# Decryption endpoint\n@app.post(\"/decrypt/\", response_model=Item)\nasync def decrypt_item(encrypted_item: EncryptedItem):\n    ciphertext = encrypted_item.ciphertext.encode(\"utf-8\")\n    try:\n        plaintext = cipher_suite.decrypt(ciphertext)\n        return {\"plaintext\": plaintext.decode(\"utf-8\")}\n    except Exception as e:\n        raise HTTPException(status_code=400, detail=\"Decryption failed\")\nThis endpoint takes the encrypted_item and encodes it to utf-8 before decrypting it to plaintext using the cipher_suite function. If the wrong ciphertext is provided, a 400 status code is returned with the detail \"Decryption failed\".\n\n\n\nEncrypting sensitive data in FastAPI\n\n\n\n\n\nRate Limiting and Throttling\nAnother way of securing APIs is by limiting the number of API calls made to the server. This is where rate limiting and throttling comes into play. Rate limiting is a technique of controlling the amount of incoming and outgoing traffic to or from a network, to prevent abuse and overloading of the server. While throttling on the other hand is temporarily slowing down the rate at which the API processes requests. To apply rate limiting and throttling to our previous example.\n\nEnsure you have installed the slowapi library, a library for implementing rate-limiting and throttling to APIs, and add the following new imports.\nfrom slowapi import Limiter, _rate_limit_exceeded_handler\nfrom slowapi.util import get_remote_address\nfrom slowapi.errors import RateLimitExceeded\nNext is to initialize the rate limiter.\nlimiter = Limiter(key_func=get_remote_address)\napp = FastAPI()\napp.state.limiter = limiter\napp.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\nApply the rate limiter to the /token/ endpoint using @limiter.limit(\"5/minute\") decorator, and the request: Request parameter in the login_for_access_token function.\n@app.post(\"/token\")\n@limiter.limit(\"5/minute\")\nasync def login_for_access_token(request: Request, form_data: OAuth2PasswordRequestForm = Depends()):\nAlso, apply a 10-minute rate limiting to the /predict endpoint. Change the parameter name in the predict function from request to prediction_request to avoid confusion with the new request: Request parameter.\n@app.post(\"/predict\")\n@limiter.limit(\"10/minute\")\nasync def predict(\n    request: Request,\n    prediction_request: PredictionRequest,\n    token: str = Depends(oauth2_scheme),\n    current_user: User = Depends(role_checker(\"admin\"))"
  },
  {
    "objectID": "posts/Securing ML APIs with FastAPI/index.html#conclusion",
    "href": "posts/Securing ML APIs with FastAPI/index.html#conclusion",
    "title": "Securing ML APIs with FastAPI",
    "section": "Conclusion",
    "text": "Conclusion\nYou can combine all these methods in your ML Model API to ensure maximum security as much as possible. In this article, you have learned how to implement various API security techniques in your FastAPI model such as authentication, authorization, input validation, sanitization, encryption, rate limiting, and throttling. If you want to dive deep into model deployment with FastAPI, here are some extra resources to keep you busy.\n\nML Model Deployment with FastAPI and Streamlit\nHow to Build an Image Classifier Application on Vultr Using FastAPI and HuggingFace\nHow to Build a WhatsApp Image Generator Chatbot with DALL-E, Vonage and FastAPI\nBuild an SMS Spam Classifier Serverless Database with FaunaDB and FastAPI\nImplementing Rate Limits in FastAPI: A Step-by-Step Guide\nImplementing Logging in FastAPI Applications\nML - Deploy Machine Learning Models Using FastAPI\nDeploying and Hosting a Machine Learning Model with FastAPI and Heroku\n\n\nNeed Help with Data? Let‚Äôs Make It Simple.\nAt LearnData.xyz, we‚Äôre here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we‚Äôve got your back.\nüìß Shoot us an email at admin@learndata.xyz‚Äîlet‚Äôs chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/Securing ML APIs with FastAPI/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/Securing ML APIs with FastAPI/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "Securing ML APIs with FastAPI",
    "section": "Your next breakthrough could be one email away. Let‚Äôs make it happen!",
    "text": "Your next breakthrough could be one email away. Let‚Äôs make it happen!"
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#introduction-to-low-codeno-code-elt-tools",
    "href": "posts/The Engineer's Guide to Low Code/index.html#introduction-to-low-codeno-code-elt-tools",
    "title": "The Engineer‚Äôs Guide to Low Code/No Code ELT Tools",
    "section": "Introduction to Low-code/No-code ELT Tools",
    "text": "Introduction to Low-code/No-code ELT Tools\nLow-code/No-code tools are tools used in building applications using drag-and-drop components, reducing or eliminating the amount of code used in development. It provides an interactive graphical user interface, making it easy for non-technical users to start developing.\nLow-code tools help developers quickly get started, writing little or no code. It helps professional developers quickly deliver applications, letting them focus on the business side of the applications. Some features of low-code tools are:\n\nIt offers an interactive user interface with high-level functions, eliminating the need to write complex code.\nIt is easy to modify or adapt.\nMostly developed for a specific use case or audience\nIt is easy to scale.\n\nNo-code, on the other hand, is a method of developing applications that allows non-technical business users; business analysts, admin officers, small business owners, and others, to build applications without the need to write a single line of code. Some features of no-code tools are:\n\nPurely visual development, that is, users develop using drag-and-drop interfaces.\nIt offers limited customization, users use what is provided in the tool and can‚Äôt extend its capabilities.\nSuited for non-technical individuals\nMostly suited for simple use cases\n\nLow-code tools require some basic coding skills, unlike no-code which does not require any programming knowledge. Low-code/no-code is based on visual programming and automatic code generation. The emergence of low-code/no-code was the fact that domain experts know how difficult it is to impart to the IT team, with the help of low-code/no-code, they can take part in the development process, coupled with the fact that the shortage of skilled developers and loads of workload on the IT professionals is another reason for the emergence of low-code/no-code."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#understanding-the-elt-process",
    "href": "posts/The Engineer's Guide to Low Code/index.html#understanding-the-elt-process",
    "title": "The Engineer‚Äôs Guide to Low Code/No Code ELT Tools",
    "section": "Understanding the ELT Process",
    "text": "Understanding the ELT Process\nELT stands for Extract, Load and Transform, which are the three stages involved in the process.\nIt‚Äôs a data preprocessing technique that involves moving raw data from a source to a data storage area, either a data lake or a data warehouse. Sources are either social media platforms, streaming platforms or any other place data is stored. During extraction, data is copied in raw form from the source location to a staging area, this is either in a structured or unstructured format from sources such as:\n\nSQL or NoSQL servers\nText and document files\nEmail\nWeb pages\n\nExtraction is either full which involves pulling all rows and columns from a particular data source, using an automated partial extraction with update notifications when data is added to the system, or incremental where update records are extracted as data is added into the data source.\nLoading involves moving the data from the staging area to the data storage area, either a data lake or a data warehouse. This process is automated, continuous and done in batch. One can load all the available data in the data source to the data storage area, load modified data from the source between certain intervals or load data into the storage area in real time.\nIn the transform stage, a pre-written schema is run on the data using SQL for analysis. This stage involves filtering, removing duplications, currency conversions, removal of encryptions, joining data into tables or performing calculations.\nUnlike in ETL where raw data is transformed before being loaded into a destination source, in ELT the data is loaded into a destination source before it‚Äôs transformed for analysis as needed.\n\nBy allowing transformation to occur after loading, data is moved quickly to the destination for availability. Because data is transformed after arrival at the destination, ELT allows the data recipient to control data manipulation. This ensures that coding errors when transforming do not affect another stage. ELT uses the powerful big data warehouse and lakes allowing transformation and scalable computation.\nData warehouses use MPP architecture (Massively Parallel Processing), and data lake processes also support the application of schema or transformation models as soon as the data is received, this makes the process flexible, especially for large amounts of data. ELT is suited for data that are in cloud environments, this provides a seamless integration since ELT is cloud-native and allows for the continuous flow of data from sources to storage destinations, hence making them on demand.\nELT is used for instant access to huge volumes of data, for example in IOT, it loads data from IOT devices making it readily available for data scientists or analysts to access raw data and work collaboratively.\nDespite its advantages, ELT has some of its limitations:\n\nData privacy is a challenge in ELT, this is because when transferring data, a breach can occur from the source to the destination storage which poses security and privacy risks.\nWhile in transit, if care is not taken sensitive information is exposed, and extra security measures have to be taken to ensure the confidentiality of the data.\nELT handle large volumes of data, making it computationally intensive, leading to delays in gaining insights.\nBecause the data is loaded into the data storage area without transformation, it makes it challenging to transform the data when compared with ETL. This requires strong technical and domain knowledge of the data scientist and analyst who will write the queries transforming the data."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#low-codeno-code-elt-tools",
    "href": "posts/The Engineer's Guide to Low Code/index.html#low-codeno-code-elt-tools",
    "title": "The Engineer‚Äôs Guide to Low Code/No Code ELT Tools",
    "section": "Low-Code/No-Code ELT Tools",
    "text": "Low-Code/No-Code ELT Tools\nLow-code /No-code ELT are used to extract, load and transform data. Here are some of the benefits of using low-code/no-code ELT tools:\n\nIt is easier compared to writing scripts to automate the ELT process.\nIt makes development faster, developers can spend more time solving business problems instead of fixing bugs that result from writing lines of code.\nIt increases automation, a lot of processes that would have to be set up manually are handled automatically, such as monitoring, logging, setting notifications when there is a problem with the pipeline and so on.\nMost ELT tools support a lot of data connectors, making it easy for an organization to connect to any data source with provisions to create custom connectors.\nELT tools lower the cost of building an ELT pipeline by ensuring the whole process is conducted with a single tool from start to finish.\nThey provide better customer experience, by ensuring that even business folks are involved in building the ELT pipeline.\n\nThere are various low-code/no-code¬† ELT tools out there, each with its strengths and limitations, here are some you can consider for building an ELT pipeline:\n\nAirbyte\nAirbyte is an open-source data movement platform with over 300+ open-source structured and unstructured data sources. Airbyte is made up of two components; platform and connectors. The platform provides all the services required to configure and run data movement operations while the connectors are the modules that pull data from sources or push data to destinations. Airbyte also has a connector builder, a drag-and-drop interface and a low-code YAML format for building data source connectors.\nAirbyte has two plans, Cloud and Enterprise, but it is free to use if you can self-host the open-source version. Airbyte offers real-time and batch data synchronization, tracks the status of data sync jobs, monitors the data pipeline and views logs with a notifications system in case things go wrong. Airbyte also allows you to add custom transformations using dbt.\n\n\nFivetran\nFivetran is an automated data movement platform used for ELT. It offers automated data movement, transformations, security and governance with over 400+ no-code source connectors. You can use the function connector in Fivetran to write the cloud function to extract the data from your source, while it takes care of loading and processing the data into your destination. Fivetran gives you options to manage Fivetran connectors, to have more control over your data integration process runs. It offers a ‚Äúpay-as-you-use‚Äù model, with five Free pricing plans, Starter, Standard, Enterprise and Business Critical.\n\n\nIntegrate.io\nFormerly known as Xplenty, this is another low-code platform for data movement, unlike the others it doesn‚Äôt have many connections, it offers both low-code ETL, Reverse ETL and an ELT platform. Pricing on Integrate.io is based on data volume and increases as the number of rows in your data increases. It offers both Starter, Professional and Enterprise plans, with an extra charge for additional connectors.\n\n\nStitch\nStitch is also another data movement platform owned by Qlik. It replicates historical data from your database for free and allows you to add multiple user accounts across your organization to manage and authenticate data sources. It is extensible and has several hundreds of connectors. It offers various pricing models such as standard, advanced and premium which are all charged based on data volume.\n\n\nMatillion\nMatillion is another ELT platform, that uses LLM components to unlock unstructured data and offers custom connectors to build your connectors. It has a complete pushdown architecture supporting SQL, Python, LLMs, Snowflake, Databricks, AWS, Azure and Google. It supports both low-code and no-code for both programmers and business users. You can create an ELT process using either SQL, Python or dbt. It also gives you Auto-Documentation to generate pipeline documentation automatically. It offers three pricing models Basic, Advanced and Enterprise which you can pay only for pipelines run and not, those on development or sampling."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#key-features-of-low-codeno-code-elt-tools",
    "href": "posts/The Engineer's Guide to Low Code/index.html#key-features-of-low-codeno-code-elt-tools",
    "title": "The Engineer‚Äôs Guide to Low Code/No Code ELT Tools",
    "section": "Key Features of low-code/no-code ELT Tools",
    "text": "Key Features of low-code/no-code ELT Tools\n\nAvailability of Connectors\nELT connectors are components of an ELT tool that allow the tool to connect to a data source and make it possible for extraction and loading. When trying to go for ELT tools it is important to go for the ELT tool with the highest number of connectors, that is why an organization needs to have a list of all the data sources it uses, this will let the organization know the ELT tool to choose based on organizational data sources, most importantly connectors for revenue-generating applications. Let‚Äôs say your organization uses Zoho as a CRM. It‚Äôs important to compare various ELT tools with a connector for Zoho and see which offers the best service at the most affordable price.\n\n\nDrag-and-Drop Interfaces\nLow-code/no-code ELT tools offer an intuitive user interface with a drag-and-drop functionality, allowing non-technical users to perform ELT by dragging and dropping components without encountering any challenges. This makes the user experience seamless and users can focus on the application‚Äôs business logic. This reduces the workload on the IT team, allowing the organization‚Äôs domain experts to partake in the development process.\n\n\nAutomated Scheduling\nDue to their ability to schedule extracting and loading, automating the ELT process is very simple. This can involve creating tasks that can be run using a specified SQL schema at specific intervals. One can easily automate documentation, document process automation, and show the manipulations occurring to data from source to data storage, enabling organizations to save time and costs.\n\n\nData Transformation Capabilities\nLow-code/no-code ELT tools manage dependencies when transforming data, this is essential when you have multiple transforms depending on each other. They support the DAG(directed acyclic graph) workflow to manage the transformation dependencies after conducting a transform job using SQL on the data, reading the transformation query and calculating a dependency workflow.\n Another important aspect is their support for incremental loading, where only the differences in data are loaded from source to destination. For example Let‚Äôs consider the case of a retail store that tracks its sales data, without an incremental approach the daily sales report would involve extracting sales data from the sales table, which contains millions of records, aggregating the data to calculate the total sales, revenue and units sold for each product, store and day, and load the aggregated data into a new table called sales_daily.\nThis is a resource-intensive approach as the system needs to process all the sales data every time the report is generated. Using an incremental approach, whenever a new sale is recorded in the sales table, a trigger or a background process is used to update the sales_daily table with new data for that day and store.¬† Whenever every report is generated, only the data for the latest day is extracted from the sales_daily table, which is a much smaller dataset than the entire sales table. The incremental approach helps in improving performance, cost and scalability.\n\n\nMonitoring and Alerting\nMonitoring and Alerting is another important feature of a low-code/no-code ELT tool because It allows you to detect anomalies, bottlenecks and failures in the workflow, provide continuous surveillance and monitor resource utilization. Key important metrics it monitors are; latency, throughput, error rates, resource utilization and pipeline lag. They also give threshold alerts, detect anomalies, and escalate alerts to SMS or phone calls.\nImagine a manufacturing company that collects sensor data from various manufacturing plants across facilities. If analysis, reveals one machine is giving higher vibration levels than others. The ELT tool anomaly detection algorithm should trigger an alert which will prompt the maintenance team to investigate. They might identify a worn-out bearing component and schedule proactive maintenance, preventing equipment failure and uninterrupted production."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#evaluating-low-codeno-code-elt-tools",
    "href": "posts/The Engineer's Guide to Low Code/index.html#evaluating-low-codeno-code-elt-tools",
    "title": "The Engineer‚Äôs Guide to Low Code/No Code ELT Tools",
    "section": "Evaluating low-code/no-code ELT Tools",
    "text": "Evaluating low-code/no-code ELT Tools\nThere are various things to consider when choosing a low-code/no-code¬† ELT tool.\n\nConnectors: When selecting an ELT tool, ensure it supports connections to various data sources and know how many SAAS integrations are available and how effectively the tool can connect to organizational data sources.\nRoadmap: Another important factor, is if the ELT tool can handle the company‚Äôs rapidly growing data. Is it responsive and scalable? This will give the organization an idea of the ELT tool‚Äôs sustainability in the long run.\nPricing: How does the ELT tool charge? Is it by data flows or data volume and are the features it offers worth its pricing? Some ELT tools offer more connectors at affordable pricing than others.\nSupport: Look for an ELT tool with available customer support, this is very crucial especially when things break. The ELT tools should also offer good documentation that is easy to understand by technical and non-technical users. An online community around the tool is also a plus, users can relate with fellow users and serve as support for each other.\nSecurity: How does the ELT tool prioritise security? Are organizational data safe and is it regulatory compliant with GDPR, SOC2, HIPPA, and other relevant regulations? These are important security questions to look for when selecting an ELT tool. It is also important that the organization knows, how the tool handles privacy and authentication.\nEase of use: A low-code/no-code ELT tool that is user-friendly and easy to customize is another priority to look out for, it makes the process of creating ELT pipelines easy and non-technical for business folks.\nMaintenance: When choosing a low-code/no-code ELT tool, it‚Äôs important to know how easy it is to fix problematic data sources, and if it gives informative logs if an execution fails. It is also important to know what skills are required, by team members to keep the ELT process running smoothly."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#implementing-low-codeno-code-elt-tools",
    "href": "posts/The Engineer's Guide to Low Code/index.html#implementing-low-codeno-code-elt-tools",
    "title": "The Engineer‚Äôs Guide to Low Code/No Code ELT Tools",
    "section": "Implementing Low-code/No-code ELT Tools",
    "text": "Implementing Low-code/No-code ELT Tools\n\nPlanning the ELT Pipeline\nBefore building an ELT pipeline, you need to get the data from your source using an ELT tool like Airbyte and decide the data warehouse to use, either Google BigQuery, AWS Redshift or Snowflake. Next is to transform the data using dbt, R, Python or a no-code transformation layer such as Boltic, then consider the BI tool for presenting the data to end users.\n\n\nConfiguring Data Source and Destination Connections\nLet‚Äôs say for example using a REST API as a data source, Airbyte as the ELT tool and Amazon S3 bucks as the data destination. Create a new S3 bucket in the AWS console, in the bucket, create a folder to store data from the ELT operation in Airbyte. Create another folder in the previous folder to store the data you will extract from the REST API.\nNext, you will configure both the source and the destination connector, and connect the source and the destination. Ensure any API you use, there is a connector for it on Airbyte. If you don‚Äôt see a connector for your data source, use Airbyte‚Äôs CDK(Connector Development Kit) to create a custom connector.\nNext, you go to AWS S3 to configure the destination connector to connect Airbyte with the destination data lake, after successfully configuring both the source and the destination connector, and passing all the tests. You can now configure the ELT connection between source and destination.\n\n\n\nDesigning Data Transformation Workflows\nBefore transforming your data, you need to explore and understand it, this involves looking at your entity relationship diagrams to see how the data relate with each other, identifying missing values or misleading column names and performing a summary analysis of the data.\nWhile exploring, you might understand what is wrong with your data, and decide to perform your transformation process such as correcting misleading columns, renaming fields appropriately, adding business logic or joining data. The transformation you apply depends on what you have explored in the data. You can use tools like dbt, SQL, Python or R to transform your data or go no-code with tools like Boltic. At this stage,¬† test your data to meet business standards.\nFinally, you document your transformation process explaining the data model, key fields and metrics making the documentation easy for non-technical users to understand.\n\n\nScheduling and Automating ELT Processes\nBefore releasing the data to end users, you need to push it into a production environment in the data warehouse, these product-ready tables are what the BI analysis will query. With time you will need to update and refresh the data to meet the business needs, using a scheduler or orchestrator.¬† Using the job scheduler, you can use tools like dbt Cloud or Shipyard to create data transformations and tests within a collaborative IDE.\n\n\nMonitoring and Maintaining the ELT Pipeline\nMonitoring is important to identify bugs in data pipelines, optimise pipelines and gain valuable insights. These ELT tools provide visualisations, logging and observability systems to analyse latency, error rates and data throughput in your ELT pipeline. Most low-code/no-code provide all these out of the box, you will receive custom notifications when data problems occur allowing you to improve the quality of your data sets."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#best-practices-for-low-codeno-code-elt",
    "href": "posts/The Engineer's Guide to Low Code/index.html#best-practices-for-low-codeno-code-elt",
    "title": "The Engineer‚Äôs Guide to Low Code/No Code ELT Tools",
    "section": "Best Practices for low-code/no-code ELT",
    "text": "Best Practices for low-code/no-code ELT\n\nEnsuring Data Quality and Integrity\n\nData quality and requirements: The first step before conducting an ELT is to specify the data quality and requirements. These should specify the data accuracy, completeness, consistency, timeliness, validity, and uniqueness. This also includes details of the sources, end users and data quality metrics. This will help to understand the quality of data to expect and how to achieve it.\nValidation: The next step is to validate the data quality using the ELT tools before loading them into the warehouse, to reduce the amount of bad-quality data that gets into the data warehouse.\nData quality checks: In this step, ensure you use the ELT tools to implement quality checks on the data in the data warehouse before making it available to end users so that the data is consistent, complete and correct(3Cs).\nData quality monitoring and auditing: The next step is to monitor and audit the data quality as new data gets updated into the system, this is to resolve issues of data quality when they arise. ELT tools have various tools to get reports, alerts and logs on data quality. This ensures the successful maintenance of the data in the long run.\nData quality documentation and communication: Next is to give a report explaining to the end users or stakeholders the quality of the data, this report should contain the processes, rules, metrics and issues with the data. Doing this ensures trust and transparency of the data quality.\nReview and update the data quality: Constantly use the information from the ELT tool logging system to update the data quality, from time to time. By doing this, you are ensuring that the data remains relevant and meets the organization‚Äôs requirements.\n\n\n\nHandling Data Governance and Compliance\nIt is important to define clear roles and objectives for various stakeholders and use data governance tools to automate the tasks of data validation, cleansing, standardization, profiling, auditing, and reporting. Provide a data catalogue of the data assets and other data quality indicators and recommend a staging area for the source data before moving to the data warehouse. Finally, using a schema-on-read approach allows users to apply different views to the same data, based on their needs.\n\n\nIntegrating low-code/no-code ELT with Existing Systems\nDue to the visually driven approach of low-code/no-code ELT tools, you must thoroughly assess your existing data sources, formats and structures to identify potential compatibility issues and develop appropriate mapping strategies. Check, if a connector for your data source exists before building a custom connector.\n\n\nScaling and Optimizing ELT Workflows\nOne of the reasons for scaling and optimizing an ELT process is to reduce data loss and ELT run downtime. The following are ways to optimize an ELT workflow effectively:\n\nParallelize Data Flow: Simultaneous data flow saves more time than sequential data flow, you can load a group of unrelated tables from your data source to the data warehouse, by grouping the tables into a batch and running each batch simultaneously, instead of loading the tables one by one.\nApplying data quality checks: You should ensure each data table is tested with predefined checks such as schema-based to ensure that when a test fails, the data is rejected and when it passes, it produces an output.\nCreate Generic Pipelines: Generic pipelines ensure team members reuse a code without developing one from scratch. Practices like parameterizing the values can ease the job, when one wants to use a code/pipeline, they change the values of the parameters such as database connections.\nUse of streaming instead of batching: Streaming data from the source to the data destination is the best approach, this ensures the system is up to date whenever new data is added to allow end users access to recent data."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#future-of-low-codeno-code-elt",
    "href": "posts/The Engineer's Guide to Low Code/index.html#future-of-low-codeno-code-elt",
    "title": "The Engineer‚Äôs Guide to Low Code/No Code ELT Tools",
    "section": "Future of low-code/no-code ELT",
    "text": "Future of low-code/no-code ELT\nLow-code/no-code ELT tools have become popular, offering cloud-based options and reduced infrastructure costs. Most of them now have pre-built connectors covering most data sources and automated scheduling. Advanced low-code/no-code ELT tools now have data mapping and transformation capabilities using machine learning to detect data patterns and transformation. Some low-code/no-code tools now integrate data governance features such as profiling, data lineage tracking and automated data validation rules to ensure data integrity. These innovations aim to simplify data integration and empower non-technical users.\nEven with the innovation and simplicity low-code/no-code ELT tool offers, it has some of its limitations:\n\nLow-code/no-code ELT make it difficult to collaborate as a team, unlike a code-heavy pipeline with version control and collaborative features.\nSome low-code/no-code ELT tools give poor developer experience which can hamper productivity, such as bad and non-intuitive UI design and limited customization options.\nAnother challenge with low-code/no-code ELT is the security of the applications built on them, even though the platform is secure. This is an issue since the tool was developed for non-technical individuals with no expertise in security best practices.\nThey do not offer the source code to pipelines built and this is a challenge when an organization wants to migrate to another platform, they are forced to develop from scratch on another platform or stick with that same platform."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#conclusion",
    "href": "posts/The Engineer's Guide to Low Code/index.html#conclusion",
    "title": "The Engineer‚Äôs Guide to Low Code/No Code ELT Tools",
    "section": "Conclusion",
    "text": "Conclusion\nWhy learn data engineering since anyone can now use low-code/no-code tools? Data engineering is not just about building pipelines but also solving business problems such as designing schemas. It‚Äôs important that the data you give to your end users can answer their questions. In summary, developing your soft skills is more important to make you stand out.\nLow-code/no-code have a lot of abstraction which when things go wrong it‚Äôs difficult to find out, some organizations will not want to give out the control they have over their data pipeline by hiding away some of the pipeline core functionality using a low-code/no-code ELT tool."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#references",
    "href": "posts/The Engineer's Guide to Low Code/index.html#references",
    "title": "The Engineer‚Äôs Guide to Low Code/No Code ELT Tools",
    "section": "References",
    "text": "References\n\nUnleashing the power of ELT in AWS using Airbyte\nHow do you ensure data quality and governance in an ELT process?\n6 Best Practices to Scale and Optimize Data Pipelines\n\n\nNeed Help with Data? Let‚Äôs Make It Simple.\nAt LearnData.xyz, we‚Äôre here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we‚Äôve got your back.\nüìß Shoot us an email at admin@learndata.xyz‚Äîlet‚Äôs chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/The Engineer's Guide to Low Code/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "The Engineer‚Äôs Guide to Low Code/No Code ELT Tools",
    "section": "Your next breakthrough could be one email away. Let‚Äôs make it happen!",
    "text": "Your next breakthrough could be one email away. Let‚Äôs make it happen!"
  },
  {
    "objectID": "posts/Types of Missing Data MCAR, MAR, and MNAR Explained/index.html",
    "href": "posts/Types of Missing Data MCAR, MAR, and MNAR Explained/index.html",
    "title": "Types of Missing Data: MCAR, MAR, and MNAR Explained",
    "section": "",
    "text": "In the real world, it‚Äôs not always possible to have a complete dataset. For several reasons, cases might arrive where some observations will have missing values.\nUnderstanding the source of these missing values determines how you handle them. This article will teach you about the three types of missing values, their characteristics, statistical implications, and handling methods."
  },
  {
    "objectID": "posts/Types of Missing Data MCAR, MAR, and MNAR Explained/index.html#types-of-missing-data",
    "href": "posts/Types of Missing Data MCAR, MAR, and MNAR Explained/index.html#types-of-missing-data",
    "title": "Types of Missing Data: MCAR, MAR, and MNAR Explained",
    "section": "Types of Missing Data",
    "text": "Types of Missing Data\nA missing value refers to the absence of data in a dataset. It occurs when no observation is recorded for a particular variable. Missing values can arise for reasons such as survey errors, system failures, respondents skipping survey questions, and so on. There are various types of missing values;\n\nMissing Completely at Random (MCAR)\nMissing at Random (MAR)\nMissing Not at Random (MNAR)\n\n\nMissing Completely at Random (MCAR)\nMCAR refers to a scenario where the missing observations in a dataset are independent of the observed and unobserved data. This implies that the missingness is purely random and does not depend on any systematic factor related to the dataset.\nAn example is a survey in which a respondent skips a question due to a software glitch or a lab experiment in which test results are missing due to random equipment failure.\nMCAR can occur due to technical data collection issues, survey non-response unrelated to systematic reasons, or randomly lost or corrupted data without any pattern. MCAR does not introduce bias into a statistical analysis because it is a random occurrence.\nInferences still remain valid when data is MCAR, but the sample size is greatly reduced, which can lead to wider confidence intervals and lower power in hypothesis tests. Statistical tests like Little‚Äôs MCAR tests can help verify whether data is MCAR or not.\nSince MCAR does not introduce bias, you can use several methods to handle the missing values:\n\nListwise Deletion: Remove cases with missing values since the missing values are random.\nPairwise Deletion: Use the available data in calculations without deleting any observation.\nMean/Median/Mode Imputation: Replace missing values with the mean/median for numerical variables or mode for categorical variables.\nMultiple Imputation: Use statistical methods to estimate and replace missing values while accounting for uncertainty.\nMaximum Likelihood Estimation: A more advanced approach that estimates parameters directly using the likelihood function.\n\n\n\nMissing at Random (MAR)\nMAR is a situation in which the missing values are dependent on observed variables; that is, the probability of missingness depends on the observed data but not on the missing data itself.\nFor example, older patients might miss blood pressure readings more frequently. If age is recorded, the missingness depends on an observed variable (age) but not on the blood pressure values.\nIn MAR, the missing data is conditional on observed data, indicating it is random. This makes it easier for better statistical handling compared to Missing Not at Random (MNAR), which we will see in the next section.\nMAR can result from respondents skipping sensitive questions or non-response patterns influenced by observed data by education, age, or location.\nBecause one can predict MAR values based on observed data, you can use the following statistical methods to handle them:\n\nMultiple Imputation: Predicts missing values based on observed data.\nMaximum Likelihood Estimation (MLE): Estimate parameters without imputing missing values, just like it is common in regression and Structural Equation Modelling (SEM)\nWeighting Methods: Adjusts for missing data by assigning weights to observed data; this is often used in survey analysis.\n\n\n\nMissing Not at Random (MNAR)\nMNAR occurs when the probability of missing data depends on unobserved data. This indicates that the missing data is systematically related and not random.\nA typical example in a medical study is if patients with severe symptoms are more likely to drop out of a clinical trial and their severity is not recorded. This is a scenario of MNAR.\nConducting analysis on data with MNAR can lead to biased estimates since the missing values are not dependent on any of the observed values. MNAR also reduces the dataset‚Äôs representativeness since a significant portion is missing.\nUnlike MCAR and MAR, which one can handle using traditional imputation methods, such as mean or regression-based imputation, MNAR needs an advanced approach such as:\n\nModeling the Missing Data Mechanism: You can use external information to develop a model and estimate the missing values based on known relationships. This external information will convert the data from MNAR to MAR, where you can now use any of their respective handling methods to fill in the missing values.\nSensitivity Analysis: Conducting sensitivity analysis helps assess the impact of different assumptions about missing data.\nMultiple Imputation with MNAR Models: Using imputation techniques that account for MNAR patterns, such as Heckman selection or pattern-mixture models.\nUsing Domain Knowledge: Subject-matter expertise can guide adjustments and improve estimation techniques."
  },
  {
    "objectID": "posts/Types of Missing Data MCAR, MAR, and MNAR Explained/index.html#differences-between-mcar-mnar-and-mar",
    "href": "posts/Types of Missing Data MCAR, MAR, and MNAR Explained/index.html#differences-between-mcar-mnar-and-mar",
    "title": "Types of Missing Data: MCAR, MAR, and MNAR Explained",
    "section": "Differences between MCAR, MNAR, and MAR",
    "text": "Differences between MCAR, MNAR, and MAR\n\n\n\n\n\n\n\n\nMCAR\nMAR\nMNAR\n\n\n\n\nMissing values occur completely at random, unrelated to any data.\nMissing values depend on observed data but not on the missing values themselves.\nMissing values depend on the unobserved/missing data itself\n\n\nNo systematic pattern in missingness.\nSystematic pattern based on observed variables.\nSystematic pattern based on unobserved variables.\n\n\nLeast biased if missingness is handled properly.\nCan be handled using statistical methods like multiple imputation.\nCan lead to serious bias if not properly addressed.\n\n\nExample: A machine randomly fails to record a value.\nExample: Missing income data depends on age but not on income itself.\nExample: People with higher incomes are less likely to report earnings."
  },
  {
    "objectID": "posts/Types of Missing Data MCAR, MAR, and MNAR Explained/index.html#identifying-the-type-of-missing-data",
    "href": "posts/Types of Missing Data MCAR, MAR, and MNAR Explained/index.html#identifying-the-type-of-missing-data",
    "title": "Types of Missing Data: MCAR, MAR, and MNAR Explained",
    "section": "Identifying the Type of Missing Data",
    "text": "Identifying the Type of Missing Data\nIt‚Äôs important to know the type of missingness in a dataset to help you choose the right imputation technique. You can either use statistical tests or visualization to determine the nature of the missing data. In most cases, you can even combine the two methods to achieve better results.\n\nStatistical Tests to Determine Missing Data Mechanism\n\nLittles‚Äôs MCAR Test: This chi-square test checks if missingness is dependent on both observed and unobserved values. If the test fails, the null hypothesis is rejected, signifying that the data is MCAR. Otherwise, the missingness is either MAR or MNAR.\nt-Tests or ANOVA for MAR vs MCAR: Comparing observed values between missing and non-missing groups can help determine if missingness depends on available data.\nLogistic Regression of Missingness Indicators: You can use a ****binary missingness indicator (1 = missing, 0 = observed) to regress on observed variables. If the missingness is significantly related to observed data, it suggests MAR. If no relationships are found, it suggests MCAR.\n\n\n\nVisualizing Missing Data Patterns\n\nMissingness Matrix (Heatmap): You can use a statistical software to create a heatmap to display where values are missing in a dataset.\nBar Charts of Missing Values: Plotting the missing values per variable to show which variables are most affected.\nPairwise Missingness Scatterplots: Visualizing whether missing values in one variable correlate with missing values in another.\nPattern Plots (Upset Plots or Venn Diagrams): These show combinations of missing values in different variables and can help detect dependencies."
  },
  {
    "objectID": "posts/Types of Missing Data MCAR, MAR, and MNAR Explained/index.html#conclusion",
    "href": "posts/Types of Missing Data MCAR, MAR, and MNAR Explained/index.html#conclusion",
    "title": "Types of Missing Data: MCAR, MAR, and MNAR Explained",
    "section": "Conclusion",
    "text": "Conclusion\nData cleaning is a time consuming task, especially when it comes to handling missing values. But it‚Äôs important one treats them with caution. You can‚Äôt just discard them, they might introduce bias to your data, leading to wrong insights.\nIn this article, you learnt about the three types of missing values you are likely to encounter in a dataset, examples, their statistical implications, and how to handle them. You have also learned how to identify missing values statistically and visually."
  },
  {
    "objectID": "posts/Types of Missing Data MCAR, MAR, and MNAR Explained/index.html#resources",
    "href": "posts/Types of Missing Data MCAR, MAR, and MNAR Explained/index.html#resources",
    "title": "Types of Missing Data: MCAR, MAR, and MNAR Explained",
    "section": "Resources",
    "text": "Resources\n\n9 Python Libraries for Managing Missing Data Efficiently\nHandling Missing Data in R\n10 AI Data Analysis Tools That Will Redefine How You Make Decisions\n\n\nNeed Help with Data? Let‚Äôs Make It Simple.\nAt LearnData.xyz, we‚Äôre here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we‚Äôve got your back.\nüìß Shoot us an email at admin@learndata.xyz‚Äîlet‚Äôs chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/Types of Missing Data MCAR, MAR, and MNAR Explained/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/Types of Missing Data MCAR, MAR, and MNAR Explained/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "Types of Missing Data: MCAR, MAR, and MNAR Explained",
    "section": "Your next breakthrough could be one email away. Let‚Äôs make it happen!",
    "text": "Your next breakthrough could be one email away. Let‚Äôs make it happen!"
  },
  {
    "objectID": "posts/Z-Score vs IQR vs DBSCAN Choosing the Right Outlier Detection Method/index.html",
    "href": "posts/Z-Score vs IQR vs DBSCAN Choosing the Right Outlier Detection Method/index.html",
    "title": "Z-Score vs IQR vs DBSCAN: Choosing the Right Outlier Detection Method",
    "section": "",
    "text": "Let‚Äôs say the mathematics test results for 10 students are 2, 5, 6, 3, 4, 15, 5, 4, 3, 5, on a scale of 15. Looking at the score closely, you will notice that a particular student had a perfect score of 15, although the test performance was low. This kind of situation happens in data and is usually called outliers.\nOutliers are extreme values that can affect our data in various ways. For example, the student scoring fifteen or above will inflate the average score when calculated.\nThis is why it‚Äôs important to detect outliers when working with data. This article will teach you various outlier detection methods, use cases, pros and cons, and the right method to select when facing one."
  },
  {
    "objectID": "posts/Z-Score vs IQR vs DBSCAN Choosing the Right Outlier Detection Method/index.html#understanding-outliers",
    "href": "posts/Z-Score vs IQR vs DBSCAN Choosing the Right Outlier Detection Method/index.html#understanding-outliers",
    "title": "Z-Score vs IQR vs DBSCAN: Choosing the Right Outlier Detection Method",
    "section": "Understanding Outliers",
    "text": "Understanding Outliers\nOutliers can result from measurement errors or data variability. Their presence can skew statistical measures like the mean and standard deviation, affect model accuracy, lead to biased parameter estimates, or indicate important events such as fraud detection.\nThere are various types of outliers:\n\nGlobal Outliers (Point Anomalies): These data points significantly deviate from the rest of the dataset. For example, a person with a height of 2.5 meters in a general population dataset.\nContextual Outliers (Conditional Anomalies): These data points are called outliers in a specific context. An example is a temperature of 20¬∞C, which is normal in spring but abnormal in winter.\nCollective Outiers: These are a group of data points that collectively deviate from expected patterns, though individual points may not appear unusual. For example, if many requests suddenly come from the same IP address in a short time frame, it could indicate a bot attack.\n\nOutlier detection is necessary as it can provide more insights depending on the data type you are working with, such as fraud detection, medical diagnosis, network security, etc.\nIt‚Äôs also important to clean and process the data before applying statistical or machine learning models, as most models are sensitive to outliers. This ensures that the results are reliable and accurate."
  },
  {
    "objectID": "posts/Z-Score vs IQR vs DBSCAN Choosing the Right Outlier Detection Method/index.html#z-score-method",
    "href": "posts/Z-Score vs IQR vs DBSCAN Choosing the Right Outlier Detection Method/index.html#z-score-method",
    "title": "Z-Score vs IQR vs DBSCAN: Choosing the Right Outlier Detection Method",
    "section": "Z-Score Method",
    "text": "Z-Score Method\nThe Z-score, also known as the standard score, measures how deviated a value is from the mean(\\(\\mu\\)) of a dataset. It compares individual data points to the overall distribution and determines how unusual a value is within a dataset.\nOn the other hand, the standard deviation(\\(\\sigma\\)) is a measure of the dispersion or spread of a set of values. The formula for the Z-score is given as\n\\[\nZ = \\frac{X-\\mu}{\\sigma}\n\\]\nWhere:\n\nZ is the Z-score\nX is the individual data point\n\\(\\mu\\) is the mean of the dataset\n\\(\\sigma\\) is the standard deviation of the dataset.\n\nHere are the possible ways to interpret the Z-score:\n\nIf a point has a Z-score of 0, the data point is exactly at the mean.\nIf the Z-score is positive, the value is above the mean. For example, a Z-score of 2 means the data point is 2 standard deviations above the mean\nIf the Z-score is negative, the value is below the mean. For example, a Z-score of -1.5 means the data point is 1.5 standard deviations below the mean.\n\nThe Z-score works best when the sample size is large and is sensitive to unusual high or low values. It is mostly used in testing statistical hypotheses, confidence intervals, and regression analysis.\nIt also standardizes a variable, allowing for comparison regarding its measurement. For example, using their Z-score, you can compare two weight variables measured in pounds and kilograms.\nThe table below gives the pros and cons of the Z-score.\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nEasy to compare values from different datasets.\n\n\n\nHelps detects anomalies in the data.\nExtreme value can distort mean and standard deviation, which are needed to calculate the Z-score.\n\n\nMost effective with normally distributed data.\nNot useful for skewed or non-normal distributions, because it assumes normality.\n\n\nUseful for probability calculations.\nStatistical knowledge needed for interpretation."
  },
  {
    "objectID": "posts/Z-Score vs IQR vs DBSCAN Choosing the Right Outlier Detection Method/index.html#interquartile-range-iqr-method",
    "href": "posts/Z-Score vs IQR vs DBSCAN Choosing the Right Outlier Detection Method/index.html#interquartile-range-iqr-method",
    "title": "Z-Score vs IQR vs DBSCAN: Choosing the Right Outlier Detection Method",
    "section": "Interquartile Range (IQR) Method",
    "text": "Interquartile Range (IQR) Method\nThe IQR is a measure of spread that captures the middle 50% of a dataset. It helps detect outliers, and the boxplot is one of the best visualizations for explaining the IQR in a dataset.\n\n\n\nBoxplot. Image by SimplyPsychology\n\n\nThe boxplot is a visual representation of the distribution of a variable, and it uses five key statistics.\n\nMinimum (excluding outliers)\nFirst quartile (Q1) - 25th percentile (lower quartile)\nMedian (Q2) - 50th percentile (middle value)\nThird quartile (Q3) - 75th percentile (upper quartile)\nMaximum (excluding outliers)\n\nIt‚Äôs important to know that outliers are often represented as points outside the whiskers of the boxplot. The formula for the IQR is given as\n\\[\nIQR = Q3 - Q1\n\\]\nHere are the steps to follow to find outliers in your dataset using the IQR:\n\nSort the dataset in ascending order.\nFind Q1 and Q3\nCalculate IQR\nCompute the lower and upper bound levels using the following formula.\n\n\\[\nLower Bound = Q1 - 1.5*IQR\n\\]\n\\[\nUpper Bound = Q3 +1.5*IQR\n\\]\n\nIdentify outliers as data points not in the range between the lower and upper bound [Lower Bound, Upper Bound].\n\nThe IQR works well when the sample size is small, and the data is skewed. Here is a table showing the pros and cons of IQR:\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nNot sensitive to extreme values.\nIgnore the spread between Q1 and Q3, hence not capturing the full data distribution.\n\n\nWorks well with skewed data.\nYou can‚Äôt use it with normally distributed data.\n\n\nEasy to interpret.\nIn small datsets, IQR might misclassify important values."
  },
  {
    "objectID": "posts/Z-Score vs IQR vs DBSCAN Choosing the Right Outlier Detection Method/index.html#dbscan-density-based-spatial-clustering-of-applications-with-noise",
    "href": "posts/Z-Score vs IQR vs DBSCAN Choosing the Right Outlier Detection Method/index.html#dbscan-density-based-spatial-clustering-of-applications-with-noise",
    "title": "Z-Score vs IQR vs DBSCAN: Choosing the Right Outlier Detection Method",
    "section": "DBSCAN (Density-Based Spatial Clustering of Applications with Noise)",
    "text": "DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\nDBSCAN is a density-based clustering algorithm that groups data points based on their density. This makes it effective for identifying clusters of varying shapes and sizes and detecting outliers. DBSCAN groups data based on two parameters:\n\nEpsilon (\\(\\epsilon\\) ): The radius within which points are considered neighbors.\nMinPts (Minimum Points): The minimum number of points required within \\(\\epsilon\\) to form a dense region, that is, a cluster.\n\nDBSCAN classifies points into three categories:\n\nCore Points: Points with at least MinPts neighbors within distance \\(\\epsilon\\).\nBorder Points: Points with fewer than MinPts but within \\(\\epsilon\\) of a core point.\nNoise (Outliers): Points that do not belong to any cluster and are not close enough to a core point.\n\nThe image below explains the concept above:\n\n\n\nDBSCAN. Image by DataCamp.\n\n\nCore points (blue) form the center of clusters, border points (orange) are on the edges of clusters, and noise points (red) are isolated.\nDBSCAN identifies outliers as points that do not have enough neighboring points within to be core points. They are also unreachable from any other core point, existing in sparse regions with low density to form a cluster.\nSince DBSCAN does not force any point into a cluster, it naturally detects anomalies that don‚Äôt belong to any dense region.\nDBSCAN is particularly useful in spatial analysis. It can identify high-density areas such as traffic congestion zones and detect anomalies such as unusual weather patterns.\nHere are the pros and cons of using DBSCAN.\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nCan detect clusters of irregular shapes.\nIf clusters have varying densities, a single epsilon may not caputre all clusters accurately.\n\n\nDetermines the number of clusters automatically.\nSensitive to the epsilon and MinPts parameters, which if selected poorly can lead to incorrect clustering.\n\n\nEfficient when the sample size is large.\nPerforms poorly in high-dimensional spaces due to the curse of dimensionality."
  },
  {
    "objectID": "posts/Z-Score vs IQR vs DBSCAN Choosing the Right Outlier Detection Method/index.html#comparative-analysis",
    "href": "posts/Z-Score vs IQR vs DBSCAN Choosing the Right Outlier Detection Method/index.html#comparative-analysis",
    "title": "Z-Score vs IQR vs DBSCAN: Choosing the Right Outlier Detection Method",
    "section": "Comparative Analysis",
    "text": "Comparative Analysis\nThe table below compares the three outlier detection methods and the best scenario for using them.\n\n\n\n\n\n\n\n\nZ-Score\nIQR\nDBSCAN\n\n\n\n\nMeasures how many standard deviations a point is from the mean.\nIdentifies outliers based on the spread of the middle 50% of the data.\nDetects outliers as noise points in low-density regions.\n\n\nStatistical method\nStatistical Method\nDensity-Based clustering\n\n\nBest suited for normally distributed data.\nBest suited for non-parametric data.\nBest suited for non-linear distributions with varying densitiies.\n\n\nSuitable for small and medium datasets.\nWorks well for small datasets.\nBest for large, complex, and high-dimensional datasets.\n\n\nDoes not consider clusters.\nDoes not consider clusters.\nDetects noise points as outliers based on density.\n\n\nUse in financial fraud detection, and quality control.\nUsed in boxplot-based analysis like medical and biological data.\nUsed in anomaly detection in spatial or high-dimensional data."
  },
  {
    "objectID": "posts/Z-Score vs IQR vs DBSCAN Choosing the Right Outlier Detection Method/index.html#practical-examples",
    "href": "posts/Z-Score vs IQR vs DBSCAN Choosing the Right Outlier Detection Method/index.html#practical-examples",
    "title": "Z-Score vs IQR vs DBSCAN: Choosing the Right Outlier Detection Method",
    "section": "Practical Examples",
    "text": "Practical Examples\nHere are some case studies of how these outlier methods are applied in real life.\n\nExample 1: Financial Fraud Detection (Z-Score)\nA bank wants to detect fraudulent transactions based on transaction amounts. The Z-score method is applied since the transaction amounts typically follow a normal distribution.\nTransactions with a Z-score &gt;3 or &lt; -3 are flagged as potential fraud; hence, several high-value fraudulent transactions are detected.\n\n\nExample 2: Medical research on patient heights (IQR)\nMedical research analyzes height data to study growth patterns since the heights are often skewed. IQR is used to detect extremely short or tall patients\nHeights outside \\([Q1-1.5*IQR, Q3+1.5*IQR]\\) are flagged as outliers, and patients with these extreme values are now identified for further analysis.\n\n\nExample 3: Anomaly detection in GPS data (DBSCAN)\nA logistics company wants to identify erratic vehicle movement patterns. The GPS locations are clustered using DBSCAN, where vehicles that do not belong to any dense cluster are labeled anomalies. This ensures that vehicles deviating from usual routes are detected, helping identify potential theft or route violations."
  },
  {
    "objectID": "posts/Z-Score vs IQR vs DBSCAN Choosing the Right Outlier Detection Method/index.html#conclusion",
    "href": "posts/Z-Score vs IQR vs DBSCAN Choosing the Right Outlier Detection Method/index.html#conclusion",
    "title": "Z-Score vs IQR vs DBSCAN: Choosing the Right Outlier Detection Method",
    "section": "Conclusion",
    "text": "Conclusion\nEven though most statistical analyses are sensitive to outliers, outliers can significantly give you information about a variable in your dataset.\nThis article has explained how to handle an outlier in your dataset using the three methods discussed: Z-score, IQR, and DBSCAN. Each method has its pros and cons, and deciding which one to use depends on the nature of your dataset.\nIf you are looking for dirty datasets to practice on, you can create one using the Chaos web application that allows you to introduce various dirtiness into your dataset.\nHere are other tutorials also on outliers and data cleaning that you should find resourceful:\n\nTypes of Missing Data: MCAR, MAR, and MNAR Explained\nA Guide to the DBSCAN Clustering Algorithm\nDBSCAN Clustering in R Programming\nInterquartile Range and Quartile Deviation using NumPy and SciPy\nCalculate the Interquartile Range in R Programming ‚Äì IQR() Function\nHow to find z score in R-Easy Calculation-Quick Guide\nHow to Calculate Z-Scores in Python\n\n\nNeed Help with Data? Let‚Äôs Make It Simple.\nAt LearnData.xyz, we‚Äôre here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we‚Äôve got your back.\nüìß Shoot us an email at admin@learndata.xyz‚Äîlet‚Äôs chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/Z-Score vs IQR vs DBSCAN Choosing the Right Outlier Detection Method/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/Z-Score vs IQR vs DBSCAN Choosing the Right Outlier Detection Method/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "Z-Score vs IQR vs DBSCAN: Choosing the Right Outlier Detection Method",
    "section": "Your next breakthrough could be one email away. Let‚Äôs make it happen!",
    "text": "Your next breakthrough could be one email away. Let‚Äôs make it happen!"
  },
  {
    "objectID": "reports/Uber Eats Google Playstore Report 2025/index.html",
    "href": "reports/Uber Eats Google Playstore Report 2025/index.html",
    "title": "Uber Eats Google Playstore Report 2025",
    "section": "",
    "text": "Table of Contents\nExecutive Summary\nIntroduction\nUber Eats App Overview\nRatings Performance in 2025\nSentiment Analysis\nNegative Sentiment Drivers\nRecommendations\nContact"
  },
  {
    "objectID": "reports/Uber Eats Google Playstore Report 2025/index.html#executive-summary",
    "href": "reports/Uber Eats Google Playstore Report 2025/index.html#executive-summary",
    "title": "Uber Eats Google Playstore Report 2025",
    "section": "Executive Summary",
    "text": "Executive Summary\nIn 2025, Uber Eats experienced a significant decline in its Google Play Store performance. The proportion of positive reviews fell from 54.7% in 2024 to just 39.0% in 2025, marking a steep drop in customer satisfaction within a single year. Correspondingly, the average app rating decreased to 2.66 stars, with 56.6% of total reviews classified as negative.\nA detailed analysis of review content revealed that most negative feedback focused on technical and service-related issues. Customers also frequently expressed frustration with slow or unresponsive customer support, which emerged as a recurring theme across reviews; 81.8% of users explicitly requested faster and more effective assistance.\nConversely, positive reviews consistently emphasized Uber Eats‚Äô convenience, user-friendly interface, and reliability of delivery in certain regions, suggesting that the platform continues to deliver value when operational performance meets expectations.\nGiven these insights, our report recommends that Uber Eats prioritize enhancements to its customer support infrastructure, including faster response times, clearer communication channels, and improved issue resolution processes. In addition, optimizing app stability and refining delivery operations could further help restore consumer trust and improve overall satisfaction on the platform."
  },
  {
    "objectID": "reports/Uber Eats Google Playstore Report 2025/index.html#introduction",
    "href": "reports/Uber Eats Google Playstore Report 2025/index.html#introduction",
    "title": "Uber Eats Google Playstore Report 2025",
    "section": "Introduction",
    "text": "Introduction\nThe purpose of this report is to deliver a comprehensive analysis of Uber Eats‚Äô performance on the Google Play Store, drawing insights from large-scale user review data. This report is designed for Uber Eats product managers, marketing teams, investors, and industry competitors who are seeking actionable intelligence on user sentiment, app performance, and overall market positioning.\nA total of 95,633 reviews were collected between January 2024 and September 2025. All data was anonymized, and advanced sentiment analysis techniques were applied to identify recurring patterns in user feedback, including both complaints and positive experiences.\nWe believe that user ratings and reviews on the Google Play Store provide a direct reflection of customer sentiment and are a reliable proxy for measuring product-market fit, user satisfaction, and retention potential. This report categorizes user feedback into key areas where customers believe the app requires improvement, such as usability, reliability, and customer support, as well as areas where users expressed strong satisfaction, underscoring the app‚Äôs value proposition.\nApp store reviews are particularly critical for companies whose primary offering is a mobile application, as they influence download decisions, retention rates, and overall brand reputation. Accordingly, this report offers a strategic snapshot of Uber Eats‚Äô market position in 2025, with a specific focus on its Google Play Store ecosystem performance. By highlighting key metrics, recurring pain points, and strengths, the analysis provides valuable insights into Uber Eats‚Äô competitive standing and identifies opportunities for future growth and product optimization in the fast-evolving on-demand food delivery sector."
  },
  {
    "objectID": "reports/Uber Eats Google Playstore Report 2025/index.html#uber-eats-app-overview",
    "href": "reports/Uber Eats Google Playstore Report 2025/index.html#uber-eats-app-overview",
    "title": "Uber Eats Google Playstore Report 2025",
    "section": "Uber Eats App Overview",
    "text": "Uber Eats App Overview\nThe following table gives a snapshot of Uber Eats market position (2025 revenue, countries active, competition).\n\n\n\nMetric\nValue / Estimate\n\n\n\n\n2024 Revenue\n~$13.7 billion globally.\n\n\nGross Bookings (2024)\n~$74.6 billion.\n\n\nNumber of Users (Global, early 2025)\n~95 million.\n\n\nNumber of Restaurants / Merchants\nOver 1 million restaurant partners, in ~11,500 cities.\n\n\nGeographic Reach\nOperates in 45+ countries and over 6,000 cities (global footprint across Asia-Pacific, North & Latin America, Europe, Africa).\n\n\nTotal downloads (Play Store)\n100 million +\n\n\nAverage Rating\n4.8"
  },
  {
    "objectID": "reports/Uber Eats Google Playstore Report 2025/index.html#ratings-performance-in-2025",
    "href": "reports/Uber Eats Google Playstore Report 2025/index.html#ratings-performance-in-2025",
    "title": "Uber Eats Google Playstore Report 2025",
    "section": "Ratings Performance in 2025",
    "text": "Ratings Performance in 2025\n\nMonthly Trend Analysis\nUber Eats ratings faced a drastic drop in February 2025, but went back up the following month. Their current highest average monthly rating is on Septemeber, which is a possible sign of improvements on the application from customers‚Äô reviews.\n\n\n\n\nQuarterly Summary\nUber Eats experienced a drop in positive ratings across all quarters.\n\n\n\nUber Eats 2024 vs.¬†2025 Ratings Performance\nCompared to 2024, Uber Eats‚Äô average positive review ratings dropped from 54.7% to 39%, while their average negative review ratings increased from 41.2% to 56.0%."
  },
  {
    "objectID": "reports/Uber Eats Google Playstore Report 2025/index.html#sentiment-analysis",
    "href": "reports/Uber Eats Google Playstore Report 2025/index.html#sentiment-analysis",
    "title": "Uber Eats Google Playstore Report 2025",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\n\nOverall Sentiment Breakdown\nNegative reviews from users were the highest by 56%.\n\nThe word cloud suggests that most reviews were regarding the service, orders, and the application.\n\n\n\nNegative Sentiment Drivers\n25.4% of the negative reviews were a result of complaints on the Uber Eats application, and also account issues.\n\n\n\nPositive Sentiment Drivers\n33.3% of reviewers praised the Uber Eats application for its ease of use and convenience.\n\n\n\nFeature Request Insights\nThe most desired feature from users is faster customer support."
  },
  {
    "objectID": "reports/Uber Eats Google Playstore Report 2025/index.html#recommendations",
    "href": "reports/Uber Eats Google Playstore Report 2025/index.html#recommendations",
    "title": "Uber Eats Google Playstore Report 2025",
    "section": "Recommendations",
    "text": "Recommendations\nBased on the analysis of Uber Eats customer reviews, several key insights emerge across complaints, feature requests, and praise. The most frequent issues raised by users relate to app and account problems, which represent 25.4% of all complaints. This suggests, Uber Eats should prioritize improving the stability and reliability of its platform. Similarly, driver and app support issues account for 22% of complaints, highlighting the need for more efficient communication channels between drivers and customers. Real-time chat, clearer delivery status updates, and responsive technical support would help resolve these concerns effectively.\nDelivery and cancellation issues (21.3%) are another major source of dissatisfaction. Many users likely experience inconvenience when orders are delayed or canceled without clear explanations. Uber Eats could address this by offering proactive notifications, transparent refund processes, and possibly compensation mechanisms for service disruptions. Food and billing problems, which make up 20.8% of complaints, indicate a need for greater accuracy in menu listings and pricing, as well as more streamlined billing dispute resolutions. Customer service problems, although representing a smaller proportion (10.4%), are closely tied to user dissatisfaction overall and should not be overlooked. Faster, more empathetic, and accessible customer service would directly address both complaints and feature requests.\nFeature requests reinforce this need for improvement in customer support, with an overwhelming 81.8% of users asking for faster response times. Uber Eats could greatly benefit from expanding 24/7 live support options, integrating AI-assisted help desks, and providing users with estimated response times. Other frequently requested features, such as better restaurant filtering options (7.6%) and scheduled delivery (6.7%), indicate that users value personalization and flexibility in their ordering experience. Although dark mode (3.9%) ranks lowest, it remains a simple enhancement that could improve user comfort and satisfaction, particularly for late-night users.\nOn the positive side, Uber Eats receives significant praise for its ease and convenience, which accounts for 33.3% of all positive reviews. This demonstrates that users appreciate the platform‚Äôs simplicity and speed, and these strengths should remain a focal point in its marketing and product design. Service quality and reliability (18.4%) and food quality and enjoyment (17.9%) are also key contributors to positive sentiment, reflecting well on the company‚Äôs partnerships with restaurants and delivery operations. Additionally, users highlight positive experiences with the app and platform (16.1%) as well as brand trust and appreciation (14.3%), suggesting a generally strong perception of reliability and credibility.\nOverall, Uber Eats should continue to leverage its strengths in convenience and user trust while addressing recurring frustrations around technical reliability and customer support. By investing in faster, more efficient support systems, improving app performance, and introducing user-requested features such as scheduling and better filtering, the company can enhance satisfaction, reduce churn, and strengthen its competitive advantage."
  },
  {
    "objectID": "reports/Uber Eats Google Playstore Report 2025/index.html#contact",
    "href": "reports/Uber Eats Google Playstore Report 2025/index.html#contact",
    "title": "Uber Eats Google Playstore Report 2025",
    "section": "Contact",
    "text": "Contact\nInquiries and correspondence concerning this report should be directed to:\nAdejumo Ridwan Suleiman\nEditor-in-Chief\nEmail: admin@learndata.xyz\nPhone: +234 703 285 7263"
  },
  {
    "objectID": "visuals/daily_crude_price_trend/index.html",
    "href": "visuals/daily_crude_price_trend/index.html",
    "title": "Daily Nigeria Crude Oil Prices (2009 - 2025)",
    "section": "",
    "text": "Data Source: Central Bank of Nigeria\nRead the full report on here.\nClick here for more visuals."
  },
  {
    "objectID": "posts/10 AI Data Analysis Tools That Will Redefine How You Make Decisions/index.html",
    "href": "posts/10 AI Data Analysis Tools That Will Redefine How You Make Decisions/index.html",
    "title": "10 AI Data Analysis Tools That Will Redefine How You Make Decisions",
    "section": "",
    "text": "With the rapid rise of AI in everyday workflows, many routine tasks have become faster and more efficient; from note-taking and summarization to web scraping and automation.\nAI now plays an essential role in saving time, and allowing individuals and businesses to focus on what truly matters. Its impact continues to expand across industries, providing accessible solutions to challenges that once required significant manual effort.\nData analysis is no exception. Modern AI tools can now analyze datasets, uncover patterns, and generate meaningful insights with impressive accuracy. And this isn‚Äôt limited to simple summaries, AI is capable of performing real, in-depth analysis, including statistical modeling, data visualization, forecasting, and even machine learning. What once required specialized knowledge, you can now achieve with a few prompts.\nFor small businesses, this means they no longer need to hire a dedicated data analyst to understand their numbers. Larger organizations can also reduce repetitive analytical workload and free their data teams for higher-value tasks. By simply uploading or connecting your data into these tools, you can receive clear, actionable insights that drive better decisions and improved outcomes.\nIn this article, you‚Äôll discover a variety of AI tools currently available for data analysis, how each one works, and the types of use cases they‚Äôre best suited for. I‚Äôve personally tested these tools, and they‚Äôre already being used by many businesses worldwide. My goal is to introduce you to them so you can explore their potential and see how they can streamline your own data workflows."
  },
  {
    "objectID": "posts/10 AI Data Analysis Tools That Will Redefine How You Make Decisions/index.html#best-ai-data-analysis-tools-to-try",
    "href": "posts/10 AI Data Analysis Tools That Will Redefine How You Make Decisions/index.html#best-ai-data-analysis-tools-to-try",
    "title": "10 AI Data Analysis Tools That Will Redefine How You Make Decisions",
    "section": "Best AI Data Analysis Tools to Try",
    "text": "Best AI Data Analysis Tools to Try\nHere are some the AI Data Analysis tools you should try;\n\n1. ChatGPT\nOne of the most widely recognized AI tools today is ChatGPT, known for sparking much of the modern AI revolution. Recently, ChatGPT introduced a feature that allows users to upload data files and perform analysis directly within the chat interface.\nThe system automatically generates Python code to process your data and provides clear interpretations of the results, making data analysis accessible even to users without technical backgrounds.\nTo get started, simply click the plus icon in the interface and upload your file, whether it‚Äôs a CSV or JSON. Once the file is added, you can ask any question about the data, such as identifying trends, summarizing statistics, or creating visualizations. ChatGPT handles the underlying code and computation, allowing you to focus solely on the insights.\nHowever, the tool does come with certain limitations. It currently doesn‚Äôt support direct connections to databases, data warehouses, or other large-scale storage systems. This means it‚Äôs best suited for smaller datasets, such as Spreadsheets, survey responses, or exported reports. Despite these constraints, ChatGPT remains an excellent option for quick, intuitive, and on-the-fly analysis of manageable datasets.\n\n\n\n2. LearnFacebookAdsInsights\nAlthough this tool is designed for a very specific use case; analyzing Facebook Ads Insights, it remains highly valuable, especially for marketing agencies and e-commerce businesses.\nIts focused purpose makes it exceptionally powerful for extracting meaningful insights from Facebook advertising data, helping teams understand campaign performance, optimize budgets, and drive better ROI.\nWith LearnFacebookAdsInsights, users can seamlessly pull Facebook Ads data directly into Google Sheets, making it easy to integrate insights into existing reporting workflows.\nFrom there, the platform can connect to various dashboards or be analyzed through its built-in AI Chat Assistant. This assistant can generate summary tables, create visualizations, and perform both simple and advanced statistical analysis, giving marketers a deeper understanding of trends and performance drivers.\nThe main limitation of the tool is its narrow scope: it is designed exclusively for Facebook Ads Insights. This means it cannot be used to analyze other data sources. However, for teams whose core focus is optimizing Facebook ad campaigns, LearnFacebookAdsInsights remains a highly effective and specialized solution.\n\n\n\n3. Julius AI\nJulius AI is a powerful analytics platform renowned for its accuracy in both standard data analysis and advanced statistical modeling. It has become a popular choice among researchers, academics, and data professionals worldwide due to its reliability and depth of analytical capabilities.\nIf you‚Äôre conducting exploratory analysis, running complex regressions, or working with experimental data, Julius AI provides precise and transparent results.\nThe platform offers a variety of connectors, making it easy to integrate your data from multiple sources. Even on its free tier, Julius AI allows you to connect to Google Drive and send up to 15 messages per month at no cost. This makes it accessible for students, small teams, and individuals who need robust analytical tools without committing to a paid plan.\nIts design focuses heavily on clarity: alongside the results, Julius AI provides the exact Python or R code used in the analysis, helping users learn, verify methods, or reproduce findings.\nJulius AI also supports the creation of custom AI agents, enabling users to automate recurring tasks and optimize their workflows. It integrates seamlessly with major big-data platforms such as Google BigQuery, Snowflake, and other enterprise data warehouses.\nCollaboration is another key strength of the platform, its real-time, multi-user editing allows teams to work together on shared analytical projects. You can also push insights directly to Slack channels, making it easy to keep stakeholders informed and maintain alignment across teams.\n\n\n\n4. Bag of Words\nBag of Words (BOW) is an AI-powered analytics tool that enables users to build BI dashboards simply by using natural language prompts. Instead of manually configuring charts or writing SQL queries, you can describe the insights you want, and BOW automatically generates visualizations.\nUsers can drag, resize, and arrange these visuals on the dashboard, offering a flexible and interactive way to explore your data. Users can also easily add or remove visual components, making it simple to tailor dashboards to evolving business needs.\nBOW supports a wide range of data sources, including Salesforce, DBT, Postgres, and many others. This broad integration capability allows teams to connect their existing workflows and unify data from multiple systems.\nAdditionally, BOW enables seamless collaboration by allowing you to send insights directly to Slack and share dashboards with colleagues. This ensures insights are easily accessible and encourages data-driven decision-making across the organization.\nAs an open-source platform, BOW gives organizations full control over deployment and customization. You can run it locally or in your own cloud environment, as long as you provide valid API keys from services such as Google, OpenAI, Azure, or Anthropic. This flexibility makes BOW suitable for teams that want the power of AI-driven analytics while maintaining transparency, security, and ownership of their data infrastructure.\n\n\n\n5. PandasAI\nWhile Pandas remains one of the most popular libraries for data wrangling and analysis in Python, PandasAI is an open-source extension that simplifies the analysis process even further by integrating natural language and AI-powered capabilities directly into Pandas workflows.\nWith PandasAI, users don‚Äôt need to write extensive boilerplate code to explore or analyze a DataFrame. Instead, they can provide a plain-language prompt, attach their dataset, and execute commands that automatically generate results, insights, or even visualizations.\nPandasAI is particularly well-suited for developers, data scientists, and analysts who want to introduce AI-driven analysis into their existing data pipelines without replacing their current toolset.\nIn addition to working with Pandas DataFrames, it also allows users to connect to databases, work with external data sources via extensions, and even build custom AI agents for more advanced or repeatable analytics tasks. This flexibility makes it a powerful addition for teams seeking intelligent automation in data exploration, reporting, and decision-making processes.\nHowever, one notable limitation is that PandasAI still requires some level of coding knowledge to use effectively. Users must be comfortable working in a Python environment and managing dependencies in order to benefit from its full capabilities.\nThat said, developers can easily build applications, scripts, or user interfaces on top of PandasAI, making its functionality more accessible to non-technical users such as business analysts, product managers, or marketers through simplified dashboards or internal tools.\nOverall, PandasAI bridges the gap between traditional programmatic data analysis and modern AI-assisted workflows, making it a valuable tool for anyone who wants to combine the power of Pandas with the efficiency of natural language interaction.\n\n\n\n6. Annie\nAnnie; developed by the creators of PandasAI, is designed to make dashboard creation simple and intuitive for businesses. It supports a wide range of data sources, including Google Sheets, Google Analytics, HubSpot, and many others, allowing users to consolidate insights from multiple platforms into clean, interactive dashboards. Its user-friendly interface makes it accessible even for teams without deep technical expertise.\nUpon signing up, Annie provides 100 free credits, which allows you to create up to ten dashboards. This trial capacity is generally enough to evaluate whether the platform fits your workflow and analytical needs. While PandasAI is open source, Annie itself is not, which may be a concern for organizations that require full transparency or prefer self-hosted, fully auditable solutions.\nBeyond dashboard creation, Annie also enables conversational data analysis. Users can chat directly with their data, ask questions, generate insights, and push important findings to Slack for team visibility. This combination of dashboarding, AI-assisted exploration, and collaboration features makes Annie a versatile tool for modern data-driven teams.\n\n\n\n7. Vertex AI\nVertex AI is Google Cloud‚Äôs unified artificial intelligence and machine learning platform, designed to streamline the entire ML lifecycle; from data preparation and training to deployment and monitoring.\nOne of its most powerful features is its support for AutoML, which allows users to automatically build high-quality custom machine learning models without extensive coding or deep ML expertise.\nBy leveraging Google‚Äôs large-scale computing infrastructure, Vertex AI can efficiently process massive datasets and search through a wide range of model architectures to identify the most effective solution for your specific problem.\nBeyond automated model selection, Vertex AI also provides tools for hyperparameter tuning, model evaluation and explanation. These capabilities help teams better understand how their models make predictions and ensure that the final output is both accurate and trustworthy.\nDevelopers can take advantage of built-in dashboards, experiment tracking, and integrated versioning to maintain clear oversight over the entire modeling process. This makes Vertex AI suitable not only for rapid prototyping but also for enterprise-level machine learning pipelines.\nAnother advantage of Vertex AI is its seamless integration with the rest of the Google Cloud ecosystem. If you‚Äôre pulling data from BigQuery, orchestrating pipelines with Vertex Pipelines, or deploying models to real-time endpoints, the platform offers an end-to-end workflow with strong scalability and reliability. This allows organizations to move from raw data to production-ready ML systems more quickly and efficiently, reducing the operational burden while maximizing performance. Ultimately, Vertex AI empowers teams to build, manage, and deploy machine learning models at scale with significantly less manual effort.\n\n\n\n8. Sigma AI Browser\nAlthough Sigma AI functions as a browser, it is far more than a traditional web tool, it is an AI-powered, agentic browser designed to both gather and analyze data.\nUnlike other analytics platforms, Sigma AI can actively scrape information from websites, allowing it to serve as both a data source and a data analysis tool. This makes it particularly useful for users who need to collect real-time information from the web before running their analysis.\nSigma AI also supports connecting to various cloud storage solutions, enabling users to import and clean their existing datasets, conduct analysis, and generate comprehensive reports, all within the same environment. This end-to-end workflow provides a streamlined experience for handling small to medium-sized data projects without switching between multiple tools.\nHowever, its limitations become clear when dealing with large-scale data. Sigma AI does not support connections to major big-data platforms or data warehouses, which restricts its use to smaller datasets and lightweight analytic tasks. Despite this, it remains a powerful option for quick analysis, web scraping needs, and compact reporting workflows.\n\n\n\n9. Fellou.ai\nFellou is another powerful agentic browser designed to help users automate data collection and analysis at scale. It can scrape and aggregate information from a wide range of online sources, including product reviews, blog posts, news sites, forums, and social media platforms.\nOnce the data is gathered, Fellou allows you to organize, visualize, and interpret it by creating interactive dashboards, summaries, and reports, eliminating much of the manual effort traditionally required in research and analytics.\nOne of Fellou‚Äôs standout features is its proprietary framework, Eko, which enables users to build custom AI agents tailored to specific data analysis workflows or use cases. You can train these agents to follow predefined routines such as monitoring trends, comparing competitors, tracking customer sentiment, or generating periodic insights. This makes Fellou particularly useful for product managers, marketers, researchers, founders, and data analysts who need recurring or specialized data intelligence.\nOn its free plan, users can run up to four tasks and set up two scheduled (automated) tasks, which is sufficient for light experimentation or small, targeted projects. Even with this limitation, Fellou remains highly effective for prototype-level research or one-off analysis.\nWhat truly makes Fellou powerful is its prompt-driven workflow. You can simply describe what you want in natural language, and Fellou will automatically determine the data sources, gather the relevant information, analyze it, and present the findings in a structured report.\nFor instance, you could prompt Fellou to ‚Äúanalyze LinkedIn job descriptions for AI Product Manager roles, focusing on required skills, years of experience, tools, and hiring trends.‚Äù Based on that instruction, it will search for relevant job postings, extract and categorize key information, and return a detailed summary of current industry demand, emerging skills, and noticeable patterns in hiring.\nThis level of automation turns Fellou into more than just a browser, it acts as an intelligent research assistant capable of performing complex data analysis tasks that would otherwise take hours or days to complete manually.\n\n\n\n10. Data Formulator\nData Formulator, developed by Microsoft Research, is an AI-powered tool focused primarily on exploratory data analysis and data visualization. It is designed to help users transform raw data into meaningful, visually compelling charts with minimal manual configuration. Users can connect Data Formulator to both databases and cloud storage folders, making it easy to work with structured data from a wide variety of sources.\nOne of its most innovative features is its prompt-based interaction model. Instead of manually configuring fields, axes, and filters as in traditional BI tools, you can simply describe the type of visualization you want in natural language. For example, you might prompt: ‚ÄúShow a line chart of monthly revenue growth for the past two years, broken down by region.‚Äù The tool then intelligently determines which variables to use, how they should be mapped, and what type of chart best represents your request.\nAlternatively, users can start with a set of pre-built visual templates. From there, the app automatically suggests the most appropriate variables and relationships in the dataset, making it more approachable even for users who don‚Äôt have advanced data visualization experience.\nAt the moment, the online version of Data Formulator is limited in functionality, and running it locally may require some technical knowledge, including familiarity with development environments and configuration steps. Despite these early limitations, it is an incredibly promising and capable tool for data exploration and rapid insight generation.\nAnother major advantage is its flexibility with AI models. Users can bring their own existing AI API key (such as an OpenAI-compatible key) and integrate it directly into the application, making it easier to control model usage, scale, and costs.\nOverall, Data Formulator stands out as a highly effective and forward-thinking solution for analysts, researchers, and data-driven teams who want to speed up both the discovery and visual storytelling phases of data analysis."
  },
  {
    "objectID": "posts/10 AI Data Analysis Tools That Will Redefine How You Make Decisions/index.html#how-to-choose-the-right-ai-analysis-tool",
    "href": "posts/10 AI Data Analysis Tools That Will Redefine How You Make Decisions/index.html#how-to-choose-the-right-ai-analysis-tool",
    "title": "10 AI Data Analysis Tools That Will Redefine How You Make Decisions",
    "section": "How to Choose the Right AI Analysis Tool",
    "text": "How to Choose the Right AI Analysis Tool\nWhen choosing an AI tool for data analysis, you might have to look at a number of considerations and see if the AI tool will serve your use case. Here are what you should consider:\n\nData Size: Can the AI tool handle big amount of data? This is a question you should ask yourself. Some AI data analysis tools are meant for small files like CSV uploads or Spreadsheets like ChatGPT, while others can connect to data warehouses like Bag of Words and Vertex AI.\nSkill Level: You need to also know if the AI data analysis tool requires coding knowledge, some requires you to have a knowledge of programming to use. If you are not a developer, such AI data analysis tool might not suit your use case and you will have to look for no-code alternatives.\nBudget and Infrastructure: Most AI data analysis tools are paid, except for the ones which you can deploy and host locally. You might have to compare your budget against various AI tools and see the one that is affordable without having to break the bank.\nDeployment Needs: Some AI tools are already available to use as a web application, while others might need deployment to use. If you have technical knowledge of deployment using various platforms, you can opt for self deployed options, otherwise it‚Äôs just best to use those with already deployed web applications.\n\n\n\n\n\n\n\n\n\n\n\nAI Tool\nData Size\nSkill level\nBudget\nDeployment\n\n\n\n\nChatGPT\nSmall\nEasy\nFree but limited\nNot needed\n\n\nLearnFacebookAdsInsights\nLarge\nEasy\nPaid\nNot needed\n\n\nJulius AI\nLarge\nEasy\nFree but limited\nNot needed\n\n\nBag of Words\nLarge\nEasy\nFree but Self host needed\nNeeded\n\n\nPandasAI\nLarge\nAdvanced\nFree\nNeeded\n\n\nAnnie\nLarge\nEasy\nFree but limited\nNot needed\n\n\nVertex AI\nLarge\nAdvanced\nPaid\nNeeded\n\n\nSigma AI\nSmall\nEasy\nFree but limited\nNot needed\n\n\nFellou.ai\nSmall\nEasy\nFree but limited\nNot needed\n\n\nData Formulator\nLarge\nEasy\nFree but Self host needed\nNeeded"
  },
  {
    "objectID": "posts/10 AI Data Analysis Tools That Will Redefine How You Make Decisions/index.html#conclusion",
    "href": "posts/10 AI Data Analysis Tools That Will Redefine How You Make Decisions/index.html#conclusion",
    "title": "10 AI Data Analysis Tools That Will Redefine How You Make Decisions",
    "section": "Conclusion",
    "text": "Conclusion\nAI tools accelerate insights and decision making, and as a small business you don‚Äôt need to employ a data analyst to help analyze your data or make findings from it. Data Analyst in big businesses can also delegate repetitive analytical tasks to AI tools, while focusing on much more important tasks.\nYour choice of AI tools depends on your use case; you can go for self hosted tools if you are more concerned about privacy, or go for the ones that are easy to use if you don‚Äôt have self hosting or deployment knowledge. Which ever way, they all serve the same purpose; to make sense of your data.\nI will encourage you to try all of them, and experiment their strengths and weaknesses to see the one that will best serve your use cases. I hope you find this article useful, don‚Äôt forget to check out 10 best AI browsers to see which browser you are actually missing on.\n\nNeed Help with Data? Let‚Äôs Make It Simple.\nAt LearnData.xyz, we‚Äôre here to help you solve tough data challenges and make sense of your numbers. Whether you need custom data science solutions or hands-on training to upskill your team, we‚Äôve got your back.\nüìß Shoot us an email at admin@learndata.xyz‚Äîlet‚Äôs chat about how we can help you make smarter decisions with your data."
  },
  {
    "objectID": "posts/10 AI Data Analysis Tools That Will Redefine How You Make Decisions/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "href": "posts/10 AI Data Analysis Tools That Will Redefine How You Make Decisions/index.html#your-next-breakthrough-could-be-one-email-away.-lets-make-it-happen",
    "title": "10 AI Data Analysis Tools That Will Redefine How You Make Decisions",
    "section": "Your next breakthrough could be one email away. Let‚Äôs make it happen!",
    "text": "Your next breakthrough could be one email away. Let‚Äôs make it happen!"
  }
]