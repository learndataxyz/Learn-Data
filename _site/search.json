[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Monitoring Model Performance and Data Drift for Diabetes Classification\n\n\n\nmlops\n\n\ndata drift\n\n\nclassification\n\n\ntutorial\n\n\npython\n\n\ndata science\n\n\nmachine learning\n\n\nhealthcare\n\n\ndeployment\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Engineer’s Guide to Low Code/No Code ELT Tools\n\n\n\ndata engineering\n\n\nlow code\n\n\nno code\n\n\nELT\n\n\n\n\n\n\n\nAdejumo Ridwan Suleiman\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "navs/about.html",
    "href": "navs/about.html",
    "title": "About",
    "section": "",
    "text": "Feeling lost in the world of data? Let LearnData be your guide. Our team lives and breathes all things data - from the fundamentals to cutting-edge techniques. We’ll break it down for you step-by-step with real examples and hands-on practice. By the time you’re done, you’ll be a data dynamo ready to take on any challenge."
  },
  {
    "objectID": "navs/courses.html",
    "href": "navs/courses.html",
    "title": "Courses",
    "section": "",
    "text": "ML Model Deployment with FastAPI and Streamlit\nBuilding Interactive Shiny Web Apps with R Programming\nData Wrangling and Exploratory Data Analysis with R\nPower BI DAX Practice Test and Solutions"
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#introduction",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#introduction",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Introduction",
    "text": "Introduction\nAccording to WHO, the number of people with diabetes rose from 108 million in 1980 to 422 million in 2014. Diabetes is a serious disease that leads to blindness, kidney failure, heart attacks, strokes, and lower limb amputations. It is mostly prevalent in low- and middle-income countries.\nBuilding a healthcare system that uses machine learning to predict patients with diabetes will help in early detection making it easy for healthcare providers to screen patients with diabetes at an early stage, before diagnosis.\n\nMachine learning models tend to degrade with time, highlighting the need for effective and constant monitoring of the model to know when its performance is declining. Often, this arises as a result of the change in the distribution of the data compared to the data the model was trained upon, this phenomenon is known as Data Drift.\nIn this article, you will learn how to monitor a diabetes classifier and detect data drifts in the data received from patients in a health information management system or mobile application using NannyML.\nNannyML is an open-source library for monitoring machine learning model performance in production, even without the predicted values being ready. It allows you to track your machine-learning model over time and see checkpoints where the model degrades."
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#estimating-model-performance-with-nannyml",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#estimating-model-performance-with-nannyml",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Estimating Model Performance with NannyML",
    "text": "Estimating Model Performance with NannyML\nNannyMl offers binary class classification that one could use to estimate the model’s performance, even without targets. Model estimation performance with NannyML involves:\n\nGetting the reference and analysis sets ready: The reference set is the data where the model behaves as expected, usually the test data. The analysis set is the latest production data, either with target features or not.\nTraining a performance estimator on the reference set: NannyML uses the reference set to train a performance estimator, it’s advisable to use the test data as reference data to prevent overfitting.\nUsing the estimator to predict performance on the analysis set (simulating real-world data): NannyML estimates the model performance on the analysis data using the trained performance estimator. One can use various classification metrics, such as accuracy or F1-score. Since misclassifying a patient (false negative) is more severe than misclassifying a healthy patient as diabetic (false positive), the AUC-ROC is the most appropriate metric to use in this case."
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#detecting-data-drift-with-nannyml",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#detecting-data-drift-with-nannyml",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Detecting Data Drift with NannyML",
    "text": "Detecting Data Drift with NannyML\nLet’s say you deployed a machine-learning model. As time goes on, the model tends to degrade, This is due to the nature of the data changing. If you have an application that you initially designed for kids and you train most of your machine learning models using your current user’s data, then all of a sudden middle-aged people and the elderly start using your application, and they become more of your users than the kids you designed it for. This will change the age distribution of your data, If age is one important feature in your machine learning model, your model will get worse with time. This is where you need to monitor when such changes happen in your data so that you can update the ML model.\n\nNannyML uses various algorithms to detect data drift, either using Univariate drift detection or Multivariate drift detection methods.\n\nUnivariate drift detection: In this approach, NannyML looks at each feature used in classifying if a patient is diabetic, and compares the chunks with those created from the analysis period. The result of the comparison is called a drift metric, and it is the amount of drift between the reference and analysis chunks, which is calculated for each chunk.\nMultivariate Drift Detection Instead of taking every feature one by one, NannyML provides a single summary metric explaining the drift between the reference and the analysis sets. Although this approach can detect slight changes in the data, it is difficult to explain compared to univariate drift.\n\nIn the case of classifying diabetic patients, undetected drift is dangerous and can lead to wrong model classifications. This is worse if the number of false negatives is high, the classifier might not detect some patients with diabetes, this can lead to late diagnosis."
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#estimating-model-performance-in-the-diabetes-classifier",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#estimating-model-performance-in-the-diabetes-classifier",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Estimating Model Performance in the Diabetes Classifier",
    "text": "Estimating Model Performance in the Diabetes Classifier\nNannyML uses two main approaches to estimate model performance, Confidence-based Performance estimation (CBPE) and Direct Loss estimation (DLE). In this case, we are interested in using the CBPE, since we are dealing with a classification task.\nThe CBPE uses the confidence score of the predictions to estimate the model performance, the confidence score is a value that the diabetes classifier gives for each predicted observation, expressing its confidence in predicting if a patient is diabetic., with values ranging from 0 to 1 and the closer it is to 1, the more confident the classifier is with it’s prediction.\nThe diabetes data contains 253,680 responses and 21 features. In this section, you will learn how to use this data to build an ML model, estimate your model’s performance, and detect data drift on updated data.\n\nProject Requirements\nTo get started, ensure you have installed NannyML on your JupyterNotebook. Download the analysis and diabetes data. The diabetes data is the data you will train the machine learning model on, and the analysis data is what you would take as the production data from the patients, which you will use to estimate model performance and detect data drift later on.\n\n\nBuilding the ML Model\nLet’s build a simple random forest classifier to classify respondents as diabetic.\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load your data\ndiabetes = pd.read_csv(\"binary_diabetes.csv\")\n\n# Split the data into features (X) and target (y)\nX = diabetes.drop('Diabetes_binary', axis=1)\ny = diabetes['Diabetes_binary']\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Create and train the model\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Get the predicted probabilities\ny_pred_proba = model.predict_proba(X_test)[:, 1]  \n\n# Print the classification report\nprint(classification_report(y_test, y_pred))\n\n# Create a dataframe with the test data, predicted classes, and predicted probabilities\ntest_results = X_test.copy()\ntest_results['y_true'] = y_test\ntest_results['y_pred'] = y_pred\ntest_results['y_pred_prob'] = y_pred_proba\nHere is the output of the classification model.\n                                precision    recall  f1-score   support\n\n         0.0       0.88      0.97      0.92     65605\n         1.0       0.48      0.18      0.26     10499\n\n    accuracy                           0.86     76104\n   macro avg       0.68      0.57      0.59     76104\nweighted avg       0.83      0.86      0.83     76104\nFrom the report above, the classifier can classify those with diabetes with a precision of 0.88 accurately. This means the chances of the classifier missing a diabetic patient are low. An overall accuracy of 0.86 indicates the model is performing well.\n\n\nEstimating Model Performance\nUsing CBPE, you can estimate the model performance when in production, and the analysis data does not necessarily have to contain the target feature.\nimport nannyml as nml\n\nreference_df = test_results\nanalysis_df = pd.read_csv(\"/content/analysis_df.csv\")\n\nestimator = nml.CBPE(\n    y_pred_proba =\"y_pred_prob\",\n    y_pred = \"y_pred\",\n    y_true = \"y_true\",\n    problem_type = \"classification_binary\",\n    metrics = [\"roc_auc\"],\n    chunk_size = 3000\n)\n\nestimator.fit(reference_df)\nresults = estimator.estimate(analysis_df)\n\nmetric_fig = results.plot()\nmetric_fig.show()\n\nnml.CBPE calculates the CBPE and takes the argument, y_pred_proba the predicted probability, y_pred the predicted classification, y_true the actual classification.\nThe problem_type argument takes the type of classification one is interested in, binary classification in this case.\nResults in NannyML are presented per chunk, by aggregating the data to a single data point on the monitoring results; here I used a chunk of 3000.\n\nYou can use any other metric, but the roc-auc metric is preferred in this situation because this is a health problem and it tells us more about the ability of the classifier to detect true positives, to minimize the chance of missing any diabetic patient.\n\nThe plot above shows that the model ROC-AUC metric started to fail at some point, which led to inaccurate predictions. The blue marker indicates the model metric, if this line exceeds the upper or lower threshold in red, it raises an alert signifying a drift.\nThe next step is to see what causes the poor model performance in these chunks by applying various drift detection methods.\n\n\nDetecting Data Drift\nLet’s use a multivariate drift detection method to get a summary number that detects any changes in our data using the domain classifier approach. This provides a measure of discriminating the reference data from the examined chunk data. If there is no data drift, the datasets are not different giving a value of 0.5. The higher the drift, the higher the returned measure, with a value as high as 1. This method provides a general overview of data drift in the analysis data.\nnon_feature_columns = ['y_pred_proba', 'y_pred', 'y_true']\nfeature_column_names = [\n    col for col in reference_df.columns\n    if col not in non_feature_columns\n]\ncat_features = [feature for feature in feature_column_names if feature not in ['BMI', 'MentHlth', 'PhysHlth', 'Age']]\n\ncalc = nml.DomainClassifierCalculator(\n    feature_column_names=feature_column_names,\n    chunk_size=3000\n)\n\ncalc.fit(reference_df)\nresults = calc.calculate(analysis_df)\n\nfigure = results.plot()\nfigure.show()\n\nThe plot above shows the domain classifier values in the reference data are either a little below or above 0.5, but that of the analysis data reaches a value of 1, indicating the presence of data drift in the analysis data. Now that we know we have data drift in the analysis data, let’s see which features are causing this drift.\n\n\nRanking\nWe need to know the features contributing to drift in the analysis data. The ranking method uses the results of univariate drift detection to rank features based on alert counts or correlation ranking.\nLet’s use the alert count’s approach to know which features are causing the drift in data.\nuniv_calc = nml.UnivariateDriftCalculator(\n    column_names=feature_column_names,\n    treat_as_categorical=['y_pred', *cat_features],\n    continuous_methods=['kolmogorov_smirnov', 'jensen_shannon'],\n    categorical_methods=['chi2', 'jensen_shannon'],\n    chunk_size=3000\n)\n\nuniv_calc.fit(reference_df)\nunivariate_results = univ_calc.calculate(analysis_df)\n\nalert_count_ranker = nml.AertCountRanker()\nalert_count_ranked_features = alert_count_ranker.rank(\n    univariate_results.filter(methods=['jensen_shannon']),\n    only_drifting = False)\ndisplay(alert_count_ranked_features)\nkolmogorov_smirnov, jensen_shannonand chi2 are various univariate drift detection methods you can always choose from.\n\nThe table above shows the top 10 features likely to cause drifts based on the alert counts. Next, let’s investigate further the contribution to data drift for each variable using the univariate drift detection method.\n\n\nUnivariate Drift Detection Method\nUnivariate drift detection allows you to see the amount of drift in the suspected features, which was used earlier to rank the features.\nuniv_calc = nml.UnivariateDriftCalculator(\n    column_names=feature_column_names,\n    treat_as_categorical=['y_pred', *cat_features],\n    continuous_methods=['jensen_shannon'],\n    categorical_methods=['jensen_shannon'],\n    chunk_size=3000\n)\n\nuniv_calc.fit(reference_df)\nunivariate_results = univ_calc.calculate(analysis_df)\n\nfigure = univariate_results.filter(column_names=univariate_results.continuous_column_names, methods=['jensen_shannon']).plot(kind='drift')\nfigure.show()\n\nThe plots above show the amount of drift in some of the features using Jensen-Shannon distance, which you can apply to both continuous and categorical features. You can also see that the red dotted points exceed the dotted line in each plot, this signifies drift in the variable.\nYou can also go further into each feature to see the distribution, this lets you know how large this drift is Let’s take a critical look at PhyHlth, this is the first feature on the ranked list.\nfigure = univariate_results.filter(column_names=[\"PhysHlth\"], methods=['jensen_shannon']).plot(kind='distribution')\nfigure.show() \n\nFrom the plot, you can see in highlight chunks with data drift, the plot is wider and bigger compared to that of the reference data, the plot also tells the presence of negative values in the analysis data. You can follow this procedure for each feature in the model indicating data drift using the univariate drift detection method. From the above results, you can understand the change in model prediction and the cause of those changes."
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#conclusion",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#conclusion",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, you learned about NannyML an open-source tool for monitoring model performance and detecting data drifts. You also learned how to use NannyML on a diabetes classifier and how to apply both univariate and multivariate drift detection methods in detecting data drift using NannyML.\nWhat’s next after detecting data drift? Check out this article, to know what to do when you detect drifts in your data."
  },
  {
    "objectID": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#recommended-reads",
    "href": "posts/Monitoring Model Performance and Data Drift for Diabetes Classification/index.html#recommended-reads",
    "title": "Monitoring Model Performance and Data Drift for Diabetes Classification",
    "section": "Recommended Reads",
    "text": "Recommended Reads\n\nMonitoring a Hotel Booking Cancellation Model Part 1: Creating Reference and Analysis Set\nTutorial: Monitoring an ML Model with NannyML and Google Colab\nHow to Estimate Performance and Detect Drifting Images for a Computer Vision Model?"
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#introduction-to-low-codeno-code-elt-tools",
    "href": "posts/The Engineer's Guide to Low Code/index.html#introduction-to-low-codeno-code-elt-tools",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "Introduction to Low-code/No-code ELT Tools",
    "text": "Introduction to Low-code/No-code ELT Tools\nLow-code/No-code tools are tools used in building applications using drag-and-drop components, reducing or eliminating the amount of code used in development. It provides an interactive graphical user interface, making it easy for non-technical users to start developing.\nLow-code tools help developers quickly get started, writing little or no code. It helps professional developers quickly deliver applications, letting them focus on the business side of the applications. Some features of low-code tools are:\n\nIt offers an interactive user interface with high-level functions, eliminating the need to write complex code.\nIt is easy to modify or adapt.\nMostly developed for a specific use case or audience\nIt is easy to scale.\n\nNo-code, on the other hand, is a method of developing applications that allows non-technical business users; business analysts, admin officers, small business owners, and others, to build applications without the need to write a single line of code. Some features of no-code tools are:\n\nPurely visual development, that is, users develop using drag-and-drop interfaces.\nIt offers limited customization, users use what is provided in the tool and can’t extend its capabilities.\nSuited for non-technical individuals\nMostly suited for simple use cases\n\nLow-code tools require some basic coding skills, unlike no-code which does not require any programming knowledge. Low-code/no-code is based on visual programming and automatic code generation. The emergence of low-code/no-code was the fact that domain experts know how difficult it is to impart to the IT team, with the help of low-code/no-code, they can take part in the development process, coupled with the fact that the shortage of skilled developers and loads of workload on the IT professionals is another reason for the emergence of low-code/no-code."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#understanding-the-elt-process",
    "href": "posts/The Engineer's Guide to Low Code/index.html#understanding-the-elt-process",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "Understanding the ELT Process",
    "text": "Understanding the ELT Process\nELT stands for Extract, Load and Transform, which are the three stages involved in the process.\nIt’s a data preprocessing technique that involves moving raw data from a source to a data storage area, either a data lake or a data warehouse. Sources are either social media platforms, streaming platforms or any other place data is stored. During extraction, data is copied in raw form from the source location to a staging area, this is either in a structured or unstructured format from sources such as:\n\nSQL or NoSQL servers\nText and document files\nEmail\nWeb pages\n\nExtraction is either full which involves pulling all rows and columns from a particular data source, using an automated partial extraction with update notifications when data is added to the system, or incremental where update records are extracted as data is added into the data source.\nLoading involves moving the data from the staging area to the data storage area, either a data lake or a data warehouse. This process is automated, continuous and done in batch. One can load all the available data in the data source to the data storage area, load modified data from the source between certain intervals or load data into the storage area in real time.\nIn the transform stage, a pre-written schema is run on the data using SQL for analysis. This stage involves filtering, removing duplications, currency conversions, removal of encryptions, joining data into tables or performing calculations.\nUnlike in ETL where raw data is transformed before being loaded into a destination source, in ELT the data is loaded into a destination source before it’s transformed for analysis as needed.\n\nBy allowing transformation to occur after loading, data is moved quickly to the destination for availability. Because data is transformed after arrival at the destination, ELT allows the data recipient to control data manipulation. This ensures that coding errors when transforming do not affect another stage. ELT uses the powerful big data warehouse and lakes allowing transformation and scalable computation.\nData warehouses use MPP architecture (Massively Parallel Processing), and data lake processes also support the application of schema or transformation models as soon as the data is received, this makes the process flexible, especially for large amounts of data. ELT is suited for data that are in cloud environments, this provides a seamless integration since ELT is cloud-native and allows for the continuous flow of data from sources to storage destinations, hence making them on demand.\nELT is used for instant access to huge volumes of data, for example in IOT, it loads data from IOT devices making it readily available for data scientists or analysts to access raw data and work collaboratively.\nDespite its advantages, ELT has some of its limitations:\n\nData privacy is a challenge in ELT, this is because when transferring data, a breach can occur from the source to the destination storage which poses security and privacy risks.\nWhile in transit, if care is not taken sensitive information is exposed, and extra security measures have to be taken to ensure the confidentiality of the data.\nELT handle large volumes of data, making it computationally intensive, leading to delays in gaining insights.\nBecause the data is loaded into the data storage area without transformation, it makes it challenging to transform the data when compared with ETL. This requires strong technical and domain knowledge of the data scientist and analyst who will write the queries transforming the data."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#low-codeno-code-elt-tools",
    "href": "posts/The Engineer's Guide to Low Code/index.html#low-codeno-code-elt-tools",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "Low-Code/No-Code ELT Tools",
    "text": "Low-Code/No-Code ELT Tools\nLow-code /No-code ELT are used to extract, load and transform data. Here are some of the benefits of using low-code/no-code ELT tools:\n\nIt is easier compared to writing scripts to automate the ELT process.\nIt makes development faster, developers can spend more time solving business problems instead of fixing bugs that result from writing lines of code.\nIt increases automation, a lot of processes that would have to be set up manually are handled automatically, such as monitoring, logging, setting notifications when there is a problem with the pipeline and so on.\nMost ELT tools support a lot of data connectors, making it easy for an organization to connect to any data source with provisions to create custom connectors.\nELT tools lower the cost of building an ELT pipeline by ensuring the whole process is conducted with a single tool from start to finish.\nThey provide better customer experience, by ensuring that even business folks are involved in building the ELT pipeline.\n\nThere are various low-code/no-code  ELT tools out there, each with its strengths and limitations, here are some you can consider for building an ELT pipeline:\n\nAirbyte\nAirbyte is an open-source data movement platform with over 300+ open-source structured and unstructured data sources. Airbyte is made up of two components; platform and connectors. The platform provides all the services required to configure and run data movement operations while the connectors are the modules that pull data from sources or push data to destinations. Airbyte also has a connector builder, a drag-and-drop interface and a low-code YAML format for building data source connectors.\nAirbyte has two plans, Cloud and Enterprise, but it is free to use if you can self-host the open-source version. Airbyte offers real-time and batch data synchronization, tracks the status of data sync jobs, monitors the data pipeline and views logs with a notifications system in case things go wrong. Airbyte also allows you to add custom transformations using dbt.\n\n\nFivetran\nFivetran is an automated data movement platform used for ELT. It offers automated data movement, transformations, security and governance with over 400+ no-code source connectors. You can use the function connector in Fivetran to write the cloud function to extract the data from your source, while it takes care of loading and processing the data into your destination. Fivetran gives you options to manage Fivetran connectors, to have more control over your data integration process runs. It offers a “pay-as-you-use” model, with five Free pricing plans, Starter, Standard, Enterprise and Business Critical.\n\n\nIntegrate.io\nFormerly known as Xplenty, this is another low-code platform for data movement, unlike the others it doesn’t have many connections, it offers both low-code ETL, Reverse ETL and an ELT platform. Pricing on Integrate.io is based on data volume and increases as the number of rows in your data increases. It offers both Starter, Professional and Enterprise plans, with an extra charge for additional connectors.\n\n\nStitch\nStitch is also another data movement platform owned by Qlik. It replicates historical data from your database for free and allows you to add multiple user accounts across your organization to manage and authenticate data sources. It is extensible and has several hundreds of connectors. It offers various pricing models such as standard, advanced and premium which are all charged based on data volume.\n\n\nMatillion\nMatillion is another ELT platform, that uses LLM components to unlock unstructured data and offers custom connectors to build your connectors. It has a complete pushdown architecture supporting SQL, Python, LLMs, Snowflake, Databricks, AWS, Azure and Google. It supports both low-code and no-code for both programmers and business users. You can create an ELT process using either SQL, Python or dbt. It also gives you Auto-Documentation to generate pipeline documentation automatically. It offers three pricing models Basic, Advanced and Enterprise which you can pay only for pipelines run and not, those on development or sampling."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#key-features-of-low-codeno-code-elt-tools",
    "href": "posts/The Engineer's Guide to Low Code/index.html#key-features-of-low-codeno-code-elt-tools",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "Key Features of low-code/no-code ELT Tools",
    "text": "Key Features of low-code/no-code ELT Tools\n\nAvailability of Connectors\nELT connectors are components of an ELT tool that allow the tool to connect to a data source and make it possible for extraction and loading. When trying to go for ELT tools it is important to go for the ELT tool with the highest number of connectors, that is why an organization needs to have a list of all the data sources it uses, this will let the organization know the ELT tool to choose based on organizational data sources, most importantly connectors for revenue-generating applications. Let’s say your organization uses Zoho as a CRM. It’s important to compare various ELT tools with a connector for Zoho and see which offers the best service at the most affordable price.\n\n\nDrag-and-Drop Interfaces\nLow-code/no-code ELT tools offer an intuitive user interface with a drag-and-drop functionality, allowing non-technical users to perform ELT by dragging and dropping components without encountering any challenges. This makes the user experience seamless and users can focus on the application’s business logic. This reduces the workload on the IT team, allowing the organization’s domain experts to partake in the development process.\n\n\nAutomated Scheduling\nDue to their ability to schedule extracting and loading, automating the ELT process is very simple. This can involve creating tasks that can be run using a specified SQL schema at specific intervals. One can easily automate documentation, document process automation, and show the manipulations occurring to data from source to data storage, enabling organizations to save time and costs.\n\n\nData Transformation Capabilities\nLow-code/no-code ELT tools manage dependencies when transforming data, this is essential when you have multiple transforms depending on each other. They support the DAG(directed acyclic graph) workflow to manage the transformation dependencies after conducting a transform job using SQL on the data, reading the transformation query and calculating a dependency workflow.\n Another important aspect is their support for incremental loading, where only the differences in data are loaded from source to destination. For example Let’s consider the case of a retail store that tracks its sales data, without an incremental approach the daily sales report would involve extracting sales data from the sales table, which contains millions of records, aggregating the data to calculate the total sales, revenue and units sold for each product, store and day, and load the aggregated data into a new table called sales_daily.\nThis is a resource-intensive approach as the system needs to process all the sales data every time the report is generated. Using an incremental approach, whenever a new sale is recorded in the sales table, a trigger or a background process is used to update the sales_daily table with new data for that day and store.  Whenever every report is generated, only the data for the latest day is extracted from the sales_daily table, which is a much smaller dataset than the entire sales table. The incremental approach helps in improving performance, cost and scalability.\n\n\nMonitoring and Alerting\nMonitoring and Alerting is another important feature of a low-code/no-code ELT tool because It allows you to detect anomalies, bottlenecks and failures in the workflow, provide continuous surveillance and monitor resource utilization. Key important metrics it monitors are; latency, throughput, error rates, resource utilization and pipeline lag. They also give threshold alerts, detect anomalies, and escalate alerts to SMS or phone calls.\nImagine a manufacturing company that collects sensor data from various manufacturing plants across facilities. If analysis, reveals one machine is giving higher vibration levels than others. The ELT tool anomaly detection algorithm should trigger an alert which will prompt the maintenance team to investigate. They might identify a worn-out bearing component and schedule proactive maintenance, preventing equipment failure and uninterrupted production."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#evaluating-low-codeno-code-elt-tools",
    "href": "posts/The Engineer's Guide to Low Code/index.html#evaluating-low-codeno-code-elt-tools",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "Evaluating low-code/no-code ELT Tools",
    "text": "Evaluating low-code/no-code ELT Tools\nThere are various things to consider when choosing a low-code/no-code  ELT tool.\n\nConnectors: When selecting an ELT tool, ensure it supports connections to various data sources and know how many SAAS integrations are available and how effectively the tool can connect to organizational data sources.\nRoadmap: Another important factor, is if the ELT tool can handle the company’s rapidly growing data. Is it responsive and scalable? This will give the organization an idea of the ELT tool’s sustainability in the long run.\nPricing: How does the ELT tool charge? Is it by data flows or data volume and are the features it offers worth its pricing? Some ELT tools offer more connectors at affordable pricing than others.\nSupport: Look for an ELT tool with available customer support, this is very crucial especially when things break. The ELT tools should also offer good documentation that is easy to understand by technical and non-technical users. An online community around the tool is also a plus, users can relate with fellow users and serve as support for each other.\nSecurity: How does the ELT tool prioritise security? Are organizational data safe and is it regulatory compliant with GDPR, SOC2, HIPPA, and other relevant regulations? These are important security questions to look for when selecting an ELT tool. It is also important that the organization knows, how the tool handles privacy and authentication.\nEase of use: A low-code/no-code ELT tool that is user-friendly and easy to customize is another priority to look out for, it makes the process of creating ELT pipelines easy and non-technical for business folks.\nMaintenance: When choosing a low-code/no-code ELT tool, it’s important to know how easy it is to fix problematic data sources, and if it gives informative logs if an execution fails. It is also important to know what skills are required, by team members to keep the ELT process running smoothly."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#implementing-low-codeno-code-elt-tools",
    "href": "posts/The Engineer's Guide to Low Code/index.html#implementing-low-codeno-code-elt-tools",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "Implementing Low-code/No-code ELT Tools",
    "text": "Implementing Low-code/No-code ELT Tools\n\nPlanning the ELT Pipeline\nBefore building an ELT pipeline, you need to get the data from your source using an ELT tool like Airbyte and decide the data warehouse to use, either Google BigQuery, AWS Redshift or Snowflake. Next is to transform the data using dbt, R, Python or a no-code transformation layer such as Boltic, then consider the BI tool for presenting the data to end users.\n\n\nConfiguring Data Source and Destination Connections\nLet’s say for example using a REST API as a data source, Airbyte as the ELT tool and Amazon S3 bucks as the data destination. Create a new S3 bucket in the AWS console, in the bucket, create a folder to store data from the ELT operation in Airbyte. Create another folder in the previous folder to store the data you will extract from the REST API.\nNext, you will configure both the source and the destination connector, and connect the source and the destination. Ensure any API you use, there is a connector for it on Airbyte. If you don’t see a connector for your data source, use Airbyte’s CDK(Connector Development Kit) to create a custom connector.\nNext, you go to AWS S3 to configure the destination connector to connect Airbyte with the destination data lake, after successfully configuring both the source and the destination connector, and passing all the tests. You can now configure the ELT connection between source and destination.\n\n\n\nDesigning Data Transformation Workflows\nBefore transforming your data, you need to explore and understand it, this involves looking at your entity relationship diagrams to see how the data relate with each other, identifying missing values or misleading column names and performing a summary analysis of the data.\nWhile exploring, you might understand what is wrong with your data, and decide to perform your transformation process such as correcting misleading columns, renaming fields appropriately, adding business logic or joining data. The transformation you apply depends on what you have explored in the data. You can use tools like dbt, SQL, Python or R to transform your data or go no-code with tools like Boltic. At this stage,  test your data to meet business standards.\nFinally, you document your transformation process explaining the data model, key fields and metrics making the documentation easy for non-technical users to understand.\n\n\nScheduling and Automating ELT Processes\nBefore releasing the data to end users, you need to push it into a production environment in the data warehouse, these product-ready tables are what the BI analysis will query. With time you will need to update and refresh the data to meet the business needs, using a scheduler or orchestrator.  Using the job scheduler, you can use tools like dbt Cloud or Shipyard to create data transformations and tests within a collaborative IDE.\n\n\nMonitoring and Maintaining the ELT Pipeline\nMonitoring is important to identify bugs in data pipelines, optimise pipelines and gain valuable insights. These ELT tools provide visualisations, logging and observability systems to analyse latency, error rates and data throughput in your ELT pipeline. Most low-code/no-code provide all these out of the box, you will receive custom notifications when data problems occur allowing you to improve the quality of your data sets."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#best-practices-for-low-codeno-code-elt",
    "href": "posts/The Engineer's Guide to Low Code/index.html#best-practices-for-low-codeno-code-elt",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "Best Practices for low-code/no-code ELT",
    "text": "Best Practices for low-code/no-code ELT\n\nEnsuring Data Quality and Integrity\n\nData quality and requirements: The first step before conducting an ELT is to specify the data quality and requirements. These should specify the data accuracy, completeness, consistency, timeliness, validity, and uniqueness. This also includes details of the sources, end users and data quality metrics. This will help to understand the quality of data to expect and how to achieve it.\nValidation: The next step is to validate the data quality using the ELT tools before loading them into the warehouse, to reduce the amount of bad-quality data that gets into the data warehouse.\nData quality checks: In this step, ensure you use the ELT tools to implement quality checks on the data in the data warehouse before making it available to end users so that the data is consistent, complete and correct(3Cs).\nData quality monitoring and auditing: The next step is to monitor and audit the data quality as new data gets updated into the system, this is to resolve issues of data quality when they arise. ELT tools have various tools to get reports, alerts and logs on data quality. This ensures the successful maintenance of the data in the long run.\nData quality documentation and communication: Next is to give a report explaining to the end users or stakeholders the quality of the data, this report should contain the processes, rules, metrics and issues with the data. Doing this ensures trust and transparency of the data quality.\nReview and update the data quality: Constantly use the information from the ELT tool logging system to update the data quality, from time to time. By doing this, you are ensuring that the data remains relevant and meets the organization’s requirements.\n\n\n\nHandling Data Governance and Compliance\nIt is important to define clear roles and objectives for various stakeholders and use data governance tools to automate the tasks of data validation, cleansing, standardization, profiling, auditing, and reporting. Provide a data catalogue of the data assets and other data quality indicators and recommend a staging area for the source data before moving to the data warehouse. Finally, using a schema-on-read approach allows users to apply different views to the same data, based on their needs.\n\n\nIntegrating low-code/no-code ELT with Existing Systems\nDue to the visually driven approach of low-code/no-code ELT tools, you must thoroughly assess your existing data sources, formats and structures to identify potential compatibility issues and develop appropriate mapping strategies. Check, if a connector for your data source exists before building a custom connector.\n\n\nScaling and Optimizing ELT Workflows\nOne of the reasons for scaling and optimizing an ELT process is to reduce data loss and ELT run downtime. The following are ways to optimize an ELT workflow effectively:\n\nParallelize Data Flow: Simultaneous data flow saves more time than sequential data flow, you can load a group of unrelated tables from your data source to the data warehouse, by grouping the tables into a batch and running each batch simultaneously, instead of loading the tables one by one.\nApplying data quality checks: You should ensure each data table is tested with predefined checks such as schema-based to ensure that when a test fails, the data is rejected and when it passes, it produces an output.\nCreate Generic Pipelines: Generic pipelines ensure team members reuse a code without developing one from scratch. Practices like parameterizing the values can ease the job, when one wants to use a code/pipeline, they change the values of the parameters such as database connections.\nUse of streaming instead of batching: Streaming data from the source to the data destination is the best approach, this ensures the system is up to date whenever new data is added to allow end users access to recent data."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#future-of-low-codeno-code-elt",
    "href": "posts/The Engineer's Guide to Low Code/index.html#future-of-low-codeno-code-elt",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "Future of low-code/no-code ELT",
    "text": "Future of low-code/no-code ELT\nLow-code/no-code ELT tools have become popular, offering cloud-based options and reduced infrastructure costs. Most of them now have pre-built connectors covering most data sources and automated scheduling. Advanced low-code/no-code ELT tools now have data mapping and transformation capabilities using machine learning to detect data patterns and transformation. Some low-code/no-code tools now integrate data governance features such as profiling, data lineage tracking and automated data validation rules to ensure data integrity. These innovations aim to simplify data integration and empower non-technical users.\nEven with the innovation and simplicity low-code/no-code ELT tool offers, it has some of its limitations:\n\nLow-code/no-code ELT make it difficult to collaborate as a team, unlike a code-heavy pipeline with version control and collaborative features.\nSome low-code/no-code ELT tools give poor developer experience which can hamper productivity, such as bad and non-intuitive UI design and limited customization options.\nAnother challenge with low-code/no-code ELT is the security of the applications built on them, even though the platform is secure. This is an issue since the tool was developed for non-technical individuals with no expertise in security best practices.\nThey do not offer the source code to pipelines built and this is a challenge when an organization wants to migrate to another platform, they are forced to develop from scratch on another platform or stick with that same platform."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#conclusion",
    "href": "posts/The Engineer's Guide to Low Code/index.html#conclusion",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "Conclusion",
    "text": "Conclusion\nWhy learn data engineering since anyone can now use low-code/no-code tools? Data engineering is not just about building pipelines but also solving business problems such as designing schemas. It’s important that the data you give to your end users can answer their questions. In summary, developing your soft skills is more important to make you stand out.\nLow-code/no-code have a lot of abstraction which when things go wrong it’s difficult to find out, some organizations will not want to give out the control they have over their data pipeline by hiding away some of the pipeline core functionality using a low-code/no-code ELT tool."
  },
  {
    "objectID": "posts/The Engineer's Guide to Low Code/index.html#references",
    "href": "posts/The Engineer's Guide to Low Code/index.html#references",
    "title": "The Engineer’s Guide to Low Code/No Code ELT Tools",
    "section": "References",
    "text": "References\n\nUnleashing the power of ELT in AWS using Airbyte\nHow do you ensure data quality and governance in an ELT process?\n6 Best Practices to Scale and Optimize Data Pipelines"
  }
]